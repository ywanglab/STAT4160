[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 4160: Data Science Productivity Tools",
    "section": "",
    "text": "Preface\nThis is the lecture note for the course STAT 4160.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "13‑week plan (2 × 75‑min per week)\nWeek 1 – Setup, Colab, Git/GitHub\nWeek 2 – Reproducible reporting (Quarto) + RStudio cameo\nWeek 3 – Unix for data work + automation\nWeek 4 – SQL I (schemas, joins)\nWeek 5 – pandas for time series\nWeek 6 – APIs & Web scraping (ethics + caching)\nWeek 7 – Quality: tests, lint, minimal CI\nWeek 8 – Time‑series baselines & backtesting\nWeek 9 – Finance‑specific evaluation & leakage control\nWeek 10 – PyTorch fundamentals\nWeek 11 – Transformers for sequences (tiny GPT)\nWeek 12 – Productivity at scale (lightweight)\nWeek 13 – Communication & showcase",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#week-plan-2-75min-per-week",
    "href": "intro.html#week-plan-2-75min-per-week",
    "title": "Introduction",
    "section": "",
    "text": "Lec A: Local Python + VS Code; Colab basics (GPU, Drive mount, persistence limits), repo cloning in Colab, requirements.txt, seeds.\nLec B: Git essentials, branching, PRs, code review etiquette, .gitignore, Git‑LFS do’s/don’ts (quota pitfalls).\nDeliverable: Team repo with a Colab notebook that runs and logs environment info; one PR merged.\n\n\n\nLec A: Quarto for Python: parameters, caching, citations; publish to GitHub Pages.\nLec B (15–25 min cameo): RStudio + Quarto rendering (so they can read R‑centric docs later), then back to Python.\nDeliverable: Parameterized EDA report (symbol, date range as params).\n\n\n\nLec A: Shell basics (pipes, redirects), grep/sed/awk, find/xargs, regex.\nLec B: Shell scripts, simple Makefile/justfile targets; rsync, quick SSH/tmux tour.\nDeliverable: make get-data and make report run end‑to‑end.\n\n\n\nLec A: SQLite in repo; schema design for OHLCV + metadata; SELECT/JOIN/GROUP BY.\nLec B: Window functions; indices; pandas.read_sql pipelines.\nDeliverable: SQL notebook producing a tidy table ready for modeling.\n\n\n\nLec A: Cleaning, types, missing, merges; groupby, pivot; Parquet I/O.\nLec B: Time‑series ops: resampling, rolling windows, shifting/lagging, calendar effects.\nDeliverable: Cleaned Parquet dataset + feature snapshot.\n\n\n\nLec A: HTTP basics, requests, pagination, auth, retries, backoff; don’t hard‑code keys (python‑dotenv).\nLec B: BeautifulSoup, CSS selectors, robots.txt, throttling; cache raw pulls; persist to SQL/Parquet.\nDeliverable: One external data source ingested with caching & schema checks.\n\n\n\nLec A: pytest (2–3 meaningful tests), data validation (light Pandera or custom checks), logging, type hints.\nLec B: Pre‑commit (black, ruff, nbstripout), GitHub Actions to run tests + lint on PRs (fast jobs only).\nDeliverable: CI badge green; failing test demonstrates leakage prevention or schema guard.\n\n\n\nLec A: Problem framing; horizon, step size; MAE/sMAPE/MASE; rolling‑origin evaluation.\nLec B: Baselines: naive/seasonal‑naive; quick ARIMA/Prophet or sklearn regressor with lags.\nDeliverable: Baseline model card + backtest plot in Quarto.\n\n\n\nLec A: Feature timing & label definition (t+1 returns, multi‑step horizons), survivorship bias, look‑ahead traps, data snooping.\nLec B: Walk‑forward / expanding window, embargoed splits, drift detection; error analysis by regime (volatility bins, bull/bear).\nDeliverable: A robust evaluation plan + revised splits; leakage test added to pytest.\n\n\n\nLec A: Tensors, autograd, datasets/dataloaders for windows; training loop, early stopping; GPU in Colab; mixed precision.\nLec B: A small LSTM/TCN baseline for forecasting; monitoring loss/metrics; save best weights.\nDeliverable: PyTorch baseline surpasses classical baseline on at least one metric.\n\n\n\nLec A: Attention from scratch; tiny char‑level GPT (embeddings, positions, single head → multi‑head), sanity‑check overfitting on toy data.\nLec B: Adapt to time series: window embedding, causal masking, regression head; ablation (context length, heads, dropout) within Colab budget.\nDeliverable: Transformer results + one ablation figure; notes on compute/time.\n\n\n\nLec A: Packaging a small library (src/ layout, pyproject.toml), simple CLI (Typer) for batch inference; config via YAML.\nLec B: Optional FastAPI endpoint demo (local only) + reproducibility audit (fresh‑clone run).\nDeliverable: Tagged release v1.0-rc, CLI can score a held‑out period and write a report.\n\n\n\nLec A: Poster + abstract workshop; tell the error‑analysis story; figure polish; README & model card.\nLec B: In‑class presentations + final feedback; plan for continuing to the Spring symposium (next‑steps backlog).\nDeliverable: Poster draft, 250‑word abstract, and a reproducible repo ready to extend.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#project-spine",
    "href": "intro.html#project-spine",
    "title": "Introduction",
    "section": "Project spine",
    "text": "Project spine\n\nMilestones: W1 repo & env → W3 automated data pipeline → W6 external data → W7 CI green → W8 baselines → W9 robust eval plan → W10 PyTorch baseline → W11 tiny Transformer → W12 release candidate → W13 poster & talk.\nTracking (minimal): log experiments to a simple CSV (results/experiments.csv) and keep a Quarto “lab notebook.”\nData strategy: keep raw data out of Git (use make get-data); store processed Parquet under 100MB if you must commit; otherwise regenerate. Use Git‑LFS only for small, immutable artifacts to avoid quota pain.\nSecrets: .env with python‑dotenv + .env in .gitignore. For Colab, use environment variables or a JSON in Drive (not committed).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "lec1.html",
    "href": "lec1.html",
    "title": "1  Session 1 — Dev environment & Colab workflow",
    "section": "",
    "text": "1.1 Session 1 — Dev environment & Colab workflow",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1 — Dev environment & Colab workflow</span>"
    ]
  },
  {
    "objectID": "lec1.html#session-1-dev-environment-colab-workflow",
    "href": "lec1.html#session-1-dev-environment-colab-workflow",
    "title": "1  Session 1 — Dev environment & Colab workflow",
    "section": "",
    "text": "1.1.1 Learning goals\nBy the end of class, students can:\n\nMount Google Drive in Colab and work in a persistent course folder.\nClone a GitHub repo into Drive (or create a project folder if no repo yet).\nCreate and install from a soft‑pinned requirements.txt.\nVerify environment info (Python, OS, library versions) and GPU availability.\nUse a reproducibility seed pattern (NumPy + PyTorch) and validate it.\nSave a simple system check report to the repo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1 — Dev environment & Colab workflow</span>"
    ]
  },
  {
    "objectID": "lec1.html#agenda-75-min",
    "href": "lec1.html#agenda-75-min",
    "title": "1  Session 1 — Dev environment & Colab workflow",
    "section": "1.2 Agenda (75 min)",
    "text": "1.2 Agenda (75 min)\n\n(5 min) Course framing: how we’ll work this semester\n(12 min) Slides & demo: Colab + Drive persistence; project folders; soft vs hard pins\n(8 min) Slides & demo: reproducibility basics (seeds, RNG, deterministic ops)\n(35 min) In‑class lab (Colab): mount Drive → clone/create project → requirements → environment check → reproducibility check → write report\n(10 min) Wrap‑up, troubleshooting, and homework briefing",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1 — Dev environment & Colab workflow</span>"
    ]
  },
  {
    "objectID": "lec1.html#main-points",
    "href": "lec1.html#main-points",
    "title": "1  Session 1 — Dev environment & Colab workflow",
    "section": "1.3 Main Points",
    "text": "1.3 Main Points\nWhy Colab + Drive\n\nColab gives you GPUs and a clean Python every session.\nThe runtime is ephemeral. Anything under /content disappears.\nMount Drive and work under /content/drive/MyDrive/... to persist code and outputs.\n\nProject layout (today’s minimal)\nproject/\n  reports/\n  notebooks/\n  data/\n  requirements.txt\n  system_check.ipynb\n(We’ll add src/, tests, CI in later sessions.)\nPins: soft vs hard\n\nSoft pins (e.g., pandas&gt;=2.2,&lt;3.0) keep you compatible across machines.\nHard pins (exact versions) are for releases. Today we’ll use soft pins, then freeze to requirements-lock.txt in homework.\n\nReproducibility basics\n\nFix seeds for random, NumPy, PyTorch (and CUDA if present).\nDisable nondeterministic cuDNN behavior for repeatability in simple models.\nBeware: some ops remain nondeterministic on GPU; we’ll use simple ones.\n\nMinimal Git today\n\nIf you already have a repo: clone it into G-Drive.\nIf not: create a folder; later you can upload the notebook via GitHub web UI.\nFull Git workflow (branch/PR/CI) starts next session.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1 — Dev environment & Colab workflow</span>"
    ]
  },
  {
    "objectID": "lec1.html#inclass-lab-35-min",
    "href": "lec1.html#inclass-lab-35-min",
    "title": "1  Session 1 — Dev environment & Colab workflow",
    "section": "1.4 In‑class Lab (35 min)",
    "text": "1.4 In‑class Lab (35 min)\n\nInstructor tip: Put these as sequential Colab cells. Students should run them top‑to‑bottom. Replace placeholders like YOUR_USERNAME / YOUR_REPO before class if you already created a starter repo. If not, tell them to use the “no‑repo” path in Step 3B.\n\n\n1.4.1 1) Mount Google Drive and create a course folder\n# Colab cell\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nCOURSE_DIR = \"/content/drive/MyDrive/dspt25\"  # change if you prefer another path\nPROJECT_NAME = \"unified-stocks\"               # course project folder/repo name\nSave it as system_check.ipynb.\n# Colab cell: make directories and cd into project folder\nimport os, pathlib\nbase = pathlib.Path(COURSE_DIR)\nproj = base / PROJECT_NAME  # / is overloaded to create the path\nfor p in [base, proj, proj/\"reports\", proj/\"notebooks\", proj/\"data\"]:\n    p.mkdir(parents=True, exist_ok=True)\n\nimport os\nos.chdir(proj)\nprint(\"Working in:\", os.getcwd())\n\n\n1.4.2 2) (Optional) If you already have a GitHub repo, clone it into Drive\n\nPick A or B (not both).\n\nA. Clone an existing repo (recommended if you created a starter repo)\n# Colab cell: clone via HTTPS (public or your private; for private, you can upload later instead of pushing from Colab) ONLY clone onetime. If run this notebook again, skip this cell. \nREPO_URL = \"https://github.com/YOUR_ORG_OR_USERNAME/YOUR_REPO.git\"  # &lt;- change me\nimport subprocess, os\nos.chdir(base)  # clone next to your project folder\nsubprocess.run([\"git\", \"clone\", REPO_URL], check=True) # check if there is an error inseat of silent. \n# Optionally, use that cloned repo as the working directory:(uncomment the lines below if do this)\n# REPO_NAME = REPO_URL.split(\"/\")[-1].replace(\".git\",\"\")\n# os.chdir(base/REPO_NAME)\n# print(\"Working in:\", os.getcwd())\nos.chdir(proj) # change back to proj dir\nprint(\"Working in:\", os.getcwd())\nB. No repo yet? Stay with the folder we created. You’ll upload files via GitHub web UI after class.\n\n\n1.4.3 3) Create a soft‑pinned requirements.txt and install\n# Colab cell: write a soft-pinned requirements.txt\nreq = \"\"\"\\\npandas&gt;=2.2,&lt;3.0\nnumpy&gt;=2.0.0,&lt;3.0\npyarrow&gt;=15,&lt;17\nmatplotlib&gt;=3.8,&lt;4.0\nscikit-learn&gt;=1.6,&lt;2.0\nyfinance&gt;=0.2,&lt;0.3\npython-dotenv&gt;=1.0,&lt;2.0\n\"\"\"\nopen(\"requirements.txt\",\"w\").write(req)\nprint(open(\"requirements.txt\").read())\n# Colab cell: install (quietly). Torch is usually preinstalled in Colab; we'll check separately.\n!pip install -q -r requirements.txt\n# Colab cell: PyTorch check. If not available (rare in Colab), install CPU-only as a fallback.\ntry:\n    import torch\n    print(\"PyTorch:\", torch.__version__)\nexcept Exception as e:\n    print(\"PyTorch not found; installing CPU-only wheel as fallback...\")\n    !pip install -q torch\n    import torch\n    print(\"PyTorch:\", torch.__version__)\n\n\n1.4.4 4) Environment report (Python/OS/lib versions, GPU availability)\n# Colab cell: environment info + GPU check\nimport sys, platform, json, time\nimport pandas as pd\nimport numpy as np\n\nenv = {\n    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    \"python\": sys.version,\n    \"os\": platform.platform(),\n    \"pandas\": pd.__version__,\n    \"numpy\": np.__version__,\n}\n\ntry:\n    import torch\n    env[\"torch\"] = torch.__version__\n    env[\"cuda_available\"] = bool(torch.cuda.is_available())\n    env[\"cuda_device\"] = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\nexcept Exception as e:\n    env[\"torch\"] = \"not importable\"\n    env[\"cuda_available\"] = False\n    env[\"cuda_device\"] = \"CPU\"\n\nprint(env)\nos.makedirs(\"reports\", exist_ok=True)\nwith open(\"reports/environment.json\",\"w\") as f:\n    json.dump(env, f, indent=2)\n\n\n1.4.5 5) Reproducibility seed utility + quick validation\n# Colab cell: reproducibility helpers\nimport random\nimport numpy as np\n\ndef set_seed(seed: int = 42, deterministic_torch: bool = True):\n    random.seed(seed)\n    np.random.seed(seed)\n    try:\n        import torch\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        if deterministic_torch:\n            torch.backends.cudnn.deterministic = True\n            torch.backends.cudnn.benchmark = False\n            try:\n                torch.use_deterministic_algorithms(True)\n            except Exception:\n                pass\n    except Exception:\n        pass\n\ndef sample_rng_fingerprint(n=5, seed=42):\n    set_seed(seed)\n    a = np.random.rand(n).round(6).tolist()\n    try:\n        import torch\n        b = torch.rand(n).tolist()\n        b = [round(x,6) for x in b]\n    except Exception:\n        b = [\"torch-missing\"]*n\n    return {\"numpy\": a, \"torch\": b}\n\nf1 = sample_rng_fingerprint(n=6, seed=123)\nf2 = sample_rng_fingerprint(n=6, seed=123)\nprint(\"Fingerprint #1:\", f1)\nprint(\"Fingerprint #2:\", f2)\nprint(\"Match:\", f1 == f2)\n\nwith open(\"reports/seed_fingerprint.json\",\"w\") as f:\n    json.dump({\"f1\": f1, \"f2\": f2, \"match\": f1==f2}, f, indent=2)\n\n\n1.4.6 6) Create (or verify) tickers_25.csv for the course\n# Colab cell: create stock list if it doesn't exist yet\nimport pandas as pd, os\ntickers = [\n    \"AAPL\",\"MSFT\",\"AMZN\",\"GOOGL\",\"META\",\"NVDA\",\"TSLA\",\"JPM\",\"JNJ\",\"V\",\n    \"PG\",\"HD\",\"BAC\",\"XOM\",\"CVX\",\"PFE\",\"KO\",\"DIS\",\"NFLX\",\"INTC\",\n    \"CSCO\",\"ORCL\",\"T\",\"VZ\",\"WMT\"\n]\npath = \"tickers_25.csv\"\nif not os.path.exists(path):\n    pd.DataFrame({\"ticker\": tickers}).to_csv(path, index=False)\npd.read_csv(path).head()\n\n\n1.4.7 7) (Optional) Prove GPU works by allocating a small tensor\n# Colab cell: tiny GPU smoke test (safe if CUDA available)\nimport torch, time\n\n# change back to not use deterministic_algorithm to do the matrix computation\n# torch.use_deterministic_algorithms(False)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nx = torch.randn(1000, 1000, device=device)\ny = x @ x.T\nprint(\"Device:\", device, \"| y shape:\", y.shape, \"| mean:\", y.float().mean().item())\n\n\n1.4.8 8) Save a short Markdown environment report\n# Colab cell: write a small Markdown summary for humans\nfrom textwrap import dedent\nsummary = dedent(f\"\"\"\n# System Check\n\n- Timestamp: {env['timestamp']}\n- Python: `{env['python']}`\n- OS: `{env['os']}`\n- pandas: `{env['pandas']}` | numpy: `{env['numpy']}` | torch: `{env['torch']}`\n- CUDA available: `{env['cuda_available']}` | Device: `{env['cuda_device']}`\n\n## RNG Fingerprint\n- Match on repeated seeds: `{f1 == f2}`\n- numpy: `{f1['numpy']}`\n- torch: `{f1['torch']}`\n\"\"\").strip()\n\nopen(\"reports/system_check.md\",\"w\").write(summary)\nprint(summary)\n\n\n1.4.9 Save the file as system_check.ipynb. To do it automatically, you can use the following code:\n\n# Colab cell: save this notebook as system_check.ipynb  \nfrom google.colab import  _message\nnotebook_name = \"system_check.ipynb\"\n# Create the 'notebooks' subdirectory path\nout_dir = proj / \"notebooks\"\nout_path = out_dir / notebook_name\n\n# Make sure the folder exists\nout_dir.mkdir(parents=True, exist_ok=True)\n\n# Get the CURRENT notebook JSON from Colab\nresp = _message.blocking_request('get_ipynb', timeout_sec=10)\nnb = resp.get('ipynb') if isinstance(resp, dict) else None\n\n# Basic sanity check: ensure there are cells\nif not nb or not isinstance(nb, dict) or not nb.get('cells'):\n    raise RuntimeError(\"Could not capture the current notebook contents (no cells returned). \"\n                       \"Try running this cell again after a quick edit, or use File → Save a copy in Drive once.\")\n\n# Write to Drive\nwith open(out_path, 'w', encoding='utf-8') as f:\n    json.dump(nb, f, ensure_ascii=False, indent=2)\n\nprint(\"Saved notebook to:\", out_path)\n\nWhat to submit after class (if you already have a GitHub repo): For today, students may upload system_check.ipynb, reports/environment.json, and reports/system_check.md via the GitHub web UI (Add file → Upload files). We’ll do proper pushes/PRs next session.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1 — Dev environment & Colab workflow</span>"
    ]
  },
  {
    "objectID": "lec1.html#troubleshooting-notes-share-in-class",
    "href": "lec1.html#troubleshooting-notes-share-in-class",
    "title": "1  Session 1 — Dev environment & Colab workflow",
    "section": "1.5 Troubleshooting notes (share in class)",
    "text": "1.5 Troubleshooting notes (share in class)\n\nDrive won’t mount: Refresh the Colab tab, run the mount cell again, re‑authorize Google permissions.\npip install hangs: Rerun; if it persists, restart runtime (Runtime → Restart session) and re‑run from the top.\nPyTorch mismatch: If Colab has Torch preinstalled, don’t upgrade it. If you installed a CPU wheel by mistake and want GPU later, it’s usually easiest to restart runtime.\nPath confusion: Print os.getcwd() often; ensure you’re inside your project folder under /content/drive/MyDrive/....",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1 — Dev environment & Colab workflow</span>"
    ]
  },
  {
    "objectID": "lec1.html#homework-due-before-session-2",
    "href": "lec1.html#homework-due-before-session-2",
    "title": "1  Session 1 — Dev environment & Colab workflow",
    "section": "1.6 Homework (due before Session 2)",
    "text": "1.6 Homework (due before Session 2)\nGoal: Produce a reproducible system snapshot and a seed‑verified mini experiment, then upload to your repo (via GitHub web UI if you’re not comfortable pushing yet).\n\n1.6.1 Part A — Freeze your environment\n\nFrom the same Colab runtime (after installing), create a lock file:\n# Colab cell: freeze exact versions\n!pip freeze &gt; requirements-lock.txt\nprint(\"Wrote requirements-lock.txt with exact versions\")\n!head -n 20 requirements-lock.txt\nAdd a note to README.md explaining the difference between:\n\nrequirements.txt (soft pins for development) and\nrequirements-lock.txt (exact versions used today).\n\n\n\n\n1.6.2 Part B — Reproducibility mini‑experiment\nCreate notebooks/reproducibility_demo.ipynb with the following cells (students copy/paste):\n1) Setup & data generation\nimport numpy as np, torch, random, json, os, time\n\ndef set_seed(seed=123):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    try:\n        torch.use_deterministic_algorithms(True)\n    except Exception:\n        pass\n\ndef make_toy(n=512, d=10, noise=0.1, seed=123):\n    set_seed(seed)\n    X = torch.randn(n, d)\n    true_w = torch.randn(d, 1)\n    y = X @ true_w + noise * torch.randn(n, 1)\n    return X, y, true_w\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nX, y, true_w = make_toy()\nX, y = X.to(device), y.to(device)\n2) Minimal training loop (linear model)\ndef train_once(lr=0.05, steps=300, seed=123):\n    set_seed(seed)\n    model = torch.nn.Linear(X.shape[1], 1, bias=False).to(device)\n    opt = torch.optim.SGD(model.parameters(), lr=lr)\n    loss_fn = torch.nn.MSELoss()\n    losses=[]\n    for t in range(steps):\n        opt.zero_grad(set_to_none=True)\n        yhat = model(X)\n        loss = loss_fn(yhat, y)\n        loss.backward()\n        opt.step()\n        losses.append(loss.item())\n    return model.weight.detach().cpu().numpy(), losses[-1]\n\nw1, final_loss1 = train_once(seed=2025)\nw2, final_loss2 = train_once(seed=2025)\n\nprint(\"Final loss 1:\", round(final_loss1, 6))\nprint(\"Final loss 2:\", round(final_loss2, 6))\nprint(\"Weights equal:\", np.allclose(w1, w2, atol=1e-7))\n3) Save results JSON\nos.makedirs(\"reports\", exist_ok=True)\nresult = {\n    \"device\": device,\n    \"final_loss1\": float(final_loss1),\n    \"final_loss2\": float(final_loss2),\n    \"weights_equal\": bool(np.allclose(w1, w2, atol=1e-7)),\n    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n}\nwith open(\"reports/reproducibility_results.json\",\"w\") as f:\n    json.dump(result, f, indent=2)\nresult\nExpected outcome: the two runs with the same seed should produce the same final loss and identical weights (within tolerance). If on GPU, deterministic settings should keep this stable for this simple model.\n\n\n1.6.3 Part C — Add a .env.example\nCreate a placeholder for API keys we’ll use later:\nenv_example = \"\"\"\\\n# Example environment variables (do NOT commit a real .env with secrets)\nALPHA_VANTAGE_KEY=\nFRED_API_KEY=\n\"\"\"\nopen(\".env.example\", \"w\").write(env_example)\nprint(open(\".env.example\").read())\n\n\n1.6.4 Part D — Upload to GitHub\nUntil we set up pushes/PRs next class, use the GitHub web UI:\n\nUpload: system_check.ipynb, reports/environment.json, reports/system_check.md, requirements.txt, requirements-lock.txt, notebooks/reproducibility_demo.ipynb, reports/reproducibility_results.json, .env.example.\nIf you already cloned a repo in class and are comfortable pushing, you may push from your laptop instead. Do not paste tokens into notebooks.\n\n\n\n1.6.5 Grading (pass/revise)\n\nrequirements.txt present; requirements-lock.txt present and non‑empty.\nsystem_check.ipynb runs and writes reports/system_check.md + environment.json.\nreproducibility_demo.ipynb demonstrates identical results across repeated runs with same seed and writes reports/reproducibility_results.json.\n.env.example present with placeholders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1 — Dev environment & Colab workflow</span>"
    ]
  },
  {
    "objectID": "lec1.html#key-points",
    "href": "lec1.html#key-points",
    "title": "1  Session 1 — Dev environment & Colab workflow",
    "section": "1.7 Key points",
    "text": "1.7 Key points\n\n“Colab is ephemeral; persist to Drive.”\n“Soft pins now; freeze later.”\n“Seeds are necessary but not sufficient—watch for nondeterministic ops.”\n“Never store secrets (API keys) in the repo; use .env and keep a .env.example.”\n\nThat’s it for Session 1. In Session 2 we’ll set up Git basics and Git‑LFS and move from uploading via web UI to branch/PR workflows.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1 — Dev environment & Colab workflow</span>"
    ]
  },
  {
    "objectID": "lec2.html",
    "href": "lec2.html",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "",
    "text": "2.1 Session 2 — Git essentials & Git‑LFS (75 min)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#session-2-git-essentials-gitlfs-75-min",
    "href": "lec2.html#session-2-git-essentials-gitlfs-75-min",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "",
    "text": "2.1.1 Learning goals\nBy the end of class, students can:\n\nExplain Git’s mental model: working directory → staging → commit; branches and remotes.\nCreate a feature branch, commit changes, and push to GitHub from Colab safely.\nUse .gitignore to avoid committing generated artifacts and secrets.\nInstall and configure Git‑LFS, track large/binary files, and verify tracking.\nOpen a pull request (PR) and follow review etiquette.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#agenda-75-minutes",
    "href": "lec2.html#agenda-75-minutes",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "2.2 Agenda (75 minutes)",
    "text": "2.2 Agenda (75 minutes)\n\n(8 min) Recap & goals; overview of today’s workflow\n(12 min) Slides: Git mental model; branches; remotes; commit hygiene\n(10 min) Slides: .gitignore must‑haves; Git‑LFS (when/why); LFS quotas & pitfalls\n(35 min) In‑class lab: clone → config → branch → .gitignore → LFS → sample Parquet → push → PR\n(10 min) Wrap‑up; troubleshooting; homework briefing",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#slides",
    "href": "lec2.html#slides",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "2.3 Slides",
    "text": "2.3 Slides\n\n2.3.1 Git mental model\n\nWorking directory (your files) → git add → staging → git commit → local history\nRemote: GitHub hosts a copy. git push publishes commits; git pull brings others’ changes.\nBranch: a movable pointer to a chain of commits. Default is main. Create feature branches for each change.\n\nIn Git, a branch is essentially just a movable pointer to a commit.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#the-simple-definition",
    "href": "lec2.html#the-simple-definition",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "2.4 1. The simple definition",
    "text": "2.4 1. The simple definition\n\nA branch has a name (e.g., main, feature/login).\nThat name points to a specific commit in your repository.\nAs you make new commits on that branch, the pointer moves forward.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#visual-example",
    "href": "lec2.html#visual-example",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "2.5 2. Visual example",
    "text": "2.5 2. Visual example\nLet’s say your repo looks like this:\nA --- B --- C   ← main\nHere:\n\nmain is the branch name.\nIt points to commit C.\n\nIf you make a new branch:\ngit branch feature\nNow you have:\nA --- B --- C   ← main, feature\nIf you checkout feature and make a commit:\nA --- B --- C   ← main\n             \\\n              D   ← feature\n\nfeature moves forward to D (new commit).\nmain stays at C.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#head-and-active-branch",
    "href": "lec2.html#head-and-active-branch",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "2.6 3. HEAD and active branch",
    "text": "2.6 3. HEAD and active branch\n\nHEAD is your current position — it points to the branch you’re working on.\nWhen you commit, Git moves that branch forward.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#why-branches-matter",
    "href": "lec2.html#why-branches-matter",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "2.7 4. Why branches matter",
    "text": "2.7 4. Why branches matter\n\nLet you work on new features, bug fixes, or experiments without touching the main codebase.\nCheap to create and delete — Git branching is just updating a tiny file.\nEnable parallel development.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#branches-vs-tags",
    "href": "lec2.html#branches-vs-tags",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "2.8 5. Branches vs tags",
    "text": "2.8 5. Branches vs tags\n\nBranch → moves as you commit.\nTag → fixed pointer to a commit (used for marking releases).\n\n\n💡 Inside .git/refs/heads/, each branch is just a plain text file storing a commit hash.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#in-git-checkout--b-the--b-means-create-a-new-branch-before-checking-it-out.",
    "href": "lec2.html#in-git-checkout--b-the--b-means-create-a-new-branch-before-checking-it-out.",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "2.9 In git checkout -b, the -b means “create a new branch” before checking it out.",
    "text": "2.9 In git checkout -b, the -b means “create a new branch” before checking it out.\n\n2.9.1 Without -b\ngit checkout branchname\n\nSwitches to an existing branch.\nFails if the branch does not exist.\n\n\n\n\n2.9.2 With -b\ngit checkout -b branchname\n\nTells Git: “make a branch called branchname pointing to the current commit, and then switch to it.”\nFails if the branch already exists.\n\n\n\n\n2.9.3 Example\nIf you’re on main:\ngit checkout -b feature-x\nSteps Git takes:\n\nCreate a new branch pointer feature-x → same commit as main.\nMove HEAD to feature-x (you’re now “on” that branch).\n\n\n💡 In newer Git versions, the same idea is expressed with:\ngit switch -c feature-x   # -c means create\n-b in checkout and -c in switch both mean create.\n\n\n\n2.9.4 Branch & PR etiquette\n\nOne feature/change per branch (small, reviewable diffs).\nCommit messages: imperative mood, short subject line (≤ 72 chars), details in body if needed:\n\nfeat: add git-lfs tracking for parquet\ndocs: add README section on setup\nchore: ignore raw data directory\n\nPR description: what/why, testing notes, checklist. Tag your teammate for review.\n\n\n\n2.9.5 .gitignore must‑haves\n\nSecrets: .env, API keys (never commit).\nLarge/derived artifacts: raw/interim data, logs, cache, compiled assets.\nNotebooks’ checkpoints: .ipynb_checkpoints/.\nOS/editor cruft: .DS_Store (for Mac), Thumbs.db (for Windows), .vscode/.\n\n\n\n2.9.6 Git‑LFS\n\nGit‑LFS = Large File Storage. Keeps pointers in Git; binaries in LFS storage.\nTrack only what’s necessary to version (e.g., small processed Parquet samples, posters/PDFs, small models).\nDo not LFS huge raw data you can re‑download (make get-data).\nQuotas apply on Git hosting—be selective.\n\n\n\n2.9.7 Safe pushes from Colab\n\nUse a fine‑grained PAT limited to a single repo with Contents: Read/Write + Pull requests: Read/Write.\nEnter token via getpass (not stored). Push using a temporary URL (token not saved in git config).\nAfter push, clear cell output.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#inclass-lab-35-min",
    "href": "lec2.html#inclass-lab-35-min",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "2.10 In‑class lab (35 min)",
    "text": "2.10 In‑class lab (35 min)\n\nInstructor tip: Students should have created a repo on GitHub before this lab (e.g., unified-stocks-teamX). If not, give them 3 minutes to do so and add their partner as a collaborator.\n\nWe’ll:\n\nMount Drive & clone the repo.\nConfigure Git identity.\nCreate a feature branch.\nAdd .gitignore.\nInstall and configure Git‑LFS.\nTrack Parquet & DB files; generate a sample Parquet.\nCommit & push from Colab using a short‑lived PAT.\nOpen a PR (via web UI, optional API snippet included).\n\n\n2.10.1 0) Mount Google Drive and set variables\n# Colab cell\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\n# Adjust these two for YOUR repo\nREPO_OWNER = \"YOUR_GITHUB_USERNAME_OR_ORG\"\nREPO_NAME  = \"unified-stocks-teamX\"   # e.g., unified-stocks-team1\n\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nCLONE_DIR  = f\"{BASE_DIR}/{REPO_NAME}\"\nREPO_URL   = f\"https://github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n\nimport os, pathlib\npathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n\n\n2.10.2 1) Clone the repo (or pull latest if already cloned)\nimport os, subprocess, shutil, pathlib\n\nif not pathlib.Path(CLONE_DIR).exists():\n    !git clone {REPO_URL} {CLONE_DIR}\nelse:\n    # If the folder exists, just ensure it's a git repo and pull latest\n    os.chdir(CLONE_DIR)\n    !git status\n    !git pull --ff-only  # ff to avoid diverged branches\nos.chdir(CLONE_DIR)\nprint(\"Working dir:\", os.getcwd())\n\n\n2.10.3 2) Configure Git identity (local to this repo)\n# Replace with your name and school email\n!git config user.name \"Your Name\"\n!git config user.email \"you@example.edu\"\n\n!git config --get user.name\n!git config --get user.email\n\n\n2.10.4 3) Create and switch to a feature branch\nBRANCH = \"setup/git-lfs\"\n!git checkout -b {BRANCH}\n!git branch --show-current\n\n\n2.10.5 4) Add a robust .gitignore\ngitignore = \"\"\"\\\n# Byte-compiled / cache\n__pycache__/\n*.py[cod]\n\n# Jupyter checkpoints\n.ipynb_checkpoints/\n\n# OS/editor files\n.DS_Store\nThumbs.db\n.vscode/\n\n# Environments & secrets\n.env\n.env.*\n.venv/\n*.pem\n*.key\n\n# Data (raw & interim never committed)\ndata/raw/\ndata/interim/\n\n# Logs & caches\nlogs/\n.cache/\n\"\"\"\nopen(\".gitignore\", \"w\").write(gitignore)\nprint(open(\".gitignore\").read())\n\n\n2.10.6 5) Install and initialize Git‑LFS (Colab)\n# Install git-lfs on the Colab VM (one-time per runtime): apt-get: advanced package tool(manager)\n!apt-get -y update &gt;/dev/null # refresh vailable packages from the repositories\n!apt-get -y install git-lfs &gt;/dev/null\n!git lfs install\n!git lfs version\n\n\n2.10.7 6) Track Parquet/DB/PDF/model binaries with LFS\n# Add .gitattributes entries via git lfs track\n!git lfs track \"data/processed/*.parquet\"\n!git lfs track \"data/*.db\"\n!git lfs track \"models/*.pt\"\n!git lfs track \"reports/*.pdf\"\n\n# Show what LFS is tracking and verify .gitattributes created\n!git lfs track\nprint(\"\\n.gitattributes:\")\nprint(open(\".gitattributes\").read())\n\nWhy not LFS for raw? Raw data should be re‑downloadable with make get-data later; don’t burn LFS quota.\n\n\n\n2.10.8 7) Create a small Parquet file to test LFS\nimport pandas as pd, numpy as np, os, pathlib\n\npathlib.Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n\ntickers = pd.read_csv(\"tickers_25.csv\")[\"ticker\"].tolist() if os.path.exists(\"tickers_25.csv\") else [\n    \"AAPL\",\"MSFT\",\"AMZN\",\"GOOGL\",\"META\",\"NVDA\",\"TSLA\",\"JPM\",\"JNJ\",\"V\",\n    \"PG\",\"HD\",\"BAC\",\"XOM\",\"CVX\",\"PFE\",\"KO\",\"DIS\",\"NFLX\",\"INTC\",\"CSCO\",\"ORCL\",\"T\",\"VZ\",\"WMT\"\n]\n\n# 1000 business days x up to 25 tickers ~ 25k rows; a few MB as Parquet\ndates = pd.bdate_range(\"2018-01-01\", periods=1000)\ndf = (pd.MultiIndex.from_product([tickers, dates], names=[\"ticker\",\"date\"])\n      .to_frame(index=False))\nrng = np.random.default_rng(42)\ndf[\"r_1d\"] = rng.normal(0, 0.01, size=len(df))  # synthetic daily returns\ndf.to_parquet(\"data/processed/sample_returns.parquet\", index=False)\ndf.head()\n\n\n2.10.9 8) Stage and commit changes\n!git add .gitignore .gitattributes data/processed/sample_returns.parquet\n!git status\n\n!git commit -m \"feat: add .gitignore and git-lfs tracking; add sample Parquet\"\n!git log --oneline -n 2  # limit to the most recent 2 commits\nIf see error “error: cannot run .git/hooks/post-commit: No such file or directory”, it means the post-commit hook is not executable or missing. ### Troubleshooting post-commit hook error 1. See what Git is trying to run\nls -l .git/hooks/post-commit\n\nIf you see -rw-r--r--, it’s not executable.\n\n\nMake it executable\n\nchmod +x .git/hooks/post-commit\n\nEnsure it has a valid shebang (first line) Open it and confirm the first line is one of:\n\nhead -n 1 .git/hooks/post-commit\n#!/bin/sh\n# or\n#!/usr/bin/env bash\n# or (if it’s Node)\n#!/usr/bin/env node\nSave if you needed to fix that.\n\nTest the hook manually\n\n.git/hooks/post-commit\n# or explicitly with the interpreter you expect, e.g.:\nbash .git/hooks/post-commit\n\n\n2.10.10 9) Push from Colab with a short‑lived token (safe method)\n\nCreate a fine‑grained PAT at GitHub → Settings → Developer settings → Fine‑grained tokens\n\nResource owner: your username/org\nRepositories: only select repositories\nPermissions: Contents (Read/Write), Pull requests (Read/Write)\nExpiration: short (e.g., 7 days)\n\n\n# Colab cell: push using a temporary URL with token (not saved to git config)\nfrom getpass import getpass\ntoken = getpass(\"Enter your GitHub token (input hidden; not stored): \")\n\npush_url = f\"https://{token}@github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n!git push {push_url} {BRANCH}:{BRANCH}\n\n# Optional: immediately clear the token variable\ndel token\nIf error occurs, check:\n\n\n2.10.11 1. Check permissions\nls -l .git/hooks/pre-push\nIf it looks like -rw-r--r--, then it’s missing the executable bit. Fix:\nchmod +x .git/hooks/pre-push\n\n\n2.10.12 2. Check the first line (shebang)\nOpen it:\nhead -n 1 .git/hooks/pre-push\nYou should see something like:\n#!/bin/sh\nor\n#!/usr/bin/env bash\nIf it’s missing, add a valid shebang.\n\n\n2.10.13 3. Test the hook manually\n.git/hooks/pre-push\n# or explicitly:\nbash .git/hooks/pre-push\n\nIf the command prints the URL, clear this cell’s output after a successful push (Colab: “⋮” → “Clear output”).\n\n\n\n2.10.14 10) Open a Pull Request\nThe name “pull request” can be confusing at first — it sounds like you are “pushing” your code, but really you’re asking someone else to pull it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#origin-of-the-term",
    "href": "lec2.html#origin-of-the-term",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "2.11 Origin of the term",
    "text": "2.11 Origin of the term\n\nThe phrase comes from distributed version control (like Git before GitHub’s UI popularized it).\nIf you had changes in your branch/repo and wanted them in the upstream project, you’d contact the maintainer and say:\n\n“Please pull these changes from my branch into yours.”\n\nSo a pull request is literally a request for someone else to pull your commits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#how-it-works-e.g.-on-github-gitlab-bitbucket",
    "href": "lec2.html#how-it-works-e.g.-on-github-gitlab-bitbucket",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "2.12 How it works (e.g., on GitHub, GitLab, Bitbucket)",
    "text": "2.12 How it works (e.g., on GitHub, GitLab, Bitbucket)\n\nYou push your branch to your fork or to the remote repository.\nYou open a pull request against the target branch (usually main or develop).\nThe repository maintainers review your code.\nIf accepted, they “pull” your commits into their branch (though under the hood it’s often implemented as a merge or rebase).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#contrast-with-push",
    "href": "lec2.html#contrast-with-push",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "2.13 Contrast with “push”",
    "text": "2.13 Contrast with “push”\n\nPush: You directly upload commits to a remote branch you have permission to write to.\nPull request: You don’t merge directly — instead, you ask maintainers to pull your changes, review them, and integrate them.\n\nSummary: It’s called a pull request because you’re not pushing your changes into the target branch; you’re asking the project owner/maintainer to pull your branch into theirs.\n\nRecommended (web UI): Navigate to your repo on GitHub → Compare & pull request → base: main, compare: setup/git-lfs. Fill title/description, tag your partner, and create the PR.\nOptional (API): open a PR programmatically from Colab:\n\n# OPTIONAL: Create PR via GitHub API (requires token again)\nfrom getpass import getpass\nimport requests, json\n\ntoken = getpass(\"GitHub token (again, not stored): \")\nheaders = {\"Authorization\": f\"Bearer {token}\",\n           \"Accept\": \"application/vnd.github+json\"}\npayload = {\n    \"title\": \"Setup: .gitignore + Git-LFS + sample Parquet\",\n    \"head\": BRANCH,\n    \"base\": \"main\",\n    \"body\": \"Adds .gitignore, configures Git-LFS for parquet/db/pdf/model files, and commits a sample Parquet for verification.\"\n}\nr = requests.post(f\"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/pulls\",\n                  headers=headers, data=json.dumps(payload))\nprint(\"PR status:\", r.status_code)\ntry:\n    pr_url = r.json()[\"html_url\"]\n    print(\"PR URL:\", pr_url)\nexcept Exception as e:\n    print(\"Response:\", r.text)\ndel token\n\n2.13.1 11) Quick verification checklist\n\ngit lfs ls-files shows data/processed/sample_returns.parquet:\n\n!git lfs ls-files\n\nPR diff shows a small pointer for the Parquet, not raw binary content.\n.gitignore present; no secrets or raw data committed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#wrapup",
    "href": "lec2.html#wrapup",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "2.14 Wrap‑up",
    "text": "2.14 Wrap‑up\n\nKeep PRs small and focused; write helpful titles and descriptions.\nDon’t commit secrets or large data. Use .env + .env.example.\nUse LFS selectively—version only small, important binaries (e.g., sample processed sets, posters).\nNext time: Quarto polish (already started) and Unix automation to fetch raw data reproducibly.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#homework-due-before-session-3",
    "href": "lec2.html#homework-due-before-session-3",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "2.15 Homework (due before Session 3)",
    "text": "2.15 Homework (due before Session 3)\nGoal: Cement branch/PR hygiene, add review scaffolding, and add a small guard against large files accidentally committed outside LFS.\n\n2.15.1 Part A — Add a PR template and CODEOWNERS\nCreate a PR template so every PR includes key info.\n# Run in your repo root\nimport os, pathlib, textwrap\npathlib.Path(\".github\").mkdir(exist_ok=True)\ntpl = textwrap.dedent(\"\"\"\\\n    ## Summary\n    What does this PR do and why?\n\n    ## Changes\n    - \n\n    ## How to test\n    - From a fresh clone: steps to run\n\n    ## Checklist\n    - [ ] Runs from a fresh clone (README steps)\n    - [ ] No secrets committed; `.env` only (and `.env.example` updated if needed)\n    - [ ] Large artifacts tracked by LFS (`git lfs ls-files` shows expected files)\n    - [ ] Clear, small diff; comments where useful\n\"\"\")\nopen(\".github/pull_request_template.md\",\"w\").write(tpl)\nprint(\"Wrote .github/pull_request_template.md\")\n(Optional) Require both teammates to review by setting CODEOWNERS (edit handles):\nowners = \"\"\"\\\n# Replace with your GitHub handles\n* @teammate1 @teammate2\n\"\"\"\nopen(\".github/CODEOWNERS\",\"w\").write(owners)\nprint(\"Wrote .github/CODEOWNERS (edit handles!)\")\nCommit and push on a new branch (example: chore/pr-template), open a PR, and merge after review. If working on G-Drive: execute the following before git operations: chmod +x .git/hooks/*\n\n\n2.15.2 Part B — Add a large‑file guard (simple Python script)\nCreate a small tool that fails if files &gt; 10 MB are found and aren’t tracked by LFS. This will be used manually for now (automation later in CI).\n# tools/guard_large_files.py\nimport os, subprocess, sys\n\nLIMIT_MB = 10\nROOT = os.getcwd()\n\ndef lfs_tracked_paths(): #find all files tracked by lfs\n    try:\n        out = subprocess.check_output([\"git\", \"lfs\", \"ls-files\"], text=True)\n        tracked = set()\n        for line in out.strip().splitlines():\n            # line format: \"&lt;oid&gt; &lt;path&gt;\"\n            p = line.split(None, 1)[-1].strip()\n            tracked.add(os.path.normpath(p))\n        return tracked\n    except Exception:\n        return set()\n\ndef humanize(bytes_):\n    return f\"{bytes_/(1024*1024):.2f} MB\"\n\nlfs_set = lfs_tracked_paths()\nbad = []\nfor dirpath, dirnames, filenames in os.walk(ROOT):\n    # skip .git directory\n    if \".git\" in dirpath.split(os.sep):\n        continue\n    for fn in filenames:\n        path = os.path.normpath(os.path.join(dirpath, fn))\n        try:\n            size = os.path.getsize(path)\n        except FileNotFoundError:\n            continue\n        if size &gt;= LIMIT_MB * 1024 * 1024:\n            rel = os.path.relpath(path, ROOT)\n            if rel not in lfs_set:\n                bad.append((rel, size))\n\nif bad:\n    print(\"ERROR: Large non-LFS files found:\")\n    for rel, size in bad:\n        print(f\" - {rel} ({humanize(size)})\")\n    sys.exit(1)\nelse:\n    print(\"OK: No large non-LFS files detected.\")\nAdd a Makefile target to run it. Let’s generate the tools directory and the script:\n# Define the path to the tools directory\ntools_dir = Path(\"tools\")\n\n# Create it if it doesn't exist (including any parents)\ntools_dir.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Directory '{tools_dir}' is ready.\")\n# Create/append Makefile target\nfrom pathlib import Path\ntext = \"\\n\\nguard:\\n\\tpython tools/guard_large_files.py\\n\" # guard: Makefile target. \\t: tab required. \np = Path(\"Makefile\") # point to the Makefile\n# p.write_text(p.read_text() + text if p.exists() else text) # if p exists, read exising content and append text and overwrites. \n# the above code will append text everytime, casue error if repeatedly excute. \nif p.exists():\n    content = p.read_text()\n    if \"guard:\" not in content:\n        p.write_text(content + text)\nelse:\n    p.write_text(text)\n\nprint(\"Added 'guard' target to Makefile\")\nAfter running the snippet: Your repo has a Makefile with a guard target. Running:\nmake guard\nwill execute your Python script:\npython tools/guard_large_files.py\nRun locally/Colab:\n!python tools/guard_large_files.py\nCommit on a new branch (e.g., chore/large-file-guard), push, open PR, and merge after review.\n\n\n2.15.3 Part C — Branch/PR practice (each student)\n\nEach student creates their own branch (e.g., docs/readme-username) and:\n\nAdds a “Development workflow” section in README.md (1–2 paragraphs): how to clone, mount Drive in Colab, install requirements, and where outputs go.\nAdds themselves to README.md “Contributors” section with a GitHub handle link.\n\nPush branch and open a PR.\nPartner reviews the PR:\n\nLeave at least 2 useful comments (nits vs blockers).\nApprove when ready; the author merges.\n\n\nExpected files touched: README.md, .github/pull_request_template.md, optional .github/CODEOWNERS, tools/guard_large_files.py, Makefile.\n\n\n2.15.4 Part D — Prove LFS is working\n\nOn main, run:\n\n!git lfs ls-files\n\nYou should see data/processed/sample_returns.parquet (and any other tracked binaries).\nIn the GitHub web UI, click the file to confirm it’s an LFS pointer, not full binary contents.\n\n\n\n2.15.5 Submission checklist (pass/revise)\n\nTwo merged PRs (template + guard) with clear titles and descriptions.\nREADME updated with development workflow and contributors.\ngit lfs ls-files shows expected files.\ntools/guard_large_files.py present and passes (OK) on main.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec2.html#key-points",
    "href": "lec2.html#key-points",
    "title": "2  Session 2 — Git essentials & Git‑LFS",
    "section": "2.16 Key points",
    "text": "2.16 Key points\n\nSmall PRs win. Short diffs → fast, focused reviews.\nDon’t commit secrets. .env only; keep .env.example up to date.\nUse LFS sparingly and purposefully—prefer regenerating big raw data.\nColab pushes: use a short‑lived token, and clear outputs after use.\n\nNext session: Quarto reporting polish and pipeline hooks; soon after, Unix automation so make get-data can reproducibly fetch raw data for the unified‑stocks project.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 — Git essentials & Git‑LFS</span>"
    ]
  },
  {
    "objectID": "lec3.html",
    "href": "lec3.html",
    "title": "3  Session 3 — Quarto Reports (Python)",
    "section": "",
    "text": "3.1 Session 3 — Quarto Reports (Python) — 75 minutes",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 — Quarto Reports (Python)</span>"
    ]
  },
  {
    "objectID": "lec3.html#session-3-quarto-reports-python-75-minutes",
    "href": "lec3.html#session-3-quarto-reports-python-75-minutes",
    "title": "3  Session 3 — Quarto Reports (Python)",
    "section": "",
    "text": "3.1.1 Learning goals\nBy the end of class, students can:\n\nCreate a parameterized Quarto report (.qmd) that runs Python code.\nRender a report from Colab using the Quarto CLI (with caching).\nPass parameters on the command line to re‑render for different tickers/date ranges.\nConfigure a minimal Quarto website that builds to docs/ and publish it via GitHub Pages.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 — Quarto Reports (Python)</span>"
    ]
  },
  {
    "objectID": "lec3.html#agenda-75-min",
    "href": "lec3.html#agenda-75-min",
    "title": "3  Session 3 — Quarto Reports (Python)",
    "section": "3.2 Agenda (75 min)",
    "text": "3.2 Agenda (75 min)\n\n(8 min) Why Quarto for DS: literate programming, parameters, caching, publishing\n(12 min) Anatomy of a .qmd: YAML front matter, params:, code chunks, execute: options, figures\n(35 min) In‑class lab: install Quarto in Colab → create _quarto.yml → write reports/eda.qmd → render for AAPL/MSFT → output to docs/\n(10 min) GitHub Pages walkthrough + troubleshooting + homework briefing\n(10 min) Buffer for hiccups (first Quarto install/render often needs a minute)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 — Quarto Reports (Python)</span>"
    ]
  },
  {
    "objectID": "lec3.html#slides",
    "href": "lec3.html#slides",
    "title": "3  Session 3 — Quarto Reports (Python)",
    "section": "3.3 Slides",
    "text": "3.3 Slides\nWhy Quarto\n\nOne source of truth for code + prose + figures → reproducibility and explainability.\nParameterization = fast re‑runs with different inputs (ticker/horizon).\nPublishing to GitHub Pages gives a permanent, shareable artifact.\n\nKey concepts\n\nFront matter:\n\nformat: controls HTML/PDF/RevealJS (we’ll use HTML).\nexecute: controls caching, echo, warnings.\nparams: defines inputs; accessed as params dict in Python cells.\n\nPerformance: enable execute.cache: true to avoid refetching/recomputing.\nPublishing: write to docs/ then enable GitHub Pages (Settings → Pages → “Deploy from a branch” → main / /docs).\n\nEthics/footnote\n\nFinancial data EDA here is educational only; not trading advice.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 — Quarto Reports (Python)</span>"
    ]
  },
  {
    "objectID": "lec3.html#inclass-lab-35-min",
    "href": "lec3.html#inclass-lab-35-min",
    "title": "3  Session 3 — Quarto Reports (Python)",
    "section": "3.4 In‑class lab (35 min)",
    "text": "3.4 In‑class lab (35 min)\n\nInstructor tip: Ask students to follow step‑by‑step. If they didn’t complete Session 2’s clone, they can create a fresh folder under Drive and initialize a new GitHub repo afterward.\n\n\n3.4.1 0) Mount Drive and set repo paths\nRun each block as a separate Colab cell.\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_OWNER = \"YOUR_GITHUB_USERNAME_OR_ORG\"  # &lt;- change\nREPO_NAME  = \"unified-stocks-teamX\"         # &lt;- change\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\nREPO_URL   = f\"https://github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n\nimport pathlib, os, subprocess\npathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n\nif not pathlib.Path(REPO_DIR).exists():\n    !git clone {REPO_URL} {REPO_DIR}\nelse:\n    %cd {REPO_DIR}\n    !git pull --ff-only\n%cd {REPO_DIR}\n\n\n3.4.2 1) Install Quarto CLI on Colab and verify\n# Install Quarto CLI (one-time per Colab runtime)\n!wget -q https://quarto.org/download/latest/quarto-linux-amd64.deb -O /tmp/quarto.deb\n!dpkg -i /tmp/quarto.deb || apt-get -y -f install &gt;/dev/null && dpkg -i /tmp/quarto.deb\n!quarto --version\n\n\n3.4.3 2) Minimal project config: _quarto.yml (website to docs/)\nfrom textwrap import dedent\nqproj = dedent(\"\"\"\\\nproject:\n  type: website\n  output-dir: docs\n\nwebsite:\n  title: \"Unified Stocks — EDA\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - href: reports/eda.qmd\n        text: EDA (parametrized)\n\nformat:\n  html:\n    theme: cosmo\n    toc: true\n    code-fold: false\n\nexecute:\n  echo: true\n  warning: false\n  cache: true\n\"\"\")\nopen(\"_quarto.yml\",\"w\").write(qproj)\nprint(open(\"_quarto.yml\").read())\nCreate a simple homepage:\nindex = \"\"\"\\\n---\ntitle: \"Unified Stocks Project\"\n---\n\nWelcome! Use the navigation to view the EDA report.\n\n- **Stock set**: see `tickers_25.csv`\n- **Note**: Educational use only — no trading advice.\n\"\"\"\nopen(\"index.qmd\",\"w\").write(index)\nprint(open(\"index.qmd\").read())\n\n\n3.4.4 3) Create the parameterized EDA report: reports/eda.qmd\n::::\nimport os, pathlib\npathlib.Path(\"reports/figs\").mkdir(parents=True, exist_ok=True)\n#\neda_qmd = \"\"\"\\\n---\ntitle: \"Stock EDA\"\nformat:\n  html:\n    toc: true\n    number-sections: false\nexecute-dir: \"/content/drive/MyDrive/dspt25/STAT4160/reports\"\nexecute:\n  echo: false\n  warning: false\n  cache: false     # keep off while testing params\n\njupyter: python3\nparams:\n  symbol: \"AAPL\"\n  start_date: \"2018-01-01\"\n  end_date: \"\"\n  rolling: 20\n---\n\n\n::: callout-note\nThis report is parameterized. To change inputs without editing code, pass\n`-P symbol:MSFT -P start_date:2019-01-01 -P end_date:2025-08-01 -P rolling:30` to `quarto render`.\n:::\n\n## Setup if using Python\n\n::: {#ebadf2f2 .cell tags='[\"parameters\"]' execution_count=1}\n``` {.python .cell-code}\n# Default values (overridden by -P at render time)\nSYMBOL = \"AAPL\"\nSTART  = \"2018-01-01\"\nEND    = \"\"\nROLL   =  20\n:::\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nfrom pathlib import Path\nfrom datetime import datetime\n\n# Read parameters if using R\n# SYMBOL = params.get(\"symbol\", \"AAPL\")\n# START  = params.get(\"start_date\", \"2018-01-01\")\n# END    = params.get(\"end_date\", \"\")\n# ROLL   = int(params.get(\"rolling\", 20))\n\nif not END:\n    END = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\n\nSYMBOL, START, END, ROLL",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 — Quarto Reports (Python)</span>"
    ]
  },
  {
    "objectID": "lec3.html#download-and-prepare-data",
    "href": "lec3.html#download-and-prepare-data",
    "title": "3  Session 3 — Quarto Reports (Python)",
    "section": "3.5 Download and prepare data",
    "text": "3.5 Download and prepare data\n\n# Fetch adjusted OHLCV\ntry:\n    data = yf.download(SYMBOL, start=START, end=END, auto_adjust=True, progress=False)\nexcept Exception as e:\n    print(\"yfinance failed, falling back to synthetic series:\", e)\n    idx = pd.bdate_range(START, END)\n    rng = np.random.default_rng(42)\n    ret = rng.normal(0, 0.01, len(idx))\n    price = 100 * np.exp(np.cumsum(ret))\n    vol = rng.integers(1e5, 5e6, len(idx))\n    data = pd.DataFrame({\"Close\": price, \"Volume\": vol}, index=idx)\n\n# Tidy & features\ndf = data.rename(columns=str.lower).copy()\ndf = df[[\"close\",\"volume\"]].dropna()\ndf[\"log_return\"] = np.log(df[\"close\"]).diff()\ndf[\"roll_mean\"]  = df[\"log_return\"].rolling(ROLL, min_periods=ROLL//2).mean()\ndf[\"roll_vol\"]   = df[\"log_return\"].rolling(ROLL, min_periods=ROLL//2).std()\ndf = df.dropna()\ndf.head()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 — Quarto Reports (Python)</span>"
    ]
  },
  {
    "objectID": "lec3.html#price-over-time",
    "href": "lec3.html#price-over-time",
    "title": "3  Session 3 — Quarto Reports (Python)",
    "section": "3.6 Price over time",
    "text": "3.6 Price over time\n\nfig, ax = plt.subplots(figsize=(8,3))\nax.plot(df.index, df[\"close\"])\nax.set_title(f\"{SYMBOL} — Adjusted Close\")\nax.set_xlabel(\"Date\"); ax.set_ylabel(\"Price\")\nfig.tight_layout()\n# figpath = Path(\"reports/figs\")/f\"{SYMBOL}_price.png\"\nfigpath = Path(\"figs\")/f\"{SYMBOL}_price.png\" #same changes for the rest of the figures\nfig.savefig(figpath, dpi=144)\nfigpath",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 — Quarto Reports (Python)</span>"
    ]
  },
  {
    "objectID": "lec3.html#daily-log-returns-histogram",
    "href": "lec3.html#daily-log-returns-histogram",
    "title": "3  Session 3 — Quarto Reports (Python)",
    "section": "3.7 Daily log returns — histogram",
    "text": "3.7 Daily log returns — histogram\n\nfig, ax = plt.subplots(figsize=(6,3))\nax.hist(df[\"log_return\"], bins=50, alpha=0.8)\nax.set_title(f\"{SYMBOL} — Daily Log Return Distribution\")\nax.set_xlabel(\"log return\"); ax.set_ylabel(\"count\")\nfig.tight_layout()\nfigpath = Path(\"figs\")/f\"{SYMBOL}_hist.png\"\nfig.savefig(figpath, dpi=144)\nfigpath",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 — Quarto Reports (Python)</span>"
    ]
  },
  {
    "objectID": "lec3.html#rolling-mean-volatility-window-params.rolling",
    "href": "lec3.html#rolling-mean-volatility-window-params.rolling",
    "title": "3  Session 3 — Quarto Reports (Python)",
    "section": "3.8 Rolling mean & volatility (window = {params.rolling})",
    "text": "3.8 Rolling mean & volatility (window = {params.rolling})\n\nfig, ax = plt.subplots(figsize=(8,3))\nax.plot(df.index, df[\"roll_mean\"], label=\"rolling mean\")\nax.plot(df.index, df[\"roll_vol\"],  label=\"rolling std\")\nax.set_title(f\"{SYMBOL} — Rolling Return Stats (window={ROLL})\")\nax.set_xlabel(\"Date\"); ax.set_ylabel(\"value\")\nax.legend()\nfig.tight_layout()\nfigpath = Path(\"figs\")/f\"{SYMBOL}_rolling.png\"\nfig.savefig(figpath, dpi=144)\nfigpath",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 — Quarto Reports (Python)</span>"
    ]
  },
  {
    "objectID": "lec3.html#summary-table",
    "href": "lec3.html#summary-table",
    "title": "3  Session 3 — Quarto Reports (Python)",
    "section": "3.9 Summary table",
    "text": "3.9 Summary table\n\nsummary = pd.DataFrame({\n    \"n_days\": [len(df)],\n    \"start\": [df.index.min().date()],\n    \"end\":   [df.index.max().date()],\n    \"mean_daily_ret\": [df[\"log_return\"].mean()],\n    \"std_daily_ret\":  [df[\"log_return\"].std()],\n    \"ann_vol_approx\": [df[\"log_return\"].std()*np.sqrt(252)]\n})\nsummary\n\nNote: Educational use only. This is not trading advice. ““”\n::::\n\n\n```python\nopen(\"reports/eda.qmd\",\"w\").write(eda_qmd)\nprint(\"Wrote reports/eda.qmd\")\n\n3.9.1 4) Render the report for one ticker (AAPL) and put outputs in docs/\n# Single render with defaults (AAPL)\n!quarto render reports/eda.qmd --output-dir docs/\nOpen the produced HTML (Colab file browser → docs/reports/eda.html). If the HTML is under docs/reports/eda.html, that’s expected (Quarto keeps layout mirroring source folders).\n\n\n3.9.2 5) Render for multiple tickers by passing parameters\n# Render for MSFT with custom dates and rolling window\n!quarto render reports/eda.qmd -P symbol:MSFT -P start_date:2019-01-01 -P end_date:2025-08-01 -P rolling:30 --output-dir docs/\n\n# Render for NVDA with a different window\n!quarto render reports/eda.qmd -P symbol:NVDA -P start_date:2018-01-01 -P end_date:2025-08-01 -P rolling:60 --output-dir docs/\nThis will create docs/reports/eda.html for the last render (Quarto overwrites the same output path by default). If you want separate pages per ticker, render to different filenames:\n# Example: write MSFT to docs/reports/eda-MSFT.html via project copy\nimport shutil, os\nshutil.copy(\"reports/eda.qmd\", \"reports/eda-MSFT.qmd\")\n!quarto render reports/eda-MSFT.qmd -P symbol:MSFT -P start_date:2019-01-01 -P end_date:2025-08-01 -P rolling:30 --output-dir docs/\n\n\n3.9.3 6) Add nav links to specific ticker pages (optional)\n# Append MSFT page to navbar\nfrom ruamel.yaml import YAML\nyaml = YAML()\ncfg = yaml.load(open(\"_quarto.yml\"))\ncfg[\"website\"][\"navbar\"][\"left\"].append({\"href\": \"reports/eda-MSFT.qmd\", \"text\": \"MSFT EDA\"})\nwith open(\"_quarto.yml\",\"w\") as f:\n    yaml.dump(cfg, f)\n!quarto render --output-dir docs/\n\n\n3.9.4 7) Commit and push site to GitHub (so Pages can serve docs/)\n!git add _quarto.yml index.qmd reports/eda*.qmd reports/figs docs\n!git status\n!git commit -m \"feat: add parameterized Quarto EDA and publish to docs/\"\n# Push using a short-lived fine-grained token (as in Session 2)\nfrom getpass import getpass\ntoken = getpass(\"GitHub token (not stored): \")\npush_url = f\"https://{token}@github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n!git push {push_url} HEAD:main\ndel token\n\n\n3.9.5 8) Enable GitHub Pages (one-time, UI)\n\nOn GitHub: Settings → Pages\n\nSource: Deploy from a branch\nBranch: main\nFolder: /docs\n\nSave. Wait ~1–3 minutes. Your site will be live at the URL GitHub shows (usually https://&lt;owner&gt;.github.io/&lt;repo&gt;/).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 — Quarto Reports (Python)</span>"
    ]
  },
  {
    "objectID": "lec3.html#wrapup-10-min",
    "href": "lec3.html#wrapup-10-min",
    "title": "3  Session 3 — Quarto Reports (Python)",
    "section": "3.10 Wrap‑up (10 min)",
    "text": "3.10 Wrap‑up (10 min)\n\nRe‑rendering with -P lets you build many variants quickly.\nKeep data fetches cached and/or saved to files to speed up renders.\nYour team can add more pages (e.g., Methodology, Results, Model Card) and link them via _quarto.yml.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 — Quarto Reports (Python)</span>"
    ]
  },
  {
    "objectID": "lec3.html#homework-due-before-session-4",
    "href": "lec3.html#homework-due-before-session-4",
    "title": "3  Session 3 — Quarto Reports (Python)",
    "section": "3.11 Homework (due before Session 4)",
    "text": "3.11 Homework (due before Session 4)\nGoal: Enhance the EDA report with two features and publish distinct pages for three tickers from tickers_25.csv.\n\n3.11.1 Part A — Add drawdown & simple regime shading\n\nEdit reports/eda.qmd. After computing df[\"log_return\"], compute:\n\ncum_return and drawdown\nA simple volatility regime indicator (e.g., rolling std quantiles)\n\n\n# Add to the \"Tidy & features\" section in eda.qmd\ndf[\"cum_return\"] = df[\"log_return\"].cumsum().fillna(0.0)\npeak = df[\"cum_return\"].cummax()\ndf[\"drawdown\"] = df[\"cum_return\"] - peak\n\n# Regime via rolling volatility terciles\nvol = df[\"log_return\"].rolling(ROLL, min_periods=ROLL//2).std()\nq1, q2 = vol.quantile([0.33, 0.66])\ndef regime(v):\n    if np.isnan(v): return \"mid\"\n    return \"low\" if v &lt; q1 else (\"high\" if v &gt; q2 else \"mid\")\ndf[\"regime\"] = [regime(v) for v in vol]\ndf[\"regime\"].value_counts().to_frame(\"days\").T\n\nAdd a drawdown plot and shade high‑volatility regimes:\n\n# Drawdown plot\nfig, ax = plt.subplots(figsize=(8,3))\nax.plot(df.index, df[\"drawdown\"])\nax.set_title(f\"{SYMBOL} — Drawdown (log-return cumulative)\")\nax.set_xlabel(\"Date\"); ax.set_ylabel(\"drawdown\")\nfig.tight_layout()\nfigpath = Path(\"reports/figs\")/f\"{SYMBOL}_drawdown.png\"\nfig.savefig(figpath, dpi=144)\nfigpath\n# Price with regime shading (simple)\nfig, ax = plt.subplots(figsize=(8,3))\nax.plot(df.index, df[\"close\"])\nax.set_title(f\"{SYMBOL} — Price with High-Volatility Shading\")\nax.set_xlabel(\"Date\"); ax.set_ylabel(\"Price\")\n\n# Shade where regime == 'high'\nmask = (df[\"regime\"] == \"high\")\n# merge contiguous regions\nin_region = False\nstart = None\nfor i, (ts, is_high) in enumerate(zip(df.index, mask)):\n    if is_high and not in_region:\n        in_region = True\n        start = ts\n    if in_region and (not is_high or i == len(df)-1):\n        end = df.index[i-1] if not is_high else ts\n        ax.axvspan(start, end, alpha=0.15)  # shaded band\n        in_region = False\nfig.tight_layout()\nfigpath = Path(\"reports/figs\")/f\"{SYMBOL}_price_regimes.png\"\nfig.savefig(figpath, dpi=144)\nfigpath\n\n\n3.11.2 Part B — Render three separate pages and link them in the navbar\n\nMake copies of the report source so each produces its own page:\n\nimport shutil\nshutil.copy(\"reports/eda.qmd\", \"reports/eda-AAPL.qmd\")\nshutil.copy(\"reports/eda.qmd\", \"reports/eda-MSFT.qmd\")\nshutil.copy(\"reports/eda.qmd\", \"reports/eda-NVDA.qmd\")\n\nRender each with different parameters:\n\n!quarto render reports/eda-AAPL.qmd -P symbol:AAPL -P start_date:2018-01-01 -P end_date:2025-08-01 -P rolling:30 --output-dir docs/\n!quarto render reports/eda-MSFT.qmd -P symbol:MSFT -P start_date:2018-01-01 -P end_date:2025-08-01 -P rolling:30 --output-dir docs/\n!quarto render reports/eda-NVDA.qmd -P symbol:NVDA -P start_date:2018-01-01 -P end_date:2025-08-01 -P rolling:30 --output-dir docs/\n\nAdd to the navbar in _quarto.yml and rebuild site:\n\nfrom ruamel.yaml import YAML\nyaml = YAML()\ncfg = yaml.load(open(\"_quarto.yml\"))\ncfg[\"website\"][\"navbar\"][\"left\"].extend([\n  {\"href\": \"reports/eda-AAPL.qmd\", \"text\": \"AAPL\"},\n  {\"href\": \"reports/eda-MSFT.qmd\", \"text\": \"MSFT\"},\n  {\"href\": \"reports/eda-NVDA.qmd\", \"text\": \"NVDA\"},\n])\nwith open(\"_quarto.yml\",\"w\") as f:\n    yaml.dump(cfg, f)\n!quarto render --output-dir docs/\n\nCommit & push (use your short‑lived token as before):\n\n!git add reports/eda-*.qmd reports/figs _quarto.yml docs\n!git commit -m \"feat: EDA enhancements (drawdown/regimes) and pages for AAPL/MSFT/NVDA\"\nfrom getpass import getpass\ntoken = getpass(\"GitHub token (not stored): \")\npush_url = f\"https://{token}@github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n!git push {push_url} HEAD:main\ndel token\n\nVerify GitHub Pages shows navbar links and pages load.\n\n\n\n3.11.3 Part C — Makefile convenience targets\nAppend these to your project Makefile:\nreport:\n\\tquarto render reports/eda.qmd --output-dir docs/\n\nreports-trio:\n\\tquarto render reports/eda-AAPL.qmd -P symbol:AAPL -P start_date:2018-01-01 -P end_date:2025-08-01 --output-dir docs/\n\\tquarto render reports/eda-MSFT.qmd -P symbol:MSFT -P start_date:2018-01-01 -P end_date:2025-08-01 --output-dir docs/\n\\tquarto render reports/eda-NVDA.qmd -P symbol:NVDA -P start_date:2018-01-01 -P end_date:2025-08-01 --output-dir docs/\n\nOn Colab, running make requires make to be available (it is). Otherwise, keep using quarto render commands.\n\n\n\n3.11.4 Grading (pass/revise)\n\nreports/eda.qmd renders with parameters and caching enabled.\nAt least three ticker pages rendered and linked in navbar.\nDrawdown and simple regime shading working on the EDA page(s).\nSite published via GitHub Pages (docs/ present on main and live).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 — Quarto Reports (Python)</span>"
    ]
  },
  {
    "objectID": "lec3.html#key-poitns",
    "href": "lec3.html#key-poitns",
    "title": "3  Session 3 — Quarto Reports (Python)",
    "section": "3.12 Key poitns",
    "text": "3.12 Key poitns\n\nParameters make reports reusable; don’t copy‑paste notebooks for each ticker.\nCache for speed; docs/ for Pages.\nKeep figures saved under reports/figs/ and referenced in the report.\nKeep secrets out of the repo; EDA uses public data only.\n\nNext time (Session 4): a quick RStudio Quarto cameo and more report hygiene (citations, figure captions, alt text), then into Unix automation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 — Quarto Reports (Python)</span>"
    ]
  },
  {
    "objectID": "lec4.html",
    "href": "lec4.html",
    "title": "4  Session 4 — RStudio Quarto cameo + Report Hygiene",
    "section": "",
    "text": "4.1 Session 4 — RStudio cameo + Report Hygiene (75 min)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4 — RStudio Quarto cameo + Report Hygiene</span>"
    ]
  },
  {
    "objectID": "lec4.html#session-4-rstudio-cameo-report-hygiene-75-min",
    "href": "lec4.html#session-4-rstudio-cameo-report-hygiene-75-min",
    "title": "4  Session 4 — RStudio Quarto cameo + Report Hygiene",
    "section": "",
    "text": "4.1.1 Learning goals\nBy the end of class, students can:\n\nRender a Python‑only Quarto report from RStudio (or RStudio Cloud) as a proof that Quarto is editor‑agnostic.\nAdd hygiene features to the project: citations (references.bib), figure/table captions + cross‑references, alt text, better site navigation, custom CSS, and freeze/caching for reproducibility.\nProduce a Data Dictionary section that documents columns and dtypes, and reference it from the EDA page.\nRender & publish the cleaned site to GitHub Pages.\n\n\nPython environment in Rstudio:\n1. Tools-&gt;Global Options-&gt;Python-&gt;select, then select one available virtual environment (e.g. a conda environment). You might need to execute one Python code block before using the “Render” menu button. If no virtual environment is shown, use conda env list to show all the environments and their paths, then copy the path of the environment you want to use, and add python.exe, e.g., it should look like this: C:/Users/ywang2/.conda/envs/stat1010/python.exe. And this to the Python Interpreter path box. make sure also under the Environment tab, switch R to Python. 2. Click on the “Terminal” dropdown arrow, switch to a “command prompt” Terminal. You might need go to Option-&gt;Terminal-&gt;New terminal open with-&gt; command prompt”. Then activate the virtual environment.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4 — RStudio Quarto cameo + Report Hygiene</span>"
    ]
  },
  {
    "objectID": "lec4.html#agenda-75-min",
    "href": "lec4.html#agenda-75-min",
    "title": "4  Session 4 — RStudio Quarto cameo + Report Hygiene",
    "section": "4.2 Agenda (75 min)",
    "text": "4.2 Agenda (75 min)\n\n(10 min) Why report hygiene matters (credibility, accessibility, reusability)\n(15 min) RStudio cameo: Render the Python‑based Quarto report in RStudio\n(30 min) In‑class lab (Colab): add citations, cross‑refs, alt text, freeze/caching, CSS, data dictionary, rebuild site\n(10 min) Wrap‑up + troubleshooting + homework briefing\n4.3 (10 min) Buffer (for first‑time installs or Git pushes)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4 — RStudio Quarto cameo + Report Hygiene</span>"
    ]
  },
  {
    "objectID": "lec4.html#slides",
    "href": "lec4.html#slides",
    "title": "4  Session 4 — RStudio Quarto cameo + Report Hygiene",
    "section": "4.4 Slides",
    "text": "4.4 Slides\n\n4.4.1 Why hygiene?\n\nCredibility: citations + model/report lineage\nAccessibility: alt text, readable fonts, color‑safe figures\nReusability: parameters, freeze/caching, stable page links\nAssessability: clear captions, labeled figures & tables, cross‑references\n\n\n\n4.4.2 Quarto features we’ll use\n\nCaptions & labels: #| label: fig-price, #| fig-cap: \"Price over time\" → reference in text with @fig-price\nTables: #| label: tbl-summary, #| tbl-cap: \"Summary statistics\" → reference with @tbl-summary\nAlt text: #| fig-alt: \"One‑sentence description of the figure\"\nCitations: add bibliography: references.bib and cite with [@key]\nFreeze: project‑level freeze: auto for deterministic rebuilds\nCache: execute: cache: true to avoid redoing expensive steps\nCSS: (cascadfing style sheet) small tweaks to readability (font size, code block width)\n\n\n\n4.4.3 RStudio cameo (no R required)\n\nRStudio integrates Quarto; the Render button runs quarto render under the hood.\nYour .qmd can be Python‑only; RStudio is just the IDE.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4 — RStudio Quarto cameo + Report Hygiene</span>"
    ]
  },
  {
    "objectID": "lec4.html#rstudio-cameo-15-min-live-demo-steps",
    "href": "lec4.html#rstudio-cameo-15-min-live-demo-steps",
    "title": "4  Session 4 — RStudio Quarto cameo + Report Hygiene",
    "section": "4.5 RStudio cameo (15 min, live demo steps)",
    "text": "4.5 RStudio cameo (15 min, live demo steps)\n\nOpen RStudio (Desktop or Cloud).\nFile → Open Project and select your repo folder (e.g.: unified-stocks-teamX).\nConfirm Quarto: Help → About Quarto (or run quarto --version in the RStudio terminal).\nOpen reports/eda.qmd. Click Render (or run quarto render reports/eda.qmd).\nShow the generated HTML preview. Note: no R code, just Python chunks.\nRMarkdown is the predecessor; Quarto unifies Python & R (and more). We use Quarto.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4 — RStudio Quarto cameo + Report Hygiene</span>"
    ]
  },
  {
    "objectID": "lec4.html#inclass-lab-30-min-colabfriendly",
    "href": "lec4.html#inclass-lab-30-min-colabfriendly",
    "title": "4  Session 4 — RStudio Quarto cameo + Report Hygiene",
    "section": "4.6 In‑class lab (30 min, Colab‑friendly)",
    "text": "4.6 In‑class lab (30 min, Colab‑friendly)\n\nWe’ll: ensure Quarto CLI is present, upgrade _quarto.yml (freeze, bibliography, CSS), add references.bib, rewrite EDA with captions/labels/alt text, generate a Data Dictionary, re‑render, and push to GitHub.\n\n\n4.6.1 0) Mount Drive, set repo path, and ensure Quarto CLI\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_OWNER = \"YOUR_GITHUB_USERNAME_OR_ORG\"  # &lt;- change\nREPO_NAME  = \"unified-stocks-teamX\"         # &lt;- change\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\nREPO_URL   = f\"https://github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n\nimport pathlib, os\npathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n\nif not pathlib.Path(REPO_DIR).exists():\n    !git clone {REPO_URL} {REPO_DIR}\n%cd {REPO_DIR}\n\n# Ensure Quarto CLI\n!quarto --version || (wget -q https://quarto.org/download/latest/quarto-linux-amd64.deb -O /tmp/quarto.deb && dpkg -i /tmp/quarto.deb || (apt-get -y -f install &gt;/dev/null && dpkg -i /tmp/quarto.deb))\n!quarto --version\n\n\n4.6.2 1) Upgrade _quarto.yml: freeze, bibliography, CSS, nav polish\n# Install ruamel.yaml for safe YAML edits\n!pip -q install ruamel.yaml\n\nfrom ruamel.yaml import YAML\nfrom pathlib import Path\n\nyaml = YAML()\ncfg_path = Path(\"_quarto.yml\")\nif cfg_path.exists():\n    cfg = yaml.load(cfg_path.read_text())\nelse:\n    cfg = {\"project\": {\"type\": \"website\", \"output-dir\": \"docs\"},\n           \"website\": {\"title\": \"Unified Stocks\", \"navbar\": {\"left\": [{\"href\":\"index.qmd\",\"text\":\"Home\"}]}},\n           \"format\":{\"html\":{\"theme\":\"cosmo\",\"toc\":True}}}\n\n# Add/ensure features\ncfg.setdefault(\"format\", {}).setdefault(\"html\", {})\ncfg[\"format\"][\"html\"][\"toc\"] = True\ncfg[\"format\"][\"html\"][\"code-fold\"] = False\ncfg[\"format\"][\"html\"][\"toc-depth\"] = 2\ncfg[\"format\"][\"html\"][\"page-navigation\"] = True\ncfg[\"format\"][\"html\"][\"code-tools\"] = True\ncfg[\"format\"][\"html\"][\"fig-cap-location\"] = \"bottom\"\ncfg[\"format\"][\"html\"][\"tbl-cap-location\"] = \"top\"\ncfg[\"format\"][\"html\"][\"css\"] = \"docs/style.css\"\n\ncfg.setdefault(\"execute\", {})\ncfg[\"execute\"][\"echo\"] = True\ncfg[\"execute\"][\"warning\"] = False\ncfg[\"execute\"][\"cache\"] = True\n\n# Freeze: deterministic rebuilds until the source changes\n# cfg[\"project\"][\"freeze\"] = \"auto\"\ncfg[\"execyte\"][\"freeze\"] = \"auto\"\n\n# Bibliography\ncfg[\"bibliography\"] = \"references.bib\"\n\n# Ensure navbar has EDA link\nnav = cfg.setdefault(\"website\", {}).setdefault(\"navbar\", {}).setdefault(\"left\", [])\nif not any(item.get(\"href\") == \"reports/eda.qmd\" for item in nav if isinstance(item, dict)):\n    nav.append({\"href\": \"reports/eda.qmd\", \"text\": \"EDA\"})\n\nyaml.dump(cfg, open(\"_quarto.yml\",\"w\"))\nprint(open(\"_quarto.yml\").read())\n\n\n4.6.3 2) Add references.bib (sample entries; students will refine later)\nrefs = r\"\"\"@book{hyndman-fpp3,\n  title = {Forecasting: Principles and Practice},\n  author = {Hyndman, Rob J. and Athanasopoulos, George},\n  edition = {3},\n  year = {2021},\n  url = {https://otexts.com/fpp3/}\n}\n@misc{quarto-docs,\n  title = {Quarto Documentation},\n  author = {{Posit}},\n  year = {2025},\n  url = {https://quarto.org/}\n}\n@misc{yfinance,\n  title = {yfinance: Yahoo! Finance market data downloader},\n  author = {Ran Aroussi},\n  year = {2024},\n  url = {https://github.com/ranaroussi/yfinance}\n}\n\"\"\"\n\nopen(\"references.bib\",\"w\").write(refs)\nprint(open(\"references.bib\").read())\n\n\n\n4.6.4 3) Overwrite reports/eda.qmd with captions, labels, alt text, citations, and cross‑refs\n\nThis replaces the earlier EDA with a hygienic version. Feel free to adjust wording later.\n\nfrom textwrap import dedent\neda = dedent(r\"\"\"\\\n---\ntitle: \"Stock EDA\"\nformat:\n  html:\n    toc: true\n    number-sections: false\nexecute:\n  echo: true\n  warning: false\n  cache: true\nparams:\n  symbol: \"AAPL\"\n  start_date: \"2018-01-01\"\n  end_date: \"\"\n  rolling: 20\n---\n\n&gt; *Educational use only — not trading advice.* Data pulled via **yfinance** [@yfinance].\n\nThis page is **parameterized**; see the **Parameters** section for usage.\n\n## Setup parameters if using Python\n\n::: {#033ce8b4 .cell tags='[\"parameters\"]' execution_count=2}\n``` {.python .cell-code}\n# Default values (overridden by -P at render time)\nSYMBOL = \"AAPL\"\nSTART  = \"2018-01-01\"\nEND    = \"\"\nROLL   =  20\n```\n:::\n\n\n## Setup\n\n::: {#928f962f .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nfrom pathlib import Path\n\n# SYMBOL = params.get(\"symbol\", \"AAPL\")\n# START  = params.get(\"start_date\", \"2018-01-01\")\n# END    = params.get(\"end_date\", \"\")\n# ROLL   = int(params.get(\"rolling\", 20))\nif not END:\n  END = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\n```\n:::\n\n\n## Download and tidy\n\n::: {#f9e5fad4 .cell execution_count=4}\n``` {.python .cell-code}\ntry:\n  data = yf.download(SYMBOL, start=START, end=END, auto_adjust=True, progress=False)\nexcept Exception as e:\n  # Synthetic fallback\n  idx = pd.bdate_range(START, END)\n  rng = np.random.default_rng(42)\n  ret = rng.normal(0, 0.01, len(idx))\n  price = 100 * np.exp(np.cumsum(ret))\n  vol = rng.integers(1e5, 5e6, len(idx))\n  data = pd.DataFrame({\"Close\": price, \"Volume\": vol}, index=idx)\n\ndf = (data.rename(columns=str.lower)[[\"close\",\"volume\"]]\n        .dropna()\n        .assign(log_return=lambda d: np.log(d[\"close\"]).diff()))\ndf[\"roll_mean\"] = df[\"log_return\"].rolling(ROLL, min_periods=ROLL//2).mean()\ndf[\"roll_vol\"]  = df[\"log_return\"].rolling(ROLL, min_periods=ROLL//2).std()\ndf = df.dropna()\n```\n:::\n\n\n## Price over time\n\n\n\nAs shown in **Figure @fig-price**, prices vary over time with changing volatility.\n\n## Return distribution\n\n\n\n**Figure @fig-hist** shows the return distribution; many assets exhibit heavy tails \\[@hyndman-fpp3, pp. 20–21].\n\n## Rolling statistics (window = {params.rolling})\n\n\n\n## Summary table\n\n\n\nSee **Table @tbl-summary** for overall statistics.\n\n## Data dictionary\n\n\n\n## Parameters\n\nThis page accepts parameters: `symbol`, `start_date`, `end_date`, and `rolling`. You can re‑render with:\n\n```\nquarto render reports/eda.qmd \\\\\n  -P symbol:MSFT -P start_date:2019-01-01 -P end_date:2025-08-01 -P rolling:30\n```\n\n## References\n\n\"\"\")\nopen(\"reports/eda.qmd\",\"w\").write(eda)\nprint(\"Wrote reports/eda.qmd with hygiene features.\")\n\n\n4.6.5 4) Add a minimal CSS for readability\nfrom pathlib import Path\nPath(\"docs\").mkdir(exist_ok=True)\ncss = \"\"\"\\\n/* Increase base font and widen code blocks slightly */\nbody { font-size: 1.02rem; }\npre code { white-space: pre-wrap; }\nimg { max-width: 100%; height: auto; }\n\"\"\"\nopen(\"docs/style.css\",\"w\").write(css)\nprint(\"Wrote docs/style.css\")\n\n\n4.6.6 5) Render site to docs/ and preview\n!quarto render --output-dir docs/\nOpen docs/reports/eda.html in the Colab file browser to preview. Confirm:\n\nCaptions under figures, tables titled at top\nCross‑refs like “Figure 1”/“Table 1” clickable\n“References” section at bottom with your 2–3 entries\n\n\n\n4.6.7 6) Commit and push (short‑lived token method)\n!git add _quarto.yml references.bib reports/eda.qmd docs/style.css docs/\n!git commit -m \"chore: report hygiene (captions, cross-refs, alt text, freeze, bibliography, CSS)\"\nfrom getpass import getpass\ntoken = getpass(\"GitHub token (not stored): \")\npush_url = f\"https://{token}@github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n!git push {push_url} HEAD:main\ndel token",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4 — RStudio Quarto cameo + Report Hygiene</span>"
    ]
  },
  {
    "objectID": "lec4.html#wrapup-10-min",
    "href": "lec4.html#wrapup-10-min",
    "title": "4  Session 4 — RStudio Quarto cameo + Report Hygiene",
    "section": "4.7 Wrap‑up (10 min)",
    "text": "4.7 Wrap‑up (10 min)\n\nYour report now has citations, captions, cross‑refs, alt text, and frozen outputs for stable rebuilds.\nRStudio can render the exact same Python‑based .qmd. Teams can mix editors without friction.\nNext: Unix automation and Makefile targets to run reports end‑to‑end.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4 — RStudio Quarto cameo + Report Hygiene</span>"
    ]
  },
  {
    "objectID": "lec4.html#homework-due-before-session-5",
    "href": "lec4.html#homework-due-before-session-5",
    "title": "4  Session 4 — RStudio Quarto cameo + Report Hygiene",
    "section": "4.8 Homework (due before Session 5)",
    "text": "4.8 Homework (due before Session 5)\nGoal: Extend hygiene and add one analytic section—ACF plot—with proper captions/labels/alt text/citations.\n\n4.8.1 Part A — Add an ACF figure with cross‑ref + alt text\nAppend this code chunk to reports/eda.qmd after the “Rolling statistics” section:\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# simple ACF (biased) up to max_lag\nx = df[\"log_return\"].fillna(0.0).values\nx = x - x.mean()\nmax_lag = 20\nacf = []\nfor k in range(1, max_lag+1):\n    num = np.sum(x[:-k] * x[k:])\n    den = np.sum(x * x)\n    acf.append(num/den if den != 0 else 0.0)\n\nfig, ax = plt.subplots(figsize=(6,3))\nax.bar(range(1, max_lag+1), acf)\nax.axhline(0, linewidth=1)\nax.set_xlabel(\"Lag\"); ax.set_ylabel(\"ACF\")\nfig.tight_layout()\n\n\nFigure 4.1\n\n\n\nThen reference it in the prose:\n\nShort‑memory patterns are visible in Figure Figure 4.1 (see also (Hyndman and Athanasopoulos 2021), Chapter 2).\n\n\n\n4.8.2 Part B — Add a monthly returns table with caption + label\nAdd a new section “Monthly returns” with a cross‑ref’d table:\n\n\n\nTable 4.1: {SYMBOL} — Monthly mean of daily log returns\n\n\nmonthly = (df[\"log_return\"]\n           .groupby([df.index.year.rename(\"year\"), df.index.month.rename(\"month\")])\n           .mean()\n           .reset_index()\n           .pivot(index=\"year\", columns=\"month\", values=\"log_return\")\n           .round(4))\nmonthly\n\n\n\nIn text: “See Table Table 4.1 for month‑by‑month averages.”\n\n\n4.8.3 Part C — Add two real citations and tidy your references\n\nReplace the placeholder references with at least two credible sources (textbook, API docs, or peer‑reviewed).\n\nCite them in relevant sections of eda.qmd.\n\nEnsure References renders at the bottom.\n\n(Tip: you can add more @misc{key, title=..., url=...} entries for web docs.)\n\n\n4.8.4 Part D — Verify freeze and caching behavior\n\nIn _quarto.yml, ensure:\n\nexecute:\n  cache: true\n  freeze: auto\nRe‑render once (quarto render --output-dir docs/), note speed.\nChange a small line in eda.qmd and re‑render; confirm only affected chunks rebuild.\n\n\n\n4.8.5 Part E — Commit & push\n!git add reports/eda.qmd references.bib docs/\n!git commit -m \"feat: ACF figure and monthly returns table; references updated\"\nfrom getpass import getpass\ntoken = getpass(\"GitHub token (not stored): \")\npush_url = f\"https://{token}@github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n!git push {push_url} HEAD:main\ndel token\n\n\n4.8.6 Grading (pass/revise)\n\nEDA page includes ACF figure with caption, label, and alt text; cross‑referenced in text.\nMonthly returns table present with caption/label; referenced in text.\nAt least two new, relevant citations included and rendered under References.\nfreeze and cache enabled; site renders to docs/ and loads on GitHub Pages.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4 — RStudio Quarto cameo + Report Hygiene</span>"
    ]
  },
  {
    "objectID": "lec4.html#key-points",
    "href": "lec4.html#key-points",
    "title": "4  Session 4 — RStudio Quarto cameo + Report Hygiene",
    "section": "4.9 Key points",
    "text": "4.9 Key points\n\nAccessibility is part of professionalism: always write alt text, don’t rely on color alone, and keep captions informative.\nCitations are not optional for serious work; treat the report like a short paper.\nFreeze + cache save time and prevent accidental drift.\nRStudio is a comfortable alternative editor for Quarto even in a Python‑only workflow.\n\nNext up (Session 5): Unix for data work—shell power tools and Make automation to glue everything together.\n\n\n\n\nHyndman, Rob J., and George Athanasopoulos. 2021. Forecasting: Principles and Practice. 3rd ed. https://otexts.com/fpp3/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4 — RStudio Quarto cameo + Report Hygiene</span>"
    ]
  },
  {
    "objectID": "lec5.html",
    "href": "lec5.html",
    "title": "5  Session 5 — Unix/Shell Essentials for Data Work",
    "section": "",
    "text": "5.0.1 Learning goals\nBy the end of class, students can:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5 — Unix/Shell Essentials for Data Work</span>"
    ]
  },
  {
    "objectID": "lec5.html#agenda-75-min",
    "href": "lec5.html#agenda-75-min",
    "title": "5  Session 5 — Unix/Shell Essentials for Data Work",
    "section": "5.1 Agenda (75 min)",
    "text": "5.1 Agenda (75 min)\n\n(8 min) Why shell for data science; mental model of pipelines\n(12 min) Core commands & patterns: pipes/redirects, quoting, regex, grep/sed/awk\n(35 min) In‑class lab (Colab): filesystem → CSV pipelines → find/xargs → QA shell script\n(10 min) Wrap‑up, troubleshooting, and homework briefing\n(10 min) Buffer for slow installs / student issues",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5 — Unix/Shell Essentials for Data Work</span>"
    ]
  },
  {
    "objectID": "lec5.html#slides",
    "href": "lec5.html#slides",
    "title": "5  Session 5 — Unix/Shell Essentials for Data Work",
    "section": "5.2 Slides",
    "text": "5.2 Slides\nWhy shell?\n\nFast iteration for data plumbing (ETL glue: Extract,-Transform-Load) and repeatable ops.\nWorks on any POSIX (Portable OS Interface: IEEE strandard for Unix-like system) host (your laptop, Colab VM, servers).\nLets you compose small tools with pipes: producer | filter | summarize &gt; report.txt.\n\nMental model\n\nStream text through commands. Each command reads STDIN, writes STDOUT; | connects them.\nRedirection: &gt; (truncate to file), &gt;&gt; (append), &lt; (read from file), 2&gt; (errors).\nExit code: 0 success; non‑zero = error. Use && (only if success) and || (if failure).\n\nQuoting\n\n\"double quotes\" expand variables and backslashes;\n'single quotes' are literal (best for regex/cut/sed patterns);\nAlways quote paths that might contain spaces: \"$FILE\".\n\nRegex quick guide\n\n^ start, $ end, . any char, * 0+, + 1+, ? 0/1, [A-Z] class, (foo|bar) alt.\nUse grep -E (Extended Regex) for + and |. Plain grep is basic (Basic re).\n\nCSV caution\n\nUnix tools are line‑oriented. They’re fine for simple CSVs (no embedded commas/quotes).\nFor tricky CSVs, prefer Python/pandas. Today’s examples are simple CSVs.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5 — Unix/Shell Essentials for Data Work</span>"
    ]
  },
  {
    "objectID": "lec5.html#inclass-lab-35-min",
    "href": "lec5.html#inclass-lab-35-min",
    "title": "5  Session 5 — Unix/Shell Essentials for Data Work",
    "section": "5.3 In‑class lab (35 min)",
    "text": "5.3 In‑class lab (35 min)\n\nInstructor tip: run these as separate Colab cells. Cells labeled “Bash” use %%bash. Cells labeled “Python” are only used to generate a small synthetic CSV we can play with offline (no API keys needed).\n\n\n5.3.1 0) Mount Drive, set repo paths, and cd into the repo\nPython (Colab)\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_OWNER = \"YOUR_GITHUB_USERNAME_OR_ORG\"   # &lt;- change for your class\nREPO_NAME  = \"unified-stocks-teamX\"          # &lt;- change\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport pathlib, os, subprocess\npathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\nif not pathlib.Path(REPO_DIR).exists():\n    print(\"Repo not found in Drive; please clone it in Session 2/3 or adjust REPO_DIR.\")\nelse:\n    os.chdir(REPO_DIR)\n    print(\"Working dir:\", os.getcwd())\nBash\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/STAT4160\"\npwd\nls -la\n\n\n5.3.2 1) Create a small synthetic prices CSV to work with (safe & reproducible)\nPython\n# Generates data/raw/prices.csv with columns: ticker,date,adj_close,volume,log_return\nimport pandas as pd, numpy as np, os\nfrom pathlib import Path\n\nPath(\"data/raw\").mkdir(parents=True, exist_ok=True)\ntickers = pd.read_csv(\"tickers_25.csv\")[\"ticker\"].tolist() if os.path.exists(\"tickers_25.csv\") else [\n    \"AAPL\",\"MSFT\",\"AMZN\",\"GOOGL\",\"META\",\"NVDA\",\"TSLA\",\"JPM\",\"JNJ\",\"V\",\n    \"PG\",\"HD\",\"BAC\",\"XOM\",\"CVX\",\"PFE\",\"KO\",\"DIS\",\"NFLX\",\"INTC\",\"CSCO\",\"ORCL\",\"T\",\"VZ\",\"WMT\"\n]\ndates = pd.bdate_range(\"2020-01-01\", periods=180)  # ~ 9 months\nrng = np.random.default_rng(7)\n\nframes=[]\nfor t in tickers:\n    r = rng.normal(0, 0.01, len(dates))\n    price = 100*np.exp(np.cumsum(r))\n    vol = rng.integers(1e5, 5e6, len(dates))\n    df = pd.DataFrame({\"ticker\": t, \"date\": dates, \"adj_close\": price, \"volume\": vol})\n    df[\"log_return\"] = np.log(df[\"adj_close\"]).diff().fillna(0)\n    frames.append(df)\n\nout = pd.concat(frames, ignore_index=True)\nout.to_csv(\"data/raw/prices.csv\", index=False)\nout.head()\n\n\n5.3.3 2) Pipes & redirects warm‑up\nBash\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\n\n# How many lines? (including header)\nwc -l data/raw/prices.csv | tee reports/prices_wc.txt\n\n# First 5 lines, save to a sample\nhead -n 5 data/raw/prices.csv | tee data/raw/prices_sample.csv\n\n# Show ticker column only (field 1), excluding header\ncut -d, -f1 data/raw/prices.csv | tail -n +2 | head -n 10\n\n\n5.3.4 3) grep filters (basic and extended) + regex\nBash\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\n\n# All rows for NVDA OR MSFT (extended regex with alternation)\ngrep -E '^(NVDA|MSFT),' data/raw/prices.csv | head -n 3\n\n# Rows where ticker starts with a vowel (A, E, I, O, U)\ngrep -E '^(A|E|I|O|U)[A-Z]*,' data/raw/prices.csv | head -n 3\n\n# Count rows per ticker quickly (just for demo)\ncut -d, -f1 data/raw/prices.csv | tail -n +2 | sort | uniq -c | head\n\n\n5.3.5 4) sed transformations (search/replace; in‑place edits)\nBash\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\n\n# Make a copy so we don't touch the raw file\ncp data/raw/prices.csv data/interim/ || mkdir -p data/interim && cp data/raw/prices.csv data/interim\ncp data/interim/prices.csv data/interim/prices_copy.csv\n\n# Replace ISO date dashes with slashes (2020-01-02 -&gt; 2020/01/02) in-place\nsed -i '1!s/\\([0-9]\\{4\\}\\)-\\([0-9]\\{2\\}\\)-\\([0-9]\\{2\\}\\)/\\1\\/\\2\\/\\3/g' data/interim/prices_copy.csv\n\n# Normalize ticker to lowercase (first column) using sed's capture groups and tolower via awk (hybrid example)\nhead -n 1 data/interim/prices_copy.csv &gt; data/interim/prices_lower.csv\ntail -n +2 data/interim/prices_copy.csv | awk -F, 'BEGIN{OFS=\",\"}{ $1=tolower($1); print }' &gt;&gt; data/interim/prices_lower.csv\n\nhead -n 3 data/interim/prices_lower.csv\n\n\n5.3.6 5) awk for CSV summarization\nBash\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\n\n# Compute mean log_return per ticker (skip header). -F, sets comma as field separator.\nawk -F, 'NR&gt;1 { sum[$1]+=$5; n[$1]++ } END { OFS=\",\"; print \"ticker\",\"mean_log_return\"; for (t in sum) print t, sum[t]/n[t] }' data/raw/prices.csv \\\n| sort -t, -k2,2nr | head\n\n# Top 5 dates with highest absolute log_return for NVDA\nawk -F, 'NR&gt;1 && $1==\"NVDA\" { print $2, $5 }' data/raw/prices.csv \\\n| awk '{ if ($2&lt;0) s=-$2; else s=$2; print $1\",\"$2\",\"s }' \\\n| sort -t, -k3,3nr | head -n 5\n\nNR&gt;1 skips header.\nsum[$1] and n[$1] are associative arrays keyed by ticker.\nWe sort numerically on column 2 (mean) with -k2,2n or nr for descending.\n\n\n\n5.3.7 6) sort | uniq deduping and comm to compare lists\nBash\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\n\n# Unique tickers actually present in the file\ncut -d, -f1 data/raw/prices.csv | tail -n +2 | sort | uniq &gt; data/interim/tickers_in_data.txt\n\n# Compare to our canonical list from tickers_25.csv\ncut -d, -f1 tickers_25.csv | tail -n +2 | sort &gt; data/interim/tickers_25.txt\n\necho \"Only in data:\"; comm -23 data/interim/tickers_in_data.txt data/interim/tickers_25.txt | sed 's/^/  /'\necho \"Only in canonical:\"; comm -13 data/interim/tickers_in_data.txt data/interim/tickers_25.txt | sed 's/^/  /'\n\n\n5.3.8 7) find and xargs (safe, null‑terminated)\nBash\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\n\n# Show all CSVs under data/, printing sizes\nfind data -type f -name \"*.csv\" -printf \"%p,%s bytes\\n\" | sort | head\n\n# Count lines in each CSV (null-safe for weird filenames)\nfind data -type f -name \"*.csv\" -print0 | xargs -0 -I{} sh -c 'echo -n \"{},\"; wc -l &lt; \"{}\"'\n\n# Gzip-compress any CSV larger than ~1MB (demo threshold: 1e6 bytes)\nfind data -type f -name \"*.csv\" -size +1000k -print0 | xargs -0 -I{} gzip -kf \"{}\"  # -k keeps original\n\nPattern: prefer -print0 | xargs -0 to safely handle spaces/newlines in filenames.\n\n\n\n5.3.9 8) Build a defensive CSV QA script and run it\nBash\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\nmkdir -p scripts\n\ncat &gt; scripts/qa_csv.sh &lt;&lt; 'EOF'\n#!/usr/bin/env bash\n# Simple CSV health check\n# Usage: scripts/qa_csv.sh path/to/file.csv required_columns_csv\nset -euo pipefail\nIFS=$'\\n\\t'\n\nFILE=\"${1:-}\"\nREQUIRED=\"${2:-ticker,date,adj_close,volume,log_return}\"\n\nerr() { echo \"ERROR: $*\" &gt;&2; exit 1; }\n[[ -z \"$FILE\" ]] && err \"No CSV file provided.\"\n[[ ! -f \"$FILE\" ]] && err \"File not found: $FILE\"\n\n# 1) Non-empty and header present\nLINES=$(wc -l &lt; \"$FILE\" || true)\n[[ \"${LINES:-0}\" -lt 2 ]] && err \"File has &lt;2 lines (missing data?): $FILE\"\n\nHEADER=$(head -n 1 \"$FILE\")\n# 2) All required columns present\nIFS=',' read -r -a req &lt;&lt;&lt; \"$REQUIRED\"\nfor col in \"${req[@]}\"; do\n  echo \"$HEADER\" | grep -q -E \"(^|,)${col}(,|$)\" || err \"Missing required column: $col\"\ndone\n\n# 3) No obvious NA/blank values in required numeric cols (basic check)\nNUMERIC=\"adj_close,volume,log_return\"\nIFS=',' read -r -a nums &lt;&lt;&lt; \"$NUMERIC\"\nfor col in \"${nums[@]}\"; do\n  # find column index\n  idx=$(awk -F, -v COL=\"$col\" 'NR==1{for(i=1;i&lt;=NF;i++) if($i==COL) print i}' \"$FILE\")\n  [[ -z \"${idx:-}\" ]] && err \"Column not found: $col\"\n  # check any blank values from row 2 onward\n  bad=$(awk -F, -v I=\"$idx\" 'NR&gt;1 && ($I==\"\" || $I==\"NA\") {c++} END{print c+0}' \"$FILE\")\n  [[ \"$bad\" -gt 0 ]] && err \"Found $bad blank/NA in column: $col\"\ndone\n\necho \"OK: $FILE passed basic CSV QA ($LINES lines).\"\nEOF\n\nchmod +x scripts/qa_csv.sh\nRun the QA script\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\nscripts/qa_csv.sh data/raw/prices.csv\n\nIntentionally break it (optional): open data/raw/prices.csv, blank out a value, and re‑run to watch it fail with non‑zero exit code.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5 — Unix/Shell Essentials for Data Work</span>"
    ]
  },
  {
    "objectID": "lec5.html#wrapup-10-min",
    "href": "lec5.html#wrapup-10-min",
    "title": "5  Session 5 — Unix/Shell Essentials for Data Work",
    "section": "5.4 Wrap‑up (10 min)",
    "text": "5.4 Wrap‑up (10 min)\n\nShell is about composable building blocks. Learn 15 commands deeply; combine them fluently.\nPrefer null‑safe find … -print0 | xargs -0 patterns; always quote variables: \"$FILE\".\nFor complex CSV logic, fall back to Python; but shell shines for quick filters and QA.\nWe’ll hook these into Make next session so one command runs your whole pipeline.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5 — Unix/Shell Essentials for Data Work</span>"
    ]
  },
  {
    "objectID": "lec5.html#homework-due-before-session-6",
    "href": "lec5.html#homework-due-before-session-6",
    "title": "5  Session 5 — Unix/Shell Essentials for Data Work",
    "section": "5.5 Homework (due before Session 6)",
    "text": "5.5 Homework (due before Session 6)\nGoal: Practice and codify shell workflows into your project: (1) a data stats pipeline, (2) a per‑ticker split utility, (3) a Makefile target, and (4) a short shell‑only EDA text report.\n\n5.5.1 Part A — Data stats pipeline (one‑liners saved to files)\nBash (Colab)\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\nmkdir -p reports data/interim\n\n# 1) Count lines and unique tickers\n{ \n  echo \"Lines (incl header): $(wc -l &lt; data/raw/prices.csv)\";\n  echo \"Unique tickers: $(cut -d, -f1 data/raw/prices.csv | tail -n +2 | sort | uniq | wc -l)\";\n} | tee reports/data_counts.txt\n\n# 2) Top-10 days by absolute log_return across all tickers\ntail -n +2 data/raw/prices.csv \\\n| awk -F, '{a=$5; if(a&lt;0) a=-a; print $1\",\"$2\",\"$5\",\"a}' \\\n| sort -t, -k4,4nr | head -n 10 \\\n| tee reports/top10_abs_moves.csv\n\n# 3) Mean log_return per ticker (CSV)\nawk -F, 'NR&gt;1 { s[$1]+=$5; n[$1]++ } END { OFS=\",\"; print \"ticker,mean_log_return\"; for(t in s) print t, s[t]/n[t] }' \\\n  data/raw/prices.csv | sort -t, -k2,2nr | tee reports/mean_return_by_ticker.csv\n\n\n5.5.2 Part B — Split per‑ticker CSVs into data/interim/ticker=XYZ/ directories\nBash\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\nmkdir -p data/interim\n\n# Extract header once\nHEADER=$(head -n 1 data/raw/prices.csv)\n\n# Create per-ticker files with header + rows (null-safe not necessary here)\ncut -d, -f1 data/raw/prices.csv | tail -n +2 | sort | uniq | while read -r T; do\n  mkdir -p \"data/interim/ticker=${T}\"\n  {\n    echo \"$HEADER\"\n    awk -F, -v TK=\"$T\" 'NR==1 || $1==TK' data/raw/prices.csv\n  } &gt; \"data/interim/ticker=${T}/prices_${T}.csv\"\ndone\n\n# Verify one example\nls -la data/interim/ticker=AAPL | head\n\n\n5.5.3 Part C — Add Makefile targets for QA and per‑ticker split\nBash\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\n\n# Append or create a Makefile\n{\n  echo \"\"\n  echo \"qa:\"\n  echo \"\\tscripts/qa_csv.sh data/raw/prices.csv\"\n  echo \"\"\n  echo \"split-by-ticker:\"\n  echo \"\\tbash -c 'HEADER=\\$(head -n 1 data/raw/prices.csv); cut -d, -f1 data/raw/prices.csv | tail -n +2 | sort | uniq | while read -r T; do mkdir -p data/interim/ticker=\\$\\$T; { echo \\\"\\$\\$HEADER\\\"; awk -F, -v TK=\\\"\\$\\$T\\\" '\\\"'NR==1 || \\$1==TK'\\\"' data/raw/prices.csv; } &gt; data/interim/ticker=\\$\\$T/prices_\\$\\$T.csv; done'\"\n} &gt;&gt; Makefile\n\ncat Makefile\nRun the targets\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\nmake qa\nmake split-by-ticker\n\n\n5.5.4 Part D — Shell‑only mini EDA report\nCreate reports/mini_eda.txt with three sections: counts, top moves, mean returns.\nBash\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\n\n{\n  echo \"# Mini EDA (shell-only)\"\n  echo \"Generated: $(date)\"\n  echo\n  echo \"## Counts\"\n  echo \"Lines (incl header): $(wc -l &lt; data/raw/prices.csv)\"\n  echo \"Unique tickers: $(cut -d, -f1 data/raw/prices.csv | tail -n +2 | sort | uniq | wc -l)\"\n  echo\n  echo \"## Top 5 absolute daily moves\"\n  tail -n +2 data/raw/prices.csv \\\n  | awk -F, '{a=$5; if(a&lt;0) a=-a; print $1\",\"$2\",\"$5\",\"a}' \\\n  | sort -t, -k4,4nr | head -n 5\n  echo\n  echo \"## Mean log_return by ticker (top 10)\"\n  awk -F, 'NR&gt;1 { s[$1]+=$5; n[$1]++ } END { for(t in s) printf \"%s,%.6f\\n\", t, s[t]/n[t] }' \\\n    data/raw/prices.csv | sort -t, -k2,2nr | head -n 10\n} | tee reports/mini_eda.txt\n\n\n5.5.5 Part E — Commit & push your changes (use your short‑lived token as in Session 2)\nBash + Python (getpass)\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\ngit add scripts/qa_csv.sh data/raw/prices.csv data/interim reports/*.txt reports/*.csv Makefile\ngit status\ngit commit -m \"feat(shell): data QA script, per-ticker split, shell mini-EDA, Make targets\"\nfrom getpass import getpass\nimport os, subprocess\ntoken = getpass(\"GitHub token (not stored): \")\nREPO_OWNER = \"YOUR_GITHUB_USERNAME_OR_ORG\"\nREPO_NAME  = \"unified-stocks-teamX\"\npush_url = f\"https://{token}@github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n# Push current HEAD to main (adjust if you prefer a branch + PR)\nsubprocess.run([\"git\", \"push\", push_url, \"HEAD:main\"], check=True)\ndel token\n\n\n5.5.6 Grading (pass/revise)\n\nscripts/qa_csv.sh exists, is executable, and fails on malformed CSV, passes on clean CSV.\nreports/data_counts.txt, reports/top10_abs_moves.csv, reports/mean_return_by_ticker.csv, and reports/mini_eda.txt generated.\nmake qa and make split-by-ticker run successfully.\nPer‑ticker CSVs created under data/interim/ticker=XYZ/.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5 — Unix/Shell Essentials for Data Work</span>"
    ]
  },
  {
    "objectID": "lec5.html#key-points",
    "href": "lec5.html#key-points",
    "title": "5  Session 5 — Unix/Shell Essentials for Data Work",
    "section": "5.6 Key points",
    "text": "5.6 Key points\n\nQuote variables and paths. Prefer -print0 | xargs -0 with find.\nFail fast in scripts (set -euo pipefail) and return non‑zero exit codes for CI (Continuous Integraion).\nShell is for plumbing—it complements, not replaces, Python.\n\nNext session (6): “Make/just, rsync, ssh/tmux (survey)” and we’ll wire make get-data and make report into a reproducible one‑command pipeline.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5 — Unix/Shell Essentials for Data Work</span>"
    ]
  },
  {
    "objectID": "lec6.html",
    "href": "lec6.html",
    "title": "6  Session 6 — Make/Automation + rsync + ssh/tmux (survey)",
    "section": "",
    "text": "6.1 Session 6 — Make/Automation + rsync + ssh/tmux (75 min)\nAssumptions: You’re using the same Drive‑mounted repo from prior sessions (e.g., unified-stocks-teamX). No trading advice—this is educational.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6 — Make/Automation + rsync + ssh/tmux (survey)</span>"
    ]
  },
  {
    "objectID": "lec6.html#session-6-makeautomation-rsync-sshtmux-75-min",
    "href": "lec6.html#session-6-makeautomation-rsync-sshtmux-75-min",
    "title": "6  Session 6 — Make/Automation + rsync + ssh/tmux (survey)",
    "section": "",
    "text": "6.1.1 Learning goals\nStudents will be able to:\n\nExplain how Make turns scripts into a reproducible pipeline via targets, dependencies, and incremental builds.\nCreate and use a Makefile with helpful defaults, variables, and a help target.\nUse rsync to back up project artifacts and understand --delete and exclude patterns.\nUnderstand the ssh key flow and a tmux workflow for long‑running jobs (survey).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6 — Make/Automation + rsync + ssh/tmux (survey)</span>"
    ]
  },
  {
    "objectID": "lec6.html#agenda-75-min",
    "href": "lec6.html#agenda-75-min",
    "title": "6  Session 6 — Make/Automation + rsync + ssh/tmux (survey)",
    "section": "6.2 Agenda (75 min)",
    "text": "6.2 Agenda (75 min)\n\n(12 min) Slides: Why automation? How Make models dependencies & incremental builds; best practices\n(10 min) Slides: rsync fundamentals; ssh keys & config; tmux workflow (survey)\n(33 min) In‑class lab (Colab): create scripts → author Makefile → run make all → rsync backup\n(10 min) Wrap‑up & troubleshooting\n(10 min) Buffer",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6 — Make/Automation + rsync + ssh/tmux (survey)</span>"
    ]
  },
  {
    "objectID": "lec6.html#slides",
    "href": "lec6.html#slides",
    "title": "6  Session 6 — Make/Automation + rsync + ssh/tmux (survey)",
    "section": "6.3 Slides",
    "text": "6.3 Slides\n\n6.3.1 Why Make for DS pipelines?\n\nEncodes your workflow as targets that depend on files or other targets.\nIncremental: only rebuilds what changed.\nPlays nicely with CI (make all from a clean clone).\nStable across OSes; no new runtime to learn.\n\nCore syntax\ntarget: dependencies\n&lt;TAB&gt; recipe commands\n\nUse variables: PY := python, QUARTO := quarto.\nUse PHONY for meta‑targets that don’t produce files.\nPrefer deterministic outputs: fixed seeds, pinned versions, stable paths.\n\n\n\n6.3.2 rsync basics\n\nrsync -avh SRC/ DST/ → syncs directory trees, preserving metadata.\n--delete makes DST exactly match SRC (removes files not in SRC).\n--exclude to skip folders (--exclude 'raw/').\nRemote with SSH: rsync -avz -e ssh SRC/ user@host:/path/.\n\n\n\n6.3.3 ssh keys & tmux (survey)\n\nKeys: ssh-keygen -t ed25519 -C \"you@school.edu\"; add the public key to servers/GitHub; keep private key private.\n~/.ssh/config holds named hosts; ssh myhpc uses that stanza.\ntmux: start tmux new -s train; detach (Ctrl-b d); list (tmux ls); reattach (tmux attach -t train). Keeps jobs alive on remote shells.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6 — Make/Automation + rsync + ssh/tmux (survey)</span>"
    ]
  },
  {
    "objectID": "lec6.html#inclass-lab-33-min",
    "href": "lec6.html#inclass-lab-33-min",
    "title": "6  Session 6 — Make/Automation + rsync + ssh/tmux (survey)",
    "section": "6.4 In‑class lab (33 min)",
    "text": "6.4 In‑class lab (33 min)\nWe’ll create three tiny scripts and a Makefile that ties them together:\n\nscripts/get_prices.py → data/raw/prices.csv (Yahoo via yfinance, with synthetic fallback)\nscripts/build_features.py → data/processed/features.parquet\nscripts/backup.sh → rsync your artifacts to backups/&lt;timestamp&gt;/\nMakefile → make all runs end‑to‑end; make report renders Quarto; make backup syncs artifacts.\n\n\n6.4.1 0) Mount Drive and set repo path\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_OWNER = \"YOUR_GITHUB_USERNAME_OR_ORG\"  # &lt;- change\nREPO_NAME  = \"unified-stocks-teamX\"         # &lt;- change\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport pathlib, os, subprocess\npathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\nif not pathlib.Path(REPO_DIR).exists():\n    raise SystemExit(\"Repo not found in Drive. Clone it first (see Session 2/3).\")\nos.chdir(REPO_DIR)\nprint(\"Working dir:\", os.getcwd())\n\n\n6.4.2 1) Quick tool checks (Make, rsync, Quarto)\nimport subprocess, shutil\ndef check(cmd):\n    try:\n        out = subprocess.check_output(cmd, text=True)\n        print(cmd[0], \"OK\")\n    except Exception as e:\n        print(cmd[0], \"NOT FOUND\")\ncheck([\"make\", \"--version\"])\ncheck([\"rsync\", \"--version\"])\ncheck([\"quarto\", \"--version\"])\nIf Quarto is missing, re-run the installer from Session 3 before make report.\n\n\n6.4.3 2) Script: scripts/get_prices.py\nfrom pathlib import Path\nPath(\"scripts\").mkdir(exist_ok=True)\n\nget_py = r\"\"\"#!/usr/bin/env python\nimport argparse, sys, time\nfrom pathlib import Path\nimport pandas as pd, numpy as np\n\ndef fetch_yf(ticker, start, end):\n    import yfinance as yf\n    df = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n    if df is None or df.empty:\n        raise RuntimeError(\"empty\")\n    df = df.rename(columns=str.lower)[[\"close\",\"volume\"]]\n    df.index.name = \"date\"\n    df = df.reset_index()\n    df[\"ticker\"] = ticker\n    return df[[\"ticker\",\"date\",\"close\",\"volume\"]]\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--tickers\", default=\"tickers_25.csv\")\n    ap.add_argument(\"--start\", default=\"2020-01-01\")\n    ap.add_argument(\"--end\", default=\"\")\n    ap.add_argument(\"--out\", default=\"data/raw/prices.csv\")\n    args = ap.parse_args()\n\n    out = Path(args.out)\n    out.parent.mkdir(parents=True, exist_ok=True)\n    tickers = pd.read_csv(args.tickers)[\"ticker\"].dropna().unique().tolist()\n\n    rows = []\n    for t in tickers:\n        try:\n            df = fetch_yf(t, args.start, args.end or None)\n        except Exception:\n            # synthetic fallback\n            idx = pd.bdate_range(args.start, args.end or pd.Timestamp.today().date())\n            rng = np.random.default_rng(42 + hash(t)%1000)\n            r = rng.normal(0, 0.01, len(idx))\n            price = 100*np.exp(np.cumsum(r))\n            vol = rng.integers(1e5, 5e6, len(idx))\n            df = pd.DataFrame({\"ticker\": t, \"date\": idx, \"close\": price, \"volume\": vol})\n        df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n        df[\"adj_close\"] = df[\"close\"]\n        df = df.drop(columns=[\"close\"])\n        df[\"log_return\"] = np.log(df[\"adj_close\"]).diff().fillna(0.0)\n        rows.append(df)\n\n    allp = pd.concat(rows, ignore_index=True)\n    allp = allp[[\"ticker\",\"date\",\"adj_close\",\"volume\",\"log_return\"]]\n    allp.to_csv(out, index=False)\n    print(\"Wrote\", out, \"rows:\", len(allp))\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n\"\"\"\nopen(\"scripts/get_prices.py\",\"w\").write(get_py)\nimport os, stat\nos.chmod(\"scripts/get_prices.py\", os.stat(\"scripts/get_prices.py\").st_mode | stat.S_IEXEC)\nprint(\"Created scripts/get_prices.py\")\n\n\n6.4.4 3) Script: scripts/build_features.py\nfeat_py = r\"\"\"#!/usr/bin/env python\nimport argparse\nfrom pathlib import Path\nimport pandas as pd, numpy as np\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--input\", default=\"data/raw/prices.csv\")\n    ap.add_argument(\"--out\", default=\"data/processed/features.parquet\")\n    ap.add_argument(\"--roll\", type=int, default=20)\n    args = ap.parse_args()\n\n    df = pd.read_csv(args.input, parse_dates=[\"date\"])\n    df = df.sort_values([\"ticker\",\"date\"])\n    # groupwise lags\n    df[\"r_1d\"] = df[\"log_return\"]\n    for k in (1,2,3):\n        df[f\"lag{k}\"] = df.groupby(\"ticker\")[\"r_1d\"].shift(k)\n    df[\"roll_mean\"] = (df.groupby(\"ticker\")[\"r_1d\"]\n                         .rolling(args.roll, min_periods=args.roll//2).mean()\n                         .reset_index(level=0, drop=True))\n    df[\"roll_std\"]  = (df.groupby(\"ticker\")[\"r_1d\"]\n                         .rolling(args.roll, min_periods=args.roll//2).std()\n                         .reset_index(level=0, drop=True))\n    out = Path(args.out)\n    out.parent.mkdir(parents=True, exist_ok=True)\n    # Save compactly\n    df.to_parquet(out, index=False)\n    print(\"Wrote\", out, \"rows:\", len(df))\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\nopen(\"scripts/build_features.py\",\"w\").write(feat_py)\nimport os, stat\nos.chmod(\"scripts/build_features.py\", os.stat(\"scripts/build_features.py\").st_mode | stat.S_IEXEC)\nprint(\"Created scripts/build_features.py\")\n\n\n6.4.5 4) Script: scripts/backup.sh (rsync)\nbackup_sh = r\"\"\"#!/usr/bin/env bash\n# Sync selected artifacts to backups/&lt;timestamp&gt; using rsync.\n# Usage: scripts/backup.sh [DEST_ROOT]\nset -euo pipefail\nROOT=\"${1:-backups}\"\nSTAMP=\"$(date +%Y%m%d-%H%M%S)\"\nDEST=\"${ROOT}/run-${STAMP}\"\nmkdir -p \"$DEST\"\n\n# What to back up (adjust as needed)\nINCLUDE=(\"data/processed\" \"reports\" \"docs\")\n\nfor src in \"${INCLUDE[@]}\"; do\n  if [[ -d \"$src\" ]]; then\n    echo \"Syncing $src -&gt; $DEST/$src\"\n    rsync -avh --delete --exclude 'raw/' --exclude 'interim/' \"$src\"/ \"$DEST/$src\"/\n  fi\ndone\n\necho \"Backup complete at $DEST\"\n\"\"\"\nopen(\"scripts/backup.sh\",\"w\").write(backup_sh)\nimport os, stat\nos.chmod(\"scripts/backup.sh\", os.stat(\"scripts/backup.sh\").st_mode | stat.S_IEXEC)\nprint(\"Created scripts/backup.sh\")\n\n\n6.4.6 5) Makefile (robust, with variables and help)\nmakefile = r\"\"\"# Makefile — unified-stocks\nSHELL := /bin/bash\n.SHELLFLAGS := -eu -o pipefail -c\n\nPY := python\nQUARTO := quarto\n\nSTART ?= 2020-01-01\nEND   ?= 2025-08-01\nROLL  ?= 30\n\nDATA_RAW := data/raw/prices.csv\nFEATS    := data/processed/features.parquet\nREPORT   := docs/reports/eda.html\n\n# Default target\n.DEFAULT_GOAL := help\n\n.PHONY: help all clean clobber qa report backup\n\nhelp: ## Show help for each target\n    @awk 'BEGIN {FS = \":.*##\"; printf \"Available targets:\\n\"} /^[a-zA-Z0-9_\\-]+:.*##/ {printf \"  \\033[36m%-18s\\033[0m %s\\n\", $$1, $$2}' $(MAKEFILE_LIST)\n\nall: $(DATA_RAW) $(FEATS) report backup ## Run the full pipeline and back up artifacts\n\n$(DATA_RAW): scripts/get_prices.py tickers_25.csv\n    $(PY) scripts/get_prices.py --tickers tickers_25.csv --start $(START) --end $(END) --out $(DATA_RAW)\n\n$(FEATS): scripts/build_features.py $(DATA_RAW) scripts/qa_csv.sh\n    # Basic QA first\n    scripts/qa_csv.sh $(DATA_RAW)\n    $(PY) scripts/build_features.py --input $(DATA_RAW) --out $(FEATS) --roll $(ROLL)\n\nreport: $(REPORT) ## Render Quarto EDA to docs/\n$(REPORT): reports/eda.qmd _quarto.yml docs/style.css\n    $(QUARTO) render reports/eda.qmd -P symbol:AAPL -P start_date=$(START) -P end_date=$(END) -P rolling=$(ROLL) --output-dir docs/\n    @test -f $(REPORT) || (echo \"Report not generated.\" && exit 1)\n\nbackup: ## Rsync selected artifacts to backups/&lt;timestamp&gt;/\n    ./scripts/backup.sh\n\nclean: ## Remove intermediate artifacts (safe)\n    rm -rf data/interim\n    rm -rf data/processed/*.parquet || true\n\nclobber: clean ## Remove generated reports and backups (dangerous)\n    rm -rf docs/reports || true\n    rm -rf backups || true\n\"\"\"\nopen(\"Makefile\",\"w\").write(makefile)\nprint(open(\"Makefile\").read())\n\nNote: The Makefile expects scripts/qa_csv.sh from Session 5. If a student missed it, set scripts/qa_csv.sh to a no‑op or remove that dependency temporarily.\n\n\n\n6.4.7 6) Try the pipeline\nimport subprocess, os, textwrap, sys\nprint(subprocess.check_output([\"make\", \"help\"], text=True))\n# Fetch raw, build features, render report, back up artifacts\nimport subprocess\nprint(subprocess.check_output([\"make\", \"all\"], text=True))\nConfirm:\n\ndata/raw/prices.csv exists\ndata/processed/features.parquet exists\ndocs/reports/eda.html renders\nbackups/run-&lt;timestamp&gt;/ contains synced folders\n\n\n\n6.4.8 7) (Optional) just command‑runner\n\nOptional: If just is available on your system, create a justfile that mirrors common Make targets. On Colab, installation may or may not be available; this is just for reference.\n\n%%bash\nset -e\nif ! command -v just &gt;/dev/null 2&gt;&1; then\n  echo \"just not found; skipping optional step.\"\n  exit 0\nfi\ncat &gt; justfile &lt;&lt; 'EOF'\n# justfile — optional convenience recipes\nset shell := [\"bash\", \"-eu\", \"-o\", \"pipefail\", \"-c\"]\n\nstart := \"2020-01-01\"\nend   := \"2025-08-01\"\nroll  := \"30\"\n\nhelp:\n\\t@echo \"Recipes: get-data, features, report, all, backup\"\n\nget-data:\n\\tpython scripts/get_prices.py --tickers tickers_25.csv --start {{start}} --end {{end}} --out data/raw/prices.csv\n\nfeatures:\n\\tbash -lc 'scripts/qa_csv.sh data/raw/prices.csv'\n\\tpython scripts/build_features.py --input data/raw/prices.csv --out data/processed/features.parquet --roll {{roll}}\n\nreport:\n\\tquarto render reports/eda.qmd -P symbol:AAPL -P start_date={{start}} -P end_date={{end}} -P rolling:{{roll}} --output-dir docs/\n\nall: get-data features report\n\nbackup:\n\\t./scripts/backup.sh\nEOF\necho \"Wrote justfile (optional).\"\n\n\n6.4.9 8) ssh & tmux quickstarts (survey, run locally, not in Colab)\nssh key generation (local terminal):\nssh-keygen -t ed25519 -C \"you@school.edu\"\n# Press enter to accept default path (~/.ssh/id_ed25519), set a passphrase (recommended)\ncat ~/.ssh/id_ed25519.pub   # copy this PUBLIC key where needed (GitHub/servers)\nSSH config (~/.ssh/config, local):\nHost github\n  HostName github.com\n  User git\n  IdentityFile ~/.ssh/id_ed25519\n  AddKeysToAgent yes\n  IdentitiesOnly yes\n\nHost myhpc\n  HostName login.hpc.university.edu\n  User your_netid\n  IdentityFile ~/.ssh/id_ed25519\nTest GitHub SSH (local):\nssh -T git@github.com\ntmux essentials (remote or local):\ntmux new -s train              # start session \"train\"\n# ... run your long job ...\n# detach: press Ctrl-b then d\ntmux ls                        # list sessions\ntmux attach -t train           # reattach\ntmux kill-session -t train     # end session",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6 — Make/Automation + rsync + ssh/tmux (survey)</span>"
    ]
  },
  {
    "objectID": "lec6.html#wrapup-10-min",
    "href": "lec6.html#wrapup-10-min",
    "title": "6  Session 6 — Make/Automation + rsync + ssh/tmux (survey)",
    "section": "6.5 Wrap‑up (10 min)",
    "text": "6.5 Wrap‑up (10 min)\n\nMake codifies your pipeline; the file graph serves as your dependency DAG.\nIncremental builds save time: edit one script → only downstream targets rebuild.\nrsync is your friend for backups/snapshots; be deliberate with --delete.\nssh/tmux: you don’t need them in Colab, but you will on servers/HPC.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6 — Make/Automation + rsync + ssh/tmux (survey)</span>"
    ]
  },
  {
    "objectID": "lec6.html#homework-due-before-session-7",
    "href": "lec6.html#homework-due-before-session-7",
    "title": "6  Session 6 — Make/Automation + rsync + ssh/tmux (survey)",
    "section": "6.6 Homework (due before Session 7)",
    "text": "6.6 Homework (due before Session 7)\nGoal: Extend your automation with a tiny baseline training & evaluation step and polish your Makefile.\n\n6.6.1 Part A — Add a minimal baseline trainer\nCreate scripts/train_baseline.py that learns a linear regression on lagged returns (toy baseline) and writes metrics.\ntrain_py = r\"\"\"#!/usr/bin/env python\nimport argparse, json\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--features\", default=\"data/processed/features.parquet\")\n    ap.add_argument(\"--out-metrics\", default=\"reports/baseline_metrics.json\")\n    args = ap.parse_args()\n\n    df = pd.read_parquet(args.features)\n    # Train/test split by date (last 20% for test)\n    df = df.dropna(subset=[\"lag1\",\"lag2\",\"lag3\",\"r_1d\"])\n    n = len(df)\n    split = int(n*0.8)\n    Xtr = df[[\"lag1\",\"lag2\",\"lag3\"]].iloc[:split].values\n    ytr = df[\"r_1d\"].iloc[:split].values\n    Xte = df[[\"lag1\",\"lag2\",\"lag3\"]].iloc[split:].values\n    yte = df[\"r_1d\"].iloc[split:].values\n\n    model = LinearRegression().fit(Xtr, ytr)\n    pred = model.predict(Xte)\n    mae = float(mean_absolute_error(yte, pred))\n\n    Path(\"reports\").mkdir(exist_ok=True)\n    with open(args.out_metrics, \"w\") as f:\n        json.dump({\"model\":\"linear(lag1,lag2,lag3)\",\"test_mae\":mae,\"n_test\":len(yte)}, f, indent=2)\n    print(\"Wrote\", args.out_metrics, \"MAE:\", mae)\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\nopen(\"scripts/train_baseline.py\",\"w\").write(train_py)\nimport os, stat\nos.chmod(\"scripts/train_baseline.py\", os.stat(\"scripts/train_baseline.py\").st_mode | stat.S_IEXEC)\nprint(\"Created scripts/train_baseline.py\")\n\n\n6.6.2 Part B — Extend your Makefile with train and all\nAppend these to your Makefile:\n# --- add after FEATS definition, near other targets ---\n\nTRAIN_METRICS := reports/baseline_metrics.json\n\n.PHONY: train\ntrain: $(TRAIN_METRICS) ## Train toy baseline and write metrics\n\n$(TRAIN_METRICS): scripts/train_baseline.py $(FEATS)\n    $(PY) scripts/train_baseline.py --features $(FEATS) --out-metrics $(TRAIN_METRICS)\n\n# Update 'all' to include 'train'\n# all: $(DATA_RAW) $(FEATS) report backup   # OLD\n# Replace with:\n# all: $(DATA_RAW) $(FEATS) report train backup\nRun:\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\nmake train\ncat reports/baseline_metrics.json\n\n\n6.6.3 Part C — Add a help description to every target and verify make help\nEnsure each target in your Makefile has a ## comment. Run:\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\nmake help\n\n\n6.6.4 Part D — (Optional) Track small models/metrics with Git‑LFS\nIf you decide to save model artifacts (e.g., models/baseline.pkl), track them:\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\ngit lfs track \"models/*.pkl\"\ngit add .gitattributes\ngit commit -m \"chore: track small model files via LFS\"\n(You can extend train_baseline.py to save models/baseline.pkl using joblib.)\n\n\n6.6.5 Part E — Commit & push\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\ngit add scripts/*.py scripts/backup.sh Makefile reports/baseline_metrics.json\ngit status\ngit commit -m \"feat: automated pipeline with Make (data-&gt;features-&gt;report-&gt;train) and rsync backup\"\nfrom getpass import getpass\nimport subprocess\ntoken = getpass(\"GitHub token (not stored): \")\nREPO_OWNER = \"YOUR_GITHUB_USERNAME_OR_ORG\"\nREPO_NAME  = \"unified-stocks-teamX\"\npush_url = f\"https://{token}@github.com/{REPO_OWNER}/{REPO_NAME}.git\"\nsubprocess.run([\"git\", \"push\", push_url, \"HEAD:main\"], check=True)\ndel token\n\n\n6.6.6 Grading (pass/revise)\n\nmake all runs from a fresh clone (with minimal edits for tokens/Quarto install) and produces: data/raw/prices.csv, data/processed/features.parquet, docs/reports/eda.html, reports/baseline_metrics.json, and a backups/run-*/ snapshot.\nMakefile has helpful help output and variables (START, END, ROLL).\nscripts/backup.sh uses rsync -avh --delete and excludes raw/ & interim/.\n(Optional) LFS tracking updated for models.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6 — Make/Automation + rsync + ssh/tmux (survey)</span>"
    ]
  },
  {
    "objectID": "lec6.html#key-points",
    "href": "lec6.html#key-points",
    "title": "6  Session 6 — Make/Automation + rsync + ssh/tmux (survey)",
    "section": "6.7 Key points:",
    "text": "6.7 Key points:\n\nMake is a thin layer over shell commands—it doesn’t replace Python; it orchestrates it.\nKeep targets idempotent: running twice shouldn’t break; only rebuild on changes.\nUse rsync with care: --delete is powerful—double‑check DEST paths.\nssh/tmux: you’ll want this the first time you run a long model on a remote machine.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6 — Make/Automation + rsync + ssh/tmux (survey)</span>"
    ]
  },
  {
    "objectID": "lec7.html",
    "href": "lec7.html",
    "title": "7  Session 7 — SQL I: SQLite Schemas & Joins",
    "section": "",
    "text": "7.1 Session 7 — SQL I: SQLite Schemas & Joins (75 min)\nAssumptions: You’re continuing in the same Drive‑mounted repo (e.g., unified-stocks-teamX). You have data/raw/prices.csv from prior sessions. If not, the lab includes a fallback generator.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7 — SQL I: SQLite Schemas & Joins</span>"
    ]
  },
  {
    "objectID": "lec7.html#session-7-sql-i-sqlite-schemas-joins-75-min",
    "href": "lec7.html#session-7-sql-i-sqlite-schemas-joins-75-min",
    "title": "7  Session 7 — SQL I: SQLite Schemas & Joins",
    "section": "",
    "text": "7.1.1 Learning goals\nBy the end of class, students can:\n\nCreate a SQLite database with proper tables, primary keys, constraints, and indexes.\nLoad CSV data into SQLite safely (parameterized inserts or pandas.to_sql), avoiding duplicates.\nWrite SELECT queries with WHERE/ORDER/LIMIT and basic JOINs.\nUse parameterized queries from Python to avoid SQL injection.\nBuild a small SQL I/O helper to streamline queries from Python.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7 — SQL I: SQLite Schemas & Joins</span>"
    ]
  },
  {
    "objectID": "lec7.html#agenda-75-minutes",
    "href": "lec7.html#agenda-75-minutes",
    "title": "7  Session 7 — SQL I: SQLite Schemas & Joins",
    "section": "7.2 Agenda (75 minutes)",
    "text": "7.2 Agenda (75 minutes)\n\n(10 min) Why relational databases for DS; SQLite types; PK (Primary Key)/constraints; indexes\n(10 min) DDL (Data Definition Language) overview (CREATE TABLE/INDEX); transactions; parameterized queries\n(35 min) In‑class lab (Colab): create prices.db → load prices + meta → write joins\n(10 min) Wrap‑up & homework briefing\n(10 min) Buffer",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7 — SQL I: SQLite Schemas & Joins</span>"
    ]
  },
  {
    "objectID": "lec7.html#slides",
    "href": "lec7.html#slides",
    "title": "7  Session 7 — SQL I: SQLite Schemas & Joins",
    "section": "7.3 Slides",
    "text": "7.3 Slides\nWhy SQLite for DS\n\nSingle file DB → easy to version, ship, query; no server admin.\nStronger guarantees than loose CSVs: types, constraints, unique keys, foreign keys.\nFast filters/joins with indexes; JIT (Just in time) queries from Python, R, or CLI.\n\nSQLite types & constraints\n\nSQLite uses dynamic typing but honors affinities: INTEGER, REAL, TEXT, BLOB.\nUse PRIMARY KEY (uniqueness + index), NOT NULL, and CHECK (e.g., volume ≥ 0).\nTurn on foreign keys: PRAGMA foreign_keys = ON; In SQLite, PRAGMA is a special command used to change database settings or get internal info.\nSQLite → a self-contained database engine (embedded, serverless, file-based). Apps link the SQLite library and read/write a .sqlite file. It enforces SQL, transactions, indexes, etc.\nSQLAlchemy → a Python database toolkit/ORM. It doesn’t store data itself; it generates SQL and talks to engines (SQLite, Postgres, MySQL, DuckDB, etc.).\n\n\n7.3.1 When you’d use each\n\nUse SQLite when you need a lightweight, zero-admin database bundled with your app (desktop, mobile, edge/IoT, tests).\nUse SQLAlchemy when you’re writing Python and want a high-level, engine-agnostic way to model tables and run queries (ORM or Core SQL). It can point at SQLite (great for local dev) or a server DB in prod.\nThey’re not substitutes: SQLite = database, SQLAlchemy = Python ORM/toolkit.\nOverall ubiquity: SQLite.\nWithin Python ecosystems (as a library): SQLAlchemy is one of the most widely used database libraries.\n\nORM = Object–Relational Mapping\nAn ORM is a programming technique (and usually a library) that maps database tables ↔︎ objects in your code.\nInstead of writing raw SQL (SELECT … FROM …), you work with classes and objects, and the ORM translates your code into SQL statements under the hood.\nIndexes & performance\n\nIndex columns used in joins and filters.\nComposite PK (ticker, date) makes common lookups fast.\n\nWhat NOT to commit\n\nLarge .db files. Keep DB small or regenerate from CSV with a script.\nIf you must version a small DB, ensure Git‑LFS tracks data/*.db (we set this in Session 2).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7 — SQL I: SQLite Schemas & Joins</span>"
    ]
  },
  {
    "objectID": "lec7.html#inclass-lab-35-min",
    "href": "lec7.html#inclass-lab-35-min",
    "title": "7  Session 7 — SQL I: SQLite Schemas & Joins",
    "section": "7.4 In‑class lab (35 min)",
    "text": "7.4 In‑class lab (35 min)\n\n7.4.1 0) Mount Drive & enter repo\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_OWNER = \"YOUR_GITHUB_USERNAME_OR_ORG\"   # &lt;- change\nREPO_NAME  = \"unified-stocks-teamX\"          # &lt;- change\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, pandas as pd, numpy as np\npathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\nassert pathlib.Path(REPO_DIR).exists(), \"Repo not found in Drive. Clone it first.\"\nos.chdir(REPO_DIR)\nprint(\"Working dir:\", os.getcwd())\n\n\n7.4.2 1) Ensure prerequisites & create a small prices.csv if missing\n# Ensure pandas and sqlite3 are available (sqlite3 is in stdlib)\nimport pandas as pd, sqlite3, numpy as np, os\nfrom pathlib import Path\n\nPath(\"data/raw\").mkdir(parents=True, exist_ok=True)\nif not Path(\"data/raw/prices.csv\").exists():\n    print(\"No prices.csv found; generating a small synthetic one.\")\n    tickers = [\"AAPL\",\"MSFT\",\"NVDA\",\"AMZN\",\"GOOGL\"]\n    dates = pd.bdate_range(\"2022-01-03\", periods=120)\n    rng = np.random.default_rng(7)\n    frames=[]\n    for t in tickers:\n        r = rng.normal(0, 0.01, len(dates))\n        price = 100*np.exp(np.cumsum(r))\n        vol = rng.integers(1e5, 5e6, len(dates))\n        df = pd.DataFrame({\"ticker\": t, \"date\": dates, \"adj_close\": price, \"volume\": vol})\n        df[\"log_return\"] = np.log(df[\"adj_close\"]).diff().fillna(0)\n        frames.append(df)\n    pd.concat(frames, ignore_index=True).to_csv(\"data/raw/prices.csv\", index=False)\n\n# Show a peek\npd.read_csv(\"data/raw/prices.csv\").head()\n\n\n7.4.3 2) Design schema & create the database data/prices.db\nWe’ll use two tables:\n\nmeta(ticker TEXT PRIMARY KEY, name TEXT, sector TEXT NOT NULL)\nprices(ticker TEXT NOT NULL, date TEXT NOT NULL, adj_close REAL NOT NULL, volume INTEGER NOT NULL, log_return REAL NOT NULL, PRIMARY KEY (ticker,date), FOREIGN KEY (ticker) REFERENCES meta(ticker))\n\nimport sqlite3, textwrap, os\nfrom pathlib import Path\n\ndb_path = Path(\"data/prices.db\")\nif db_path.exists(): db_path.unlink()  # start fresh for class; remove this in real life\ncon = sqlite3.connect(db_path)\ncur = con.cursor()\n\n# Turn on foreign keys\ncur.execute(\"PRAGMA foreign_keys = ON;\")\n# (Optional) WAL can help concurrency; not critical here\ncur.execute(\"PRAGMA journal_mode = WAL;\")\n\nddl = textwrap.dedent(\"\"\"\nCREATE TABLE meta (\n  ticker TEXT PRIMARY KEY,\n  name   TEXT,\n  sector TEXT NOT NULL\n);\n\nCREATE TABLE prices (\n  ticker     TEXT NOT NULL,\n  date       TEXT NOT NULL,               -- ISO 'YYYY-MM-DD'\n  adj_close  REAL NOT NULL CHECK (adj_close &gt;= 0),\n  volume     INTEGER NOT NULL CHECK (volume &gt;= 0),\n  log_return REAL NOT NULL,\n  PRIMARY KEY (ticker, date),\n  FOREIGN KEY (ticker) REFERENCES meta(ticker)\n);\n\n-- Index to speed up date-range scans across all tickers\nCREATE INDEX IF NOT EXISTS idx_prices_date ON prices(date);\n\"\"\")\ncur.executescript(ddl)\ncon.commit()\nprint(\"Created:\", db_path)\n\n\n7.4.4 3) Populate meta (try yfinance sector; fallback to synthetic)\nimport pandas as pd, numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Read tickers (from existing CSV or fallback)\nif Path(\"tickers_25.csv\").exists():\n    tickers = pd.read_csv(\"tickers_25.csv\")[\"ticker\"].dropna().unique().tolist()\nelse:\n    tickers = pd.read_csv(\"data/raw/prices.csv\")[\"ticker\"].dropna().unique().tolist()\n\ndef fetch_sector_map(tickers):\n    try:\n        import yfinance as yf\n        out=[]\n        for t in tickers:\n            info = yf.Ticker(t).info or {}\n            name  = info.get(\"shortName\") or info.get(\"longName\") or t\n            sector= info.get(\"sector\") or \"Unknown\"\n            out.append({\"ticker\": t, \"name\": name, \"sector\": sector})\n        return pd.DataFrame(out)\n    except Exception:\n        pass\n    # Fallback: deterministic synthetic sectors\n    sectors = [\"Technology\",\"Financials\",\"Healthcare\",\"Energy\",\"Consumer\"]\n    rng = np.random.default_rng(42)\n    return pd.DataFrame({\n        \"ticker\": tickers,\n        \"name\": tickers,\n        \"sector\": [sectors[i % len(sectors)] for i in range(len(tickers))]\n    })\n\nmeta_df = fetch_sector_map(tickers)\nmeta_df.head()\nThe with context block may not be working in a notebook. If so, simply execute the code outsie a with block.\n# Insert meta with parameterized query\nwith con:\n    con.executemany(\n        \"INSERT INTO meta(ticker, name, sector) VALUES(?, ?, ?)\",\n        meta_df[[\"ticker\",\"name\",\"sector\"]].itertuples(index=False, name=None)\n    )\nprint(pd.read_sql_query(\"SELECT * FROM meta LIMIT 5;\", con))\n\n\n7.4.5 4) Load data/raw/prices.csv into a staging DataFrame and insert into prices\nWe’ll use parameterized bulk insert (executemany) which is fast and safe.\nprices = pd.read_csv(\"data/raw/prices.csv\", parse_dates=[\"date\"])\n# Normalize date to ISO text\nprices[\"date\"] = prices[\"date\"].dt.strftime(\"%Y-%m-%d\")\n# Keep only needed columns and ensure order matches table\nprices = prices[[\"ticker\",\"date\",\"adj_close\",\"volume\",\"log_return\"]]\n\n# Optional: drop duplicates to respect PK before insert\nprices = prices.drop_duplicates(subset=[\"ticker\",\"date\"]).reset_index(drop=True)\nlen(prices)\n# Bulk insert inside one transaction; ignore rows violating FK or PK (e.g., duplicates)\nwith con:\n    con.executemany(\n        \"INSERT OR IGNORE INTO prices(ticker,date,adj_close,volume,log_return) VALUES(?,?,?,?,?)\",\n        prices.itertuples(index=False, name=None)\n    )\n\n# Quick counts\nprint(pd.read_sql_query(\"SELECT COUNT(*) AS nrows FROM prices;\", con))\nprint(pd.read_sql_query(\"SELECT ticker, COUNT(*) AS n FROM prices GROUP BY ticker ORDER BY n DESC LIMIT 5;\", con))\n\n\n7.4.6 5) Sanity queries (filters, order, limit)\nq1 = \"\"\"\nSELECT ticker, date, adj_close, volume\nFROM prices\nWHERE ticker = ? AND date BETWEEN ? AND ?\nORDER BY date ASC\nLIMIT 5;\n\"\"\"\nprint(pd.read_sql_query(q1, con, params=[\"AAPL\",\"2020-03-01\",\"2020-06-30\"]))\n# Top 10 absolute daily moves for a chosen ticker\nq2 = \"\"\"\nSELECT p.ticker, p.date, p.log_return, ABS(p.log_return) AS abs_move\nFROM prices AS p\nWHERE p.ticker = ?\nORDER BY abs_move DESC\nLIMIT 10;\n\"\"\"\nprint(pd.read_sql_query(q2, con, params=[\"NVDA\"]))\n\n\n7.4.7 6) JOIN with meta (per‑sector summaries)\n# Mean |std| of daily returns per sector over a date range\nq3 = \"\"\"\nSELECT m.sector,\n       AVG(ABS(p.log_return)) AS mean_abs_return,\n       AVG(p.log_return)      AS mean_return,\n       STDDEV(p.log_return)   AS std_return\nFROM prices p\nJOIN meta   m ON p.ticker = m.ticker\nWHERE p.date BETWEEN ? AND ?\nGROUP BY m.sector\nORDER BY mean_abs_return DESC;\n\"\"\"\ntry: \n  print(pd.read_sql_query(q3, con, params=[\"2020-01-01\",\"2020-06-30\"]))\nexcept:\n  print(\"Error\")\n# SQLite doesn't have STDDEV by default; fallback using variance formula via window? We'll compute in pandas:\ndf = pd.read_sql_query(\"\"\"\nSELECT m.sector, p.log_return\nFROM prices p JOIN meta m ON p.ticker = m.ticker\nWHERE p.date BETWEEN ? AND ?;\n\"\"\", con, params=[\"2020-01-01\",\"2020-09-08\"])\nagg = (df.assign(abs=lambda d: d[\"log_return\"].abs())\n         .groupby(\"sector\")\n         .agg(mean_abs_return=(\"abs\",\"mean\"),\n              mean_return=(\"log_return\",\"mean\"),\n              std_return=(\"log_return\",\"std\"))\n         .sort_values(\"mean_abs_return\", ascending=False))\nagg\n\n\n7.4.8 7) Create a view for convenience & test uniqueness constraint\n# View: latest available date per ticker\nwith con:\n    con.execute(\"\"\"\n    CREATE VIEW IF NOT EXISTS latest_prices AS\n    SELECT p.*\n    FROM prices p\n    JOIN (\n      SELECT ticker, MAX(date) AS max_date\n      FROM prices\n      GROUP BY ticker\n    ) t ON p.ticker = t.ticker AND p.date = t.max_date;\n    \"\"\")\npd.read_sql_query(\"SELECT * FROM latest_prices ORDER BY ticker LIMIT 10;\", con)\n# Demonstrate the UNIQUE/PK constraint: inserting a duplicate row should be ignored or fail\nimport sqlite3\nrow = pd.read_sql_query(\"SELECT * FROM prices LIMIT 1;\", con).iloc[0].to_dict()\ntry:\n    with con:\n        con.execute(\n            \"INSERT INTO prices(ticker,date,adj_close,volume,log_return) VALUES(?,?,?,?,?)\",\n            (row[\"ticker\"], row[\"date\"], row[\"adj_close\"], row[\"volume\"], row[\"log_return\"])\n        )\n    print(\"Unexpected: duplicate insert succeeded (should not).\")\nexcept sqlite3.IntegrityError as e:\n    print(\"IntegrityError as expected:\", e)\n\n\n7.4.9 8) A tiny SQL I/O helper for your project\nfrom pathlib import Path\nPath(\"src\").mkdir(exist_ok=True)\nPath(\"src/projectname\").mkdir(parents=True, exist_ok=True)\n\nsqlio_py = \"\"\"\\\nfrom __future__ import annotations\nimport sqlite3\nimport pandas as pd\nfrom contextlib import contextmanager\nfrom pathlib import Path\n\nDB_PATH = Path(\"data/prices.db\")\n\n@contextmanager\ndef connect(db_path: str | Path = DB_PATH):\n    con = sqlite3.connect(str(db_path))\n    con.execute(\"PRAGMA foreign_keys = ON;\")\n    try:\n        yield con\n    finally:\n        con.close()\n\ndef query_df(sql: str, params: tuple | list | None = None, db_path: str | Path = DB_PATH) -&gt; pd.DataFrame:\n    with connect(db_path) as con:\n        return pd.read_sql_query(sql, con, params=params)\n\ndef sector_summary(start: str, end: str, db_path: str | Path = DB_PATH) -&gt; pd.DataFrame:\n    sql = '''\n    SELECT m.sector, p.log_return\n    FROM prices p JOIN meta m ON p.ticker = m.ticker\n    WHERE p.date BETWEEN ? AND ?;\n    '''\n    df = query_df(sql, [start, end], db_path)\n    if df.empty:\n        return df\n    g = df.assign(abs=lambda d: d[\"log_return\"].abs()).groupby(\"sector\")\n    return g.agg(mean_abs_return=(\"abs\",\"mean\"),\n                 mean_return=(\"log_return\",\"mean\"),\n                 std_return=(\"log_return\",\"std\")).reset_index()\n\"\"\"\nopen(\"src/projectname/sqlio.py\",\"w\").write(sqlio_py)\nprint(\"Wrote src/projectname/sqlio.py\")\nQuick test:\nfrom src.projectname.sqlio import sector_summary\nsector_summary(\"2020-01-01\",\"2020-08-01\").head()\n\nNote on versioning: If data/prices.db stays small (a few MB), you may commit it via Git‑LFS (we tracked data/*.db in Session 2). Otherwise, do not commit—rebuild from CSV with a script (homework).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7 — SQL I: SQLite Schemas & Joins</span>"
    ]
  },
  {
    "objectID": "lec7.html#wrapup-10-min",
    "href": "lec7.html#wrapup-10-min",
    "title": "7  Session 7 — SQL I: SQLite Schemas & Joins",
    "section": "7.5 Wrap‑up (10 min)",
    "text": "7.5 Wrap‑up (10 min)\n\nYou now have a relational core for the project.\nUse PK + constraints to prevent silent data corruption.\nUse parameterized queries from Python.\nNext session: SQL II — Window functions & pandas.read_sql workflows (rolling stats, LAG/LEAD).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7 — SQL I: SQLite Schemas & Joins</span>"
    ]
  },
  {
    "objectID": "lec7.html#homework-due-before-session-8",
    "href": "lec7.html#homework-due-before-session-8",
    "title": "7  Session 7 — SQL I: SQLite Schemas & Joins",
    "section": "7.6 Homework (due before Session 8)",
    "text": "7.6 Homework (due before Session 8)\nGoal: Make database creation reproducible, add metadata, write joins you’ll reuse later, and hook it into your Makefile.\n\n7.6.1 Part A — Script to (re)build the DB\nCreate scripts/build_db.py that creates tables and loads CSVs deterministically.\n# scripts/build_db.py\n#!/usr/bin/env python\nimport argparse, sys, textwrap, sqlite3\nfrom pathlib import Path\nimport pandas as pd, numpy as np\n\nDDL = textwrap.dedent(\"\"\"\nPRAGMA foreign_keys = ON;\nCREATE TABLE IF NOT EXISTS meta (\n  ticker TEXT PRIMARY KEY,\n  name   TEXT,\n  sector TEXT NOT NULL\n);\nCREATE TABLE IF NOT EXISTS prices (\n  ticker     TEXT NOT NULL,\n  date       TEXT NOT NULL,\n  adj_close  REAL NOT NULL CHECK (adj_close &gt;= 0),\n  volume     INTEGER NOT NULL CHECK (volume &gt;= 0),\n  log_return REAL NOT NULL,\n  PRIMARY KEY (ticker,date),\n  FOREIGN KEY (ticker) REFERENCES meta(ticker)\n);\nCREATE INDEX IF NOT EXISTS idx_prices_date ON prices(date);\n\"\"\")\n\ndef load_meta(con, tickers_csv: Path):\n    if tickers_csv.exists():\n        tks = pd.read_csv(tickers_csv)[\"ticker\"].dropna().unique().tolist()\n    else:\n        raise SystemExit(f\"tickers CSV not found: {tickers_csv}\")\n    sectors = [\"Technology\",\"Financials\",\"Healthcare\",\"Energy\",\"Consumer\"]\n    meta = pd.DataFrame({\n        \"ticker\": tks,\n        \"name\": tks,\n        \"sector\": [sectors[i % len(sectors)] for i in range(len(tks))]\n    })\n    with con:\n        con.executemany(\"INSERT OR REPLACE INTO meta(ticker,name,sector) VALUES(?,?,?)\",\n                        meta.itertuples(index=False, name=None))\n\ndef load_prices(con, prices_csv: Path):\n    df = pd.read_csv(prices_csv, parse_dates=[\"date\"])\n    df[\"date\"] = df[\"date\"].dt.strftime(\"%Y-%m-%d\")\n    df = df[[\"ticker\",\"date\",\"adj_close\",\"volume\",\"log_return\"]].drop_duplicates([\"ticker\",\"date\"])\n    with con:\n        con.executemany(\n            \"INSERT OR REPLACE INTO prices(ticker,date,adj_close,volume,log_return) VALUES(?,?,?,?,?)\",\n            df.itertuples(index=False, name=None)\n        )\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--db\", default=\"data/prices.db\")\n    ap.add_argument(\"--tickers\", default=\"tickers_25.csv\")\n    ap.add_argument(\"--prices\", default=\"data/raw/prices.csv\")\n    args = ap.parse_args()\n\n    Path(args.db).parent.mkdir(parents=True, exist_ok=True)\n    con = sqlite3.connect(args.db)\n    con.executescript(DDL)\n    load_meta(con, Path(args.tickers))\n    load_prices(con, Path(args.prices))\n    con.close()\n    print(\"Built DB:\", args.db)\n\nif __name__ == \"__main__\":\n    sys.exit(main())\nMake it executable:\nimport os, stat, pathlib\np = pathlib.Path(\"scripts/build_db.py\")\nos.chmod(p, os.stat(p).st_mode | stat.S_IEXEC)\nprint(\"Ready:\", p)\n\n\n7.6.2 Part B — Add Makefile target db and a small SQL report\nAppend to your Makefile:\nDB := data/prices.db\n\n.PHONY: db sql-report\ndb: ## Build/refresh SQLite database from CSVs\n  python scripts/build_db.py --db $(DB) --tickers tickers_25.csv --prices data/raw/prices.csv\n\nsql-report: db ## Generate a simple SQL-driven CSV summary\n  python - &lt;&lt; 'PY'\n  import pandas as pd, sqlite3, os\n  con = sqlite3.connect(\"data/prices.db\")\n  df = pd.read_sql_query(\"\"\"\n  SELECT m.sector, \n    COUNT(*) AS n_obs, \n    AVG(ABS(p.log_return)) AS mean_abs_return\n  FROM prices p \n  JOIN meta m ON p.ticker=m.ticker\n  GROUP BY m.sector ORDER BY n_obs DESC;\n  \"\"\", con)\nos.makedirs(\"reports\", exist_ok=True)\ndf.to_csv(\"reports/sql_sector_summary.csv\", index=False)\nprint(df.head())\ncon.close()\nPY\nRun:\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\nmake db\nmake sql-report\n\n\n7.6.3 Part C — Write 3 JOIN queries (save as .sql under sql/)\nCreate a folder sql/ and add:\n\nsector_top_moves.sql: top 10 absolute daily moves per sector (date, ticker, abs_move).\nticker_activity.sql: per‑ticker counts, min/max date.\nrange_summary.sql: for a given date range (use placeholders), mean/std of returns by ticker and sector.\n\nExample (1):\n-- sql/sector_top_moves.sql\nSELECT m.sector, p.ticker, p.date, p.log_return, ABS(p.log_return) AS abs_move\nFROM prices p JOIN meta m ON p.ticker = m.ticker\nORDER BY abs_move DESC\nLIMIT 10;\nThen a small Python launcher to run any .sql file with optional parameters:\n# scripts/run_sql.py\n#!/usr/bin/env python\nimport argparse, sqlite3, pandas as pd\nfrom pathlib import Path\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--db\", default=\"data/prices.db\")\n    ap.add_argument(\"--sqlfile\", required=True)\n    ap.add_argument(\"--params\", nargs=\"*\", default=[])\n    ap.add_argument(\"--out\", default=\"\")\n    args = ap.parse_args()\n\n    sql = Path(args.sqlfile).read_text()\n    con = sqlite3.connect(args.db)\n    df = pd.read_sql_query(sql, con, params=args.params or None)\n    con.close()\n    if args.out:\n        Path(args.out).parent.mkdir(parents=True, exist_ok=True)\n        df.to_csv(args.out, index=False)\n    print(df.head())\n\nif __name__ == \"__main__\":\n    main()\nRun a demo:\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\npython scripts/run_sql.py --sqlfile sql/sector_top_moves.sql --out reports/sector_top_moves.csv\n\n\n7.6.4 Part D — (Stretch) Create a calendar table & missing‑day check\nCreate calendar(date TEXT PRIMARY KEY) covering min→max date in prices, and write a query that counts missing business days per ticker (join calendar LEFT JOIN prices). Save result to reports/missing_days.csv.\nHint: build the calendar in Python with pd.bdate_range(); insert into calendar; then SELECT c.date, p.ticker FROM calendar c LEFT JOIN prices p ... WHERE p.date IS NULL.\n\n\n7.6.5 Part E — Commit & push (use the short‑lived token flow from Session 2)\nRecommended files to add:\n\nscripts/build_db.py, scripts/run_sql.py, sql/*.sql, updated Makefile, reports/sql_sector_summary.csv\nOptionally do not commit data/prices.db if large; if small and you must commit, ensure LFS is tracking data/*.db.\n\n\n\n\n7.6.6 Grading (pass/revise)\n\ndata/prices.db builds from make db and contains meta + prices with PK (ticker,date) and FK to meta.\nreports/sql_sector_summary.csv generated by make sql-report.\nsql/sector_top_moves.sql, sql/ticker_activity.sql, sql/range_summary.sql present and runnable via scripts/run_sql.py.\nIf stretch completed: calendar table + reports/missing_days.csv.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7 — SQL I: SQLite Schemas & Joins</span>"
    ]
  },
  {
    "objectID": "lec7.html#key-points",
    "href": "lec7.html#key-points",
    "title": "7  Session 7 — SQL I: SQLite Schemas & Joins",
    "section": "7.7 Key points",
    "text": "7.7 Key points\n\nSchema first: clean DDL prevents downstream headaches.\nConstraints are your guardrails; test them (we did with a duplicate insert).\nParameterize queries; never string‑concat user inputs into SQL.\nKeep SQLite for analysis; push heavy analytics to Python/Polars when needed.\n\nNext time (Session 8): SQL II — Window functions & pandas.read_sql workflows (LAG/LEAD, rolling stats, and SQL↔︎pandas round‑trips).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7 — SQL I: SQLite Schemas & Joins</span>"
    ]
  },
  {
    "objectID": "lec8.html",
    "href": "lec8.html",
    "title": "8  Session 8 — SQL II: Window Functions & pandas.read_sql Workflows",
    "section": "",
    "text": "8.1 Session 8 — SQL II: Window Functions & pandas.read_sql (75 min)\nToday you’ll compute lags, leads, rolling stats, and top‑k queries in SQLite using window functions, then pull results into pandas for downstream use.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Session 8 — SQL II: Window Functions & `pandas.read_sql` Workflows</span>"
    ]
  },
  {
    "objectID": "lec8.html#session-8-sql-ii-window-functions-pandas.read_sql-75-min",
    "href": "lec8.html#session-8-sql-ii-window-functions-pandas.read_sql-75-min",
    "title": "8  Session 8 — SQL II: Window Functions & pandas.read_sql Workflows",
    "section": "8.1 Session 8 — SQL II: Window Functions & pandas.read_sql (75 min)",
    "text": "8.1 Session 8 — SQL II: Window Functions & pandas.read_sql (75 min)\n\n8.1.1 Learning goals\nBy the end of class, students can:\n\nExplain and use window functions: LAG, LEAD, ROW_NUMBER, and aggregates with OVER (PARTITION BY … ORDER BY … ROWS …).\nCompute rolling means/variance and multi‑lag features per ticker without leakage.\nUse WINDOW named frames to avoid repetition and errors.\nRun parameterized SQL from Python with pandas.read_sql_query, and optionally register a custom SQL function (e.g., SQRT).\nEvaluate query performance basics with EXPLAIN QUERY PLAN.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Session 8 — SQL II: Window Functions & `pandas.read_sql` Workflows</span>"
    ]
  },
  {
    "objectID": "lec8.html#agenda-75-min",
    "href": "lec8.html#agenda-75-min",
    "title": "8  Session 8 — SQL II: Window Functions & pandas.read_sql Workflows",
    "section": "8.2 Agenda (75 min)",
    "text": "8.2 Agenda (75 min)\n\n(10 min) Window functions: mental model & syntax (PARTITION BY, ORDER BY, ROWS frames)\n(10 min) Patterns for time series: lags, leads, rolling stats, top‑k per group\n(35 min) In‑class lab (Colab): lags/leads → rolling mean/variance → z‑scores → top days per ticker → pull into pandas and save features\n(10 min) Wrap‑up, performance notes (EXPLAIN QUERY PLAN), homework briefing\n(10 min) Buffer",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Session 8 — SQL II: Window Functions & `pandas.read_sql` Workflows</span>"
    ]
  },
  {
    "objectID": "lec8.html#slides-talking-points",
    "href": "lec8.html#slides-talking-points",
    "title": "8  Session 8 — SQL II: Window Functions & pandas.read_sql Workflows",
    "section": "8.3 Slides / talking points",
    "text": "8.3 Slides / talking points\nWhat’s a window?\n\nA window lets an aggregate or analytic function see a row + its neighbors without collapsing rows.\nTemplate:\nfunc(expr) OVER (\n   PARTITION BY key\n   ORDER BY time\n   ROWS BETWEEN N PRECEDING AND CURRENT ROW\n)\nPARTITION BY = per‑group window; ORDER BY = sequence; ROWS = how many rows to include.\n\nWindow vs GROUP BY\n\nGROUP BY returns one row per group; OVER (…) returns one row per input row with extra columns.\n\nTime‑series patterns\n\nLags: LAG(x, k) → previous k rows ⇒ features at t that use only info ≤ t.\nLeads: LEAD(x, k) → future k rows ⇒ labels (don’t leak into features).\nRolling stats: AVG(x) OVER (… ROWS BETWEEN w-1 PRECEDING AND CURRENT ROW); rolling variance via AVG(x*x) - AVG(x)^2.\nTop‑k per group: compute ROW_NUMBER() OVER (PARTITION BY key ORDER BY score DESC) and filter WHERE rn&lt;=k.\n\nROWS vs RANGE\n\nUse ROWS for fixed‑length windows on ordered rows (what we need).\nTime‑based windows (e.g., last 30 calendar days) require different techniques in SQLite (correlated subquery); we’ll note but not use today.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Session 8 — SQL II: Window Functions & `pandas.read_sql` Workflows</span>"
    ]
  },
  {
    "objectID": "lec8.html#inclass-lab-35-min",
    "href": "lec8.html#inclass-lab-35-min",
    "title": "8  Session 8 — SQL II: Window Functions & pandas.read_sql Workflows",
    "section": "8.4 In‑class lab (35 min)",
    "text": "8.4 In‑class lab (35 min)\n\nRun each block as its own Colab cell. Adjust REPO_OWNER/REPO_NAME first.\n\n\n8.4.1 0) Mount Drive, open DB, and ensure it exists\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_OWNER = \"YOUR_GITHUB_USERNAME_OR_ORG\"     # &lt;- change\nREPO_NAME  = \"unified-stocks-teamX\"            # &lt;- change\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, sqlite3, pandas as pd, numpy as np, math, textwrap\npathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\nassert pathlib.Path(REPO_DIR).exists(), \"Repo not found; clone it first.\"\nos.chdir(REPO_DIR)\nprint(\"Working dir:\", os.getcwd())\n\n# Ensure DB exists (fallback: build from CSV)\ndb_path = pathlib.Path(\"data/prices.db\")\nif not db_path.exists():\n    print(\"prices.db not found; attempting minimal build from data/raw/prices.csv …\")\n    pathlib.Path(\"data\").mkdir(exist_ok=True)\n    con = sqlite3.connect(db_path)\n    con.execute(\"PRAGMA foreign_keys = ON;\")\n    con.executescript(\"\"\"\n    CREATE TABLE IF NOT EXISTS meta (\n      ticker TEXT PRIMARY KEY,\n      name   TEXT,\n      sector TEXT NOT NULL\n    );\n    CREATE TABLE IF NOT EXISTS prices (\n      ticker     TEXT NOT NULL,\n      date       TEXT NOT NULL,\n      adj_close  REAL NOT NULL CHECK (adj_close &gt;= 0),\n      volume     INTEGER NOT NULL CHECK (volume &gt;= 0),\n      log_return REAL NOT NULL,\n      PRIMARY KEY (ticker,date),\n      FOREIGN KEY (ticker) REFERENCES meta(ticker)\n    );\n    CREATE INDEX IF NOT EXISTS idx_prices_date ON prices(date);\n    \"\"\")\n    # Minimal meta from tickers_25 or from CSV\n    if pathlib.Path(\"tickers_25.csv\").exists():\n        tks = pd.read_csv(\"tickers_25.csv\")[\"ticker\"].dropna().unique().tolist()\n    else:\n        raw = pd.read_csv(\"data/raw/prices.csv\")\n        tks = raw[\"ticker\"].dropna().unique().tolist()\n    meta = pd.DataFrame({\"ticker\": tks, \"name\": tks, \"sector\": [\"Unknown\"]*len(tks)})\n    con.executemany(\"INSERT OR IGNORE INTO meta(ticker,name,sector) VALUES(?,?,?)\",\n                    meta.itertuples(index=False, name=None))\n    # Load prices.csv if present; otherwise synthesize small sample\n    if pathlib.Path(\"data/raw/prices.csv\").exists():\n        df = pd.read_csv(\"data/raw/prices.csv\", parse_dates=[\"date\"]).copy()\n    else:\n        dates = pd.bdate_range(\"2022-01-03\", periods=90)\n        rng = np.random.default_rng(7)\n        frames=[]\n        for t in tks[:5]:\n            r = rng.normal(0, 0.01, len(dates))\n            price = 100*np.exp(np.cumsum(r))\n            vol = rng.integers(1e5, 5e6, len(dates))\n            frames.append(pd.DataFrame({\"ticker\": t, \"date\": dates,\n                                        \"adj_close\": price, \"volume\": vol}))\n        df = pd.concat(frames, ignore_index=True)\n        df[\"log_return\"] = np.log(df[\"adj_close\"]).diff().fillna(0)\n    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.strftime(\"%Y-%m-%d\")\n    df = df[[\"ticker\",\"date\",\"adj_close\",\"volume\",\"log_return\"]].drop_duplicates([\"ticker\",\"date\"])\n    con.executemany(\"INSERT OR REPLACE INTO prices(ticker,date,adj_close,volume,log_return) VALUES(?,?,?,?,?)\",\n                    df.itertuples(index=False, name=None))\n    con.commit()\n    con.close()\n\n# Connect and register SQRT (SQLite lacks STDDEV; we’ll compute var and take sqrt)\ncon = sqlite3.connect(db_path)\ncon.create_function(\"SQRT\", 1, lambda x: math.sqrt(x) if x is not None and x&gt;=0 else None)\nprint(\"SQLite version:\", sqlite3.sqlite_version)\n\n\n8.4.2 1) LAG & LEAD (no leakage in features)\nimport pandas as pd\n\nsql = \"\"\"\nSELECT ticker, date,\n       log_return AS r,\n       LAG(log_return,1) OVER (PARTITION BY ticker ORDER BY date) AS lag1,\n       LAG(log_return,2) OVER (PARTITION BY ticker ORDER BY date) AS lag2,\n       LEAD(log_return,1) OVER (PARTITION BY ticker ORDER BY date) AS r_tplus1  -- label candidate\nFROM prices\nWHERE date BETWEEN ? AND ?\nORDER BY ticker, date\nLIMIT 20;\n\"\"\"\ndf = pd.read_sql_query(sql, con, params=[\"2022-03-01\",\"2022-06-30\"])\ndf.head(10)\n\nNotes:\n\nlag1/lag2 are safe features at time t (depend on ≤ t-1).\nr_tplus1 is a label; never include in features.\n\n\n\n\n8.4.3 2) Named WINDOW + rolling mean/variance (20‑row window)\nsql = \"\"\"\nSELECT\n  ticker, date, log_return AS r,\n  AVG(log_return) OVER w AS roll_mean_20,\n  AVG(log_return*log_return) OVER w\n    - (AVG(log_return) OVER w)*(AVG(log_return) OVER w) AS roll_var_20\nFROM prices\nWHERE date BETWEEN ? AND ?\nWINDOW w AS (\n  PARTITION BY ticker\n  ORDER BY date\n  ROWS BETWEEN 19 PRECEDING AND CURRENT ROW\n)\n\nORDER BY ticker, date\nLIMIT 20;\n\"\"\"\nroll = pd.read_sql_query(sql, con, params=[\"2022-03-01\",\"2022-06-30\"])\nroll.head(10)\nCompute rolling std and z‑score in pandas (since we registered SQRT, we could also do it in SQL; here we’ll do it in pandas for clarity):\nroll[\"roll_std_20\"] = (roll[\"roll_var_20\"].clip(lower=0)).pow(0.5)\nroll[\"zscore_20\"] = (roll[\"r\"] - roll[\"roll_mean_20\"]) / roll[\"roll_std_20\"].replace(0, pd.NA)\nroll.head(5)\n\n\n8.4.4 3) Top‑k absolute moves per ticker with ROW_NUMBER\nsql = \"\"\"\nWITH ranked AS (\n  SELECT\n    ticker, date, log_return,\n    ABS(log_return) AS abs_move,\n    ROW_NUMBER() OVER (\n      PARTITION BY ticker\n      ORDER BY ABS(log_return) DESC\n    ) AS rn\n  FROM prices\n  WHERE date BETWEEN ? AND ?\n)\nSELECT * FROM ranked WHERE rn &lt;= 3\nORDER BY ticker, rn;\n\"\"\"\ntopk = pd.read_sql_query(sql, con, params=[\"2022-01-01\",\"2025-08-01\"])\ntopk.head(15)\n\n\n8.4.5 4) Build a features DataFrame directly from SQL and save to Parquet\nWe’ll assemble lags and rolling stats in one query using a named window w20:\nsql = \"\"\"\nSELECT\n  ticker, date,\n  log_return AS r_1d,\n  LAG(log_return,1) OVER (PARTITION BY ticker ORDER BY date) AS lag1,\n  LAG(log_return,2) OVER (PARTITION BY ticker ORDER BY date) AS lag2,\n  LAG(log_return,3) OVER (PARTITION BY ticker ORDER BY date) AS lag3,\n  AVG(log_return) OVER w20 AS roll_mean_20,\n  AVG(log_return*log_return) OVER w20\n    - (AVG(log_return) OVER w20)*(AVG(log_return) OVER w20) AS roll_var_20\nFROM prices\nWINDOW w20 AS (\n  PARTITION BY ticker\n  ORDER BY date\n  ROWS BETWEEN 19 PRECEDING AND CURRENT ROW\n)\nWHERE date BETWEEN ? AND ?\nORDER BY ticker, date;\n\"\"\"\nfeatures_sql = pd.read_sql_query(sql, con, params=[\"2019-01-01\",\"2025-08-01\"])\nfeatures_sql[\"roll_std_20\"] = (features_sql[\"roll_var_20\"].clip(lower=0)).pow(0.5)\nfeatures_sql[\"zscore_20\"] = (features_sql[\"r_1d\"] - features_sql[\"roll_mean_20\"]) / features_sql[\"roll_std_20\"].replace(0, pd.NA)\n\n# Drop first 2–3 rows per ticker where lags are null\nfeatures_sql = (features_sql\n                .sort_values([\"ticker\",\"date\"])\n                .groupby(\"ticker\", group_keys=False)\n                .apply(lambda g: g.iloc[3:]))\n\n# Save\npathlib.Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\nfeatures_sql.to_parquet(\"data/processed/features_sql.parquet\", index=False)\nfeatures_sql.head()\n\n\n8.4.6 5) EXPLAIN QUERY PLAN sanity & index usage\nplan = pd.read_sql_query(\"\"\"\nEXPLAIN QUERY PLAN\nSELECT ticker, date,\n       LAG(log_return,1) OVER (PARTITION BY ticker ORDER BY date) AS lag1\nFROM prices\nWHERE date BETWEEN ? AND ?\nORDER BY ticker, date;\n\"\"\", con, params=[\"2022-01-01\",\"2025-08-01\"])\nplan\n\nInterpretation tip: You should see use of the idx_prices_date or PRIMARY KEY (depending on the optimizer). If you often filter by (ticker, date), consider a composite index: CREATE INDEX IF NOT EXISTS idx_prices_ticker_date ON prices(ticker, date); (We’ll include that in homework.)\n\n\n\n8.4.7 6) Save lab outputs\npathlib.Path(\"reports\").mkdir(exist_ok=True)\nfeatures_sql.head(100).to_csv(\"reports/sql_window_demo.csv\", index=False)\ntopk.to_csv(\"reports/top3_abs_moves_per_ticker.csv\", index=False)\nprint(\"Wrote reports/sql_window_demo.csv and reports/top3_abs_moves_per_ticker.csv\")\ncon.close()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Session 8 — SQL II: Window Functions & `pandas.read_sql` Workflows</span>"
    ]
  },
  {
    "objectID": "lec8.html#wrapup-10-min",
    "href": "lec8.html#wrapup-10-min",
    "title": "8  Session 8 — SQL II: Window Functions & pandas.read_sql Workflows",
    "section": "8.5 Wrap‑up (10 min)",
    "text": "8.5 Wrap‑up (10 min)\n\nWindow functions = per‑row context (lags, rolling stats, top‑k per group) with no row collapse.\nPrefer ROWS BETWEEN N PRECEDING AND CURRENT ROW to express true rolling windows.\nNo leakage: only use LAG for features; LEAD is for labels.\nUse pandas.read_sql_query to push computation into SQL and bring back tidy frames.\nIndexes matter; check EXPLAIN QUERY PLAN, and add (ticker, date) index when filtering by both.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Session 8 — SQL II: Window Functions & `pandas.read_sql` Workflows</span>"
    ]
  },
  {
    "objectID": "lec8.html#homework-due-before-session-9",
    "href": "lec8.html#homework-due-before-session-9",
    "title": "8  Session 8 — SQL II: Window Functions & pandas.read_sql Workflows",
    "section": "8.6 Homework (due before Session 9)",
    "text": "8.6 Homework (due before Session 9)\nGoal: Productionize SQL‑side feature engineering and performance basics. You will (A) create a reusable SQL file that defines features using windows, (B) write a small Python runner that writes data/processed/features_sql.parquet, (C) add a composite index, and (D) produce two small reports.\n\n8.6.1 Part A — sql/features_window.sql (reusable)\nCreate sql/features_window.sql:\n-- sql/features_window.sql\n-- Rolling features and lags built with window functions.\nWITH base AS (\n  SELECT\n    ticker, date, log_return AS r_1d\n  FROM prices\n  WHERE date BETWEEN ? AND ?  -- placeholders: start, end\n)\nSELECT\n  ticker, date, r_1d,\n  LAG(r_1d,1) OVER (PARTITION BY ticker ORDER BY date) AS lag1,\n  LAG(r_1d,2) OVER (PARTITION BY ticker ORDER BY date) AS lag2,\n  LAG(r_1d,3) OVER (PARTITION BY ticker ORDER BY date) AS lag3,\n  AVG(r_1d) OVER w20 AS roll_mean_20,\n  AVG(r_1d*r_1d) OVER w20 - (AVG(r_1d) OVER w20)*(AVG(r_1d) OVER w20) AS roll_var_20\nFROM base\nWINDOW w20 AS (\n  PARTITION BY ticker\n  ORDER BY date\n  ROWS BETWEEN 19 PRECEDING AND CURRENT ROW\n)\nORDER BY ticker, date;\n\n\n8.6.2 Part B — Runner: scripts/build_features_sql.py\n# scripts/build_features_sql.py\n#!/usr/bin/env python\nimport argparse, sqlite3, pandas as pd, numpy as np, math\nfrom pathlib import Path\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--db\", default=\"data/prices.db\")\n    ap.add_argument(\"--sqlfile\", default=\"sql/features_window.sql\")\n    ap.add_argument(\"--start\", default=\"2019-01-01\")\n    ap.add_argument(\"--end\",   default=\"2025-08-01\")\n    ap.add_argument(\"--out\",   default=\"data/processed/features_sql.parquet\")\n    ap.add_argument(\"--drop-head\", type=int, default=3, help=\"Drop first N rows per ticker (due to lags)\")\n    args = ap.parse_args()\n\n    Path(args.out).parent.mkdir(parents=True, exist_ok=True)\n    con = sqlite3.connect(args.db)\n    con.create_function(\"SQRT\", 1, lambda x: math.sqrt(x) if x is not None and x&gt;=0 else None)\n\n    sql = Path(args.sqlfile).read_text()\n    df = pd.read_sql_query(sql, con, params=[args.start, args.end])\n\n    # Finish std & z-score in pandas\n    df[\"roll_std_20\"] = (df[\"roll_var_20\"].clip(lower=0)).pow(0.5)\n    df[\"zscore_20\"] = (df[\"r_1d\"] - df[\"roll_mean_20\"]) / df[\"roll_std_20\"].replace(0, pd.NA)\n\n    # Drop first N rows per ticker where lags are NaN\n    df = (df.sort_values([\"ticker\",\"date\"])\n            .groupby(\"ticker\", group_keys=False)\n            .apply(lambda g: g.iloc[args.drop_head:]))\n\n    df.to_parquet(args.out, index=False)\n    print(\"Wrote\", args.out, \"rows:\", len(df))\n\nif __name__ == \"__main__\":\n    main()\nMake it executable:\nimport os, stat, pathlib\np = pathlib.Path(\"scripts/build_features_sql.py\")\nos.chmod(p, os.stat(p).st_mode | stat.S_IEXEC)\nprint(\"Ready:\", p)\n\n\n8.6.3 Part C — Add a composite index and verify plan\n\nCreate sql/add_index_ticker_date.sql:\n\n-- sql/add_index_ticker_date.sql\nCREATE INDEX IF NOT EXISTS idx_prices_ticker_date ON prices(ticker, date);\n\nRunner to apply it (or just execute once in a notebook):\n\nimport sqlite3, pathlib\ncon = sqlite3.connect(\"data/prices.db\")\ncon.executescript(pathlib.Path(\"sql/add_index_ticker_date.sql\").read_text())\ncon.close()\nprint(\"Index created: idx_prices_ticker_date\")\n\nCapture the query plan to a text file:\n\nimport sqlite3, pandas as pd, pathlib\ncon = sqlite3.connect(\"data/prices.db\")\nplan = pd.read_sql_query(\"\"\"\nEXPLAIN QUERY PLAN\nSELECT ticker, date, LAG(log_return,1) OVER (PARTITION BY ticker ORDER BY date)\nFROM prices\nWHERE ticker = ? AND date BETWEEN ? AND ?\nORDER BY date;\n\"\"\", con, params=[\"AAPL\",\"2022-01-01\",\"2025-08-01\"])\npathlib.Path(\"reports\").mkdir(exist_ok=True)\nplan.to_csv(\"reports/query_plan_lag1.csv\", index=False)\ncon.close()\nprint(\"Wrote reports/query_plan_lag1.csv\")\n\n\n8.6.4 Part D — Produce two small reports\n\nTop‑k per ticker (k=5) as CSV:\n\nimport sqlite3, pandas as pd, pathlib\ncon = sqlite3.connect(\"data/prices.db\")\nsql = \"\"\"\nWITH ranked AS (\n  SELECT ticker, date, log_return, ABS(log_return) AS abs_move,\n         ROW_NUMBER() OVER (PARTITION BY ticker ORDER BY ABS(log_return) DESC) AS rn\n  FROM prices\n  WHERE date BETWEEN ? AND ?\n)\nSELECT * FROM ranked WHERE rn &lt;= 5 ORDER BY ticker, rn;\n\"\"\"\ndf = pd.read_sql_query(sql, con, params=[\"2019-01-01\",\"2025-08-01\"])\npathlib.Path(\"reports\").mkdir(exist_ok=True)\ndf.to_csv(\"reports/top5_abs_moves_per_ticker.csv\", index=False)\ncon.close()\nprint(\"Wrote reports/top5_abs_moves_per_ticker.csv\")\n\nFeatures via SQL saved to Parquet:\n\n!python scripts/build_features_sql.py --start 2019-01-01 --end 2025-08-01 --out data/processed/features_sql.parquet\n\n\n8.6.5 Part E — Makefile targets (optional but recommended)\nAppend to your Makefile:\nDB := data/prices.db\nFEATS_SQL := data/processed/features_sql.parquet\n\n.PHONY: features-sql add-index plan\nfeatures-sql: $(FEATS_SQL) ## Build features using SQL windows\n$(FEATS_SQL): scripts/build_features_sql.py sql/features_window.sql $(DB)\n\\tpython scripts/build_features_sql.py --db $(DB) --sqlfile sql/features_window.sql --start $(START) --end $(END) --out $(FEATS_SQL)\n\nadd-index: ## Create composite (ticker,date) index\n\\tsqlite3 $(DB) &lt; sql/add_index_ticker_date.sql\n\nplan: ## Save a sample EXPLAIN QUERY PLAN to reports/\n\\tpython - &lt;&lt; 'PY'\nimport sqlite3, pandas as pd, os\ncon = sqlite3.connect(\"data/prices.db\")\ndf = pd.read_sql_query(\\\"\\\"\\\"\\nEXPLAIN QUERY PLAN\\nSELECT ticker, date, LAG(log_return,1) OVER (PARTITION BY ticker ORDER BY date)\\nFROM prices WHERE ticker=? AND date BETWEEN ? AND ? ORDER BY date;\\n\\\"\\\"\\\", con, params=[\"AAPL\",\"2022-01-01\",\"2025-08-01\"])\nos.makedirs(\"reports\", exist_ok=True)\ndf.to_csv(\"reports/query_plan_lag1.csv\", index=False)\ncon.close()\nprint(\"Wrote reports/query_plan_lag1.csv\")\nPY\nRun:\n%%bash\nset -euo pipefail\ncd \"/content/drive/MyDrive/dspt25/unified-stocks-teamX\"\nmake add-index\nmake features-sql\nmake plan\n\n\n8.6.6 Submission checklist (pass/revise)\n\nsql/features_window.sql present; uses WINDOW w20 and LAG for 1–3 lags.\nscripts/build_features_sql.py runs and writes data/processed/features_sql.parquet.\nComposite index created (idx_prices_ticker_date).\nreports/top5_abs_moves_per_ticker.csv and reports/query_plan_lag1.csv generated.\n(Optional) Makefile updated with features-sql, add-index, plan.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Session 8 — SQL II: Window Functions & `pandas.read_sql` Workflows</span>"
    ]
  },
  {
    "objectID": "lec8.html#key-points",
    "href": "lec8.html#key-points",
    "title": "8  Session 8 — SQL II: Window Functions & pandas.read_sql Workflows",
    "section": "8.7 Key Points:",
    "text": "8.7 Key Points:\n\nNo leakage: features must come from LAG, not LEAD.\nRolling stats with windows are declarative and fast; avoid reinventing in pandas unless needed.\nUse WINDOW names to reduce errors and duplication.\nCheck query plans and add indexes purposefully.\n\nNext up (Session 9): Finance‑specific evaluation & leakage control — walk‑forward splits, embargo, and regime‑aware error analysis.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Session 8 — SQL II: Window Functions & `pandas.read_sql` Workflows</span>"
    ]
  },
  {
    "objectID": "lec9.html",
    "href": "lec9.html",
    "title": "9  Session 9 — Cleaning, Joins, and Parquet",
    "section": "",
    "text": "9.1 Session 9 — Cleaning, Joins, Parquet (75 min)\nAssumptions: Same Drive‑mounted repo (e.g., unified-stocks-teamX) as prior sessions. Your raw prices live under data/raw/ (either a single prices.csv or multiple CSVs). The lab includes a safe fallback (small synthetic dataset) if raw files are missing.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Session 9 — Cleaning, Joins, and Parquet</span>"
    ]
  },
  {
    "objectID": "lec9.html#session-9-cleaning-joins-parquet-75-min",
    "href": "lec9.html#session-9-cleaning-joins-parquet-75-min",
    "title": "9  Session 9 — Cleaning, Joins, and Parquet",
    "section": "",
    "text": "9.1.1 Learning goals\nBy the end of class, students can:\n\nUse merge, assign, and pipe to write clean, testable data‑wrangling code.\nChoose sensible dtypes for analytics and storage: category, pandas nullable integers (Int64, Int32, …), and string.\nWrite Parquet with compression and read it back; understand partitioning by ticker and how to filter efficiently.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Session 9 — Cleaning, Joins, and Parquet</span>"
    ]
  },
  {
    "objectID": "lec9.html#agenda-75-min",
    "href": "lec9.html#agenda-75-min",
    "title": "9  Session 9 — Cleaning, Joins, and Parquet",
    "section": "9.2 Agenda (75 min)",
    "text": "9.2 Agenda (75 min)\n\n(10 min) Slides: tidy schema; joins (merge), assign, pipe patterns\n(10 min) Slides: dtypes—category, string, nullable ints (Int64), float32 vs float64\n(10 min) Slides: Parquet vs CSV; compression; partitioning; schema preservation\n(35 min) In‑class lab: clean & join → set dtypes → write data/processed/prices.parquet and partitioned dataset by ticker\n(10 min) Wrap‑up & homework briefing",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Session 9 — Cleaning, Joins, and Parquet</span>"
    ]
  },
  {
    "objectID": "lec9.html#slides-talking-points-paste-these-bullets-into-your-deck",
    "href": "lec9.html#slides-talking-points-paste-these-bullets-into-your-deck",
    "title": "9  Session 9 — Cleaning, Joins, and Parquet",
    "section": "9.3 Slides / talking points (paste these bullets into your deck)",
    "text": "9.3 Slides / talking points (paste these bullets into your deck)\n\n9.3.1 Tidy schema for price data\n\nOne row = one ticker‑day.\nMinimal columns (snake_case): date (datetime64[ns]), ticker (category), open/high/low/close/adj_close (float32/64), volume (Int64).\nOptional metadata (from a separate table): name (string), sector (category).\n\n\n\n9.3.2 Idiomatic pandas: merge, assign, pipe\n\nmerge: combine frames by keys (e.g., prices ⟵left join⟶ tickers).\nassign: add/transform columns without breaking the chain: df = df.assign(adj_close=lambda d: d['adj_close'].fillna(d['close'])).\npipe: compose small, testable transforms: df = (raw.pipe(standardize_columns).pipe(clean_prices).pipe(join_meta, meta=meta)).\n\n\n\n9.3.3 Dtypes that help\n\nCategorical (category): compact & fast for low‑cardinality strings (ticker, sector).\nNullable integers (Int64, Int32): keep missing values and integer semantics (volume).\nString (string[python]): consistent string semantics (avoid object).\nFloats: float32 can halve memory, but consider numeric precision.\n\n\n\n9.3.4 Parquet: why & how\n\nColumnar, compressed, preserves schema better than CSV.\nFilters & projection: read only needed columns/rows (esp. with partitioned datasets).\nPartitioning by ticker/ yields fast reads of a subset (e.g., a single ticker).\nTypical settings: engine=pyarrow, compression=zstd or snappy.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Session 9 — Cleaning, Joins, and Parquet</span>"
    ]
  },
  {
    "objectID": "lec9.html#inclass-lab-35-min",
    "href": "lec9.html#inclass-lab-35-min",
    "title": "9  Session 9 — Cleaning, Joins, and Parquet",
    "section": "9.4 In‑class lab (35 min)",
    "text": "9.4 In‑class lab (35 min)\n\nRun each block as its own Colab cell. Replace REPO_NAME to match your repo.\n\n\n9.4.1 0) Setup: mount Drive, cd into repo, ensure folders\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\n# ✏️ Change this to your repo folder name\nREPO_NAME  = \"unified-stocks-teamX\"\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, sys, glob, pandas as pd, numpy as np\npathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\nassert pathlib.Path(REPO_DIR).exists(), \"Repo not found. Clone it first (Session 2/3).\"\nos.chdir(REPO_DIR)\nfor p in [\"data/raw\", \"data/static\", \"data/processed\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\n\n9.4.2 1) Locate raw price files (CSV) or create a fallback\nfrom pathlib import Path\nimport pandas as pd, numpy as np, datetime as dt\n\nraw_candidates = []\nif Path(\"data/raw/prices.csv\").exists():\n    raw_candidates = [\"data/raw/prices.csv\"]\nelse:\n    raw_candidates = sorted(glob.glob(\"data/raw/prices*.csv\")) or sorted(glob.glob(\"data/raw/prices/*.csv\"))\n\ndef _make_synthetic_prices():\n    # Small 2-year synthetic daily prices for AAPL/MSFT/GOOGL\n    tickers = [\"AAPL\",\"MSFT\",\"GOOGL\"]\n    dates = pd.bdate_range(\"2022-01-03\", periods=520, freq=\"B\")\n    rows = []\n    rng = np.random.default_rng(0)\n    for t in tickers:\n        price = 100 + rng.normal(0, 1).cumsum()\n        price = np.maximum(price, 1.0)\n        vol = rng.integers(5e6, 2e7, size=len(dates))\n        df = pd.DataFrame({\n            \"date\": dates,\n            \"ticker\": t,\n            \"open\": price * (1 + rng.normal(0, 0.002, size=len(dates))),\n            \"high\": price * (1 + rng.normal(0.003, 0.003, size=len(dates))).clip(min=1),\n            \"low\":  price * (1 - np.abs(rng.normal(0.003, 0.003, size=len(dates)))),\n            \"close\": price,\n            \"adj_close\": price * (1 + rng.normal(0, 0.0005, size=len(dates))),\n            \"volume\": vol\n        })\n        rows.append(df)\n    out = pd.concat(rows, ignore_index=True)\n    Path(\"data/raw\").mkdir(parents=True, exist_ok=True)\n    out.to_csv(\"data/raw/prices.csv\", index=False)\n    return [\"data/raw/prices.csv\"]\n\nif not raw_candidates:\n    print(\"No raw prices found; creating a small synthetic dataset...\")\n    raw_candidates = _make_synthetic_prices()\n\nraw_candidates\n\n\n9.4.3 2) Optional metadata (tickers table), or create a minimal one\nfrom pathlib import Path\nmeta_path = Path(\"data/static/tickers.csv\")\nif meta_path.exists():\n    meta = pd.read_csv(meta_path)\nelse:\n    # Build a minimal metadata table from raw tickers\n    tmp = pd.read_csv(raw_candidates[0])\n    tickers = sorted(pd.unique(tmp[\"ticker\"]))\n    meta = pd.DataFrame({\"ticker\": tickers,\n                         \"name\": tickers,\n                         \"sector\": [\"Unknown\"]*len(tickers)})\n    Path(\"data/static\").mkdir(parents=True, exist_ok=True)\n    meta.to_csv(meta_path, index=False)\nmeta.head()\n\n\n9.4.4 3) Helpers: standardize_columns, clean_prices, join_meta (showing merge/assign/pipe)\nimport re\n\ndef standardize_columns(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Lowercase snake_case; repair common price column name variants.\"\"\"\n    def snake(s):\n        s = re.sub(r\"[^\\w\\s]\", \"_\", s)\n        s = re.sub(r\"\\s+\", \"_\", s.strip().lower())\n        s = re.sub(r\"_+\", \"_\", s)\n        return s\n    out = df.copy()\n    out.columns = [snake(c) for c in out.columns]\n    # Normalize known variants\n    ren = {\n        \"adjclose\":\"adj_close\", \"adj_close_\":\"adj_close\",\n        \"close_adj\":\"adj_close\", \"adj_close_close\":\"adj_close\"\n    }\n    out = out.rename(columns={k:v for k,v in ren.items() if k in out.columns})\n    # If no adj_close but close exists, create it\n    if \"adj_close\" not in out and \"close\" in out:\n        out = out.assign(adj_close=out[\"close\"])\n    return out\n\ndef clean_prices(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Coerce dtypes, drop dupes, basic sanity checks; add minor derived fields.\"\"\"\n    cols = [\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"]\n    keep = [c for c in cols if c in df.columns]\n    out = df.loc[:, keep].copy()\n\n    # Parse date, coerce numerics\n    out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n    for c in [\"open\",\"high\",\"low\",\"close\",\"adj_close\"]:\n        if c in out: out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n    if \"volume\" in out: out[\"volume\"] = pd.to_numeric(out[\"volume\"], errors=\"coerce\")\n\n    # Drop bad rows\n    out = out.dropna(subset=[\"date\",\"ticker\",\"adj_close\"])\n    # Deduplicate by (ticker, date)\n    out = out.sort_values([\"ticker\",\"date\"])\n    out = out.drop_duplicates(subset=[\"ticker\",\"date\"], keep=\"last\")\n\n    # Enforce dtypes\n    if \"volume\" in out:\n        out[\"volume\"] = out[\"volume\"].round().astype(\"Int64\")  # nullable int\n        out.loc[out[\"volume\"] &lt; 0, \"volume\"] = pd.NA\n    # Use category for low-cardinality strings\n    out[\"ticker\"] = out[\"ticker\"].astype(\"category\")\n    # Use consistent float dtype\n    for c in [\"open\",\"high\",\"low\",\"close\",\"adj_close\"]:\n        if c in out: out[c] = out[c].astype(\"float32\")  # ok for teaching; change to float64 if you need more precision\n\n    # Quick sanity checks\n    assert out[[\"ticker\",\"date\"]].duplicated().sum() == 0, \"Duplicates remain\"\n    assert pd.api.types.is_datetime64_any_dtype(out[\"date\"]), \"date not datetime\"\n    return out.reset_index(drop=True)\n\ndef join_meta(prices: pd.DataFrame, meta: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Left join metadata; keep minimal meta columns; set dtypes.\"\"\"\n    keep_meta = [c for c in [\"ticker\",\"name\",\"sector\"] if c in meta.columns]\n    meta2 = meta.loc[:, keep_meta].copy()\n    # Make strings consistent and compact\n    if \"name\" in meta2:   meta2[\"name\"]   = meta2[\"name\"].astype(\"string\")\n    if \"sector\" in meta2: meta2[\"sector\"] = meta2[\"sector\"].astype(\"category\")\n    out = prices.merge(meta2, on=\"ticker\", how=\"left\", validate=\"many_to_one\")\n    return out\n\n\n9.4.5 4) Read, clean, and join all raw files using a pipeline\ndfs = []\nfor path in raw_candidates:\n    raw = pd.read_csv(path)\n    tidy = (raw\n            .pipe(standardize_columns)  # &lt;- consistent names\n            .pipe(clean_prices))        # &lt;- dtypes and sanity checks\n    dfs.append(tidy)\n\nprices = pd.concat(dfs, ignore_index=True)\nprices = prices.pipe(join_meta, meta=meta)\n\nprint(\"Preview:\")\ndisplay(prices.head(3))\nprint(\"\\nInfo:\")\nprint(prices.info(memory_usage=\"deep\"))\n\n\n9.4.6 5) Save clean data to Parquet (single file) and partitioned by ticker\n# Single-file Parquet\nsingle_path = \"data/processed/prices.parquet\"\nprices.to_parquet(single_path, engine=\"pyarrow\", compression=\"zstd\", index=False)\nprint(\"Wrote:\", single_path)\n\n# Partitioned dataset by ticker (directory with /ticker=.../)\npart_dir = \"data/processed/prices_by_ticker\"\n# pandas to_parquet supports partition_cols with pyarrow engine\ntry:\n    prices.to_parquet(part_dir, engine=\"pyarrow\", compression=\"zstd\",\n                      index=False, partition_cols=[\"ticker\"])\n    print(\"Wrote partitioned dataset:\", part_dir)\nexcept TypeError:\n    # Fallback via pyarrow dataset API\n    import pyarrow as pa, pyarrow.parquet as pq\n    pa_tbl = pa.Table.from_pandas(prices, preserve_index=False)\n    pq.write_to_dataset(pa_tbl, root_path=part_dir, partition_cols=[\"ticker\"], compression=\"zstd\")\n    print(\"Wrote (fallback) partitioned dataset:\", part_dir)\n\n\n9.4.7 6) Read back efficiently: projection + simple filters\n# 6a) Read a few columns from single-file Parquet\ncols = [\"ticker\",\"date\",\"adj_close\",\"volume\"]\ndf_small = pd.read_parquet(\"data/processed/prices.parquet\", columns=cols)\ndf_small.head()\n\n# 6b) Read one ticker from the partitioned dataset using pyarrow.dataset\nimport pyarrow.dataset as ds\ndataset = ds.dataset(\"data/processed/prices_by_ticker\", format=\"parquet\", partitioning=\"hive\")\n# Choose a ticker present in the data\none_ticker = str(prices[\"ticker\"].astype(\"category\").cat.categories[0])\nflt = (ds.field(\"ticker\") == one_ticker)\ntbl = dataset.to_table(filter=flt, columns=[\"date\",\"adj_close\",\"volume\"])\ndf_one = tbl.to_pandas()\ndf_one.head()\n\n\n9.4.8 7) Persist a simple schema record for reproducibility\nimport json, pathlib\nschema = {c: str(t) for c,t in prices.dtypes.items()}\npathlib.Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\nwith open(\"data/processed/prices_schema.json\",\"w\") as f:\n    json.dump(schema, f, indent=2)\nprint(\"Wrote data/processed/prices_schema.json\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Session 9 — Cleaning, Joins, and Parquet</span>"
    ]
  },
  {
    "objectID": "lec9.html#wrapup-10-min-points-to-emphasize",
    "href": "lec9.html#wrapup-10-min-points-to-emphasize",
    "title": "9  Session 9 — Cleaning, Joins, and Parquet",
    "section": "9.5 Wrap‑up (10 min) — points to emphasize",
    "text": "9.5 Wrap‑up (10 min) — points to emphasize\n\nA tidy schema makes life easier downstream.\nPrefer category for ticker, nullable ints for volume.\nUse merge (left join) to attach metadata; use assign for clear column creation; compose steps with pipe.\nParquet is compact and fast; partition by ticker for selective reads.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Session 9 — Cleaning, Joins, and Parquet</span>"
    ]
  },
  {
    "objectID": "lec9.html#homework-due-before-session-10",
    "href": "lec9.html#homework-due-before-session-10",
    "title": "9  Session 9 — Cleaning, Joins, and Parquet",
    "section": "9.6 Homework (due before Session 10)",
    "text": "9.6 Homework (due before Session 10)\nDeliverable: data/processed/returns.parquet (and optionally partitioned by ticker) containing:\n\ndate, ticker\nlog_return — daily log return \\(\\log(\\frac{\\text{adj\\_close}_t}{\\text{adj\\_close}_{t-1}})\\)\nr_1d — next‑day log return (lead of log_return)\nweekday (0=Mon..6=Sun), month (1..12) — choose compact dtypes\n\n\n9.6.1 Step‑by‑step code (Colab‑friendly)\n\nRun in your repo root after finishing the in‑class lab.\n\nimport pandas as pd, numpy as np, pathlib\n\n# 1) Read the single-file Parquet you wrote in class\nprices_path = \"data/processed/prices.parquet\"\nassert pathlib.Path(prices_path).exists(), \"Missing processed/prices.parquet — finish the lab first.\"\n\nprices = pd.read_parquet(prices_path)\n# If ticker was written as object, you can re-cast to category:\nif prices[\"ticker\"].dtype != \"category\":\n    prices[\"ticker\"] = prices[\"ticker\"].astype(\"category\")\n\n# 2) Sort and compute returns per ticker (no leakage)\nprices = prices.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n\n# Daily log return: log(adj_close_t / adj_close_{t-1})\nprices[\"log_return\"] = (prices.groupby(\"ticker\")[\"adj_close\"]\n                        .apply(lambda s: np.log(s / s.shift(1))).reset_index(level=0, drop=True))\n\n# Next-day return label r_1d = lead(log_return, 1)\nprices[\"r_1d\"] = (prices.groupby(\"ticker\")[\"log_return\"]\n                  .shift(-1))\n\n# 3) Calendar features\nprices[\"weekday\"] = prices[\"date\"].dt.weekday.astype(\"int8\")  # 0..6\nprices[\"month\"]   = prices[\"date\"].dt.month.astype(\"int8\")    # 1..12\n\n# 4) Select output columns & dtypes\nout = prices[[\"date\",\"ticker\",\"log_return\",\"r_1d\",\"weekday\",\"month\"]].copy()\nout[\"ticker\"] = out[\"ticker\"].astype(\"category\")\n\n# 5) Save to Parquet\nout_path = \"data/processed/returns.parquet\"\nout.to_parquet(out_path, engine=\"pyarrow\", compression=\"zstd\", index=False)\nprint(\"Wrote:\", out_path)\n\n# 6) (Optional) also write a partitioned dataset by ticker\npart_dir = \"data/processed/returns_by_ticker\"\ntry:\n    out.to_parquet(part_dir, engine=\"pyarrow\", compression=\"zstd\",\n                   index=False, partition_cols=[\"ticker\"])\n    print(\"Wrote partitioned dataset:\", part_dir)\nexcept TypeError:\n    import pyarrow as pa, pyarrow.parquet as pq\n    pq.write_to_dataset(pa.Table.from_pandas(out, preserve_index=False),\n                        root_path=part_dir, partition_cols=[\"ticker\"], compression=\"zstd\")\n    print(\"Wrote (fallback) partitioned dataset:\", part_dir)\n\n\n9.6.2 Quick self‑check (run after saving)\nimport pandas as pd\nr = pd.read_parquet(\"data/processed/returns.parquet\")\nassert {\"date\",\"ticker\",\"log_return\",\"r_1d\",\"weekday\",\"month\"}.issubset(r.columns)\nassert r[\"ticker\"].dtype.name in (\"category\",\"CategoricalDtype\"), \"ticker should be categorical\"\nprint(\"rows:\", len(r), \"| tickers:\", r[\"ticker\"].nunique())\nr.head()\n\n\n9.6.3 (Optional)\n\nAdd year (Int16) and is_month_end (BooleanDtype): r[\"year\"] = r[\"date\"].dt.year.astype(\"Int16\") r[\"is_month_end\"] = r[\"date\"].dt.is_month_end.astype(\"boolean\")\nCompare file sizes: CSV vs Parquet vs Parquet (zstd vs snappy).\n\n\n\n9.6.4 Submission checklist (pass/revise)\n\ndata/processed/returns.parquet exists and contains the required columns.\nticker is categorical; weekday/month are compact ints.\nr_1d is a lead of log_return (next‑day), not the same‑day return.\nYou can read it back without errors.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Session 9 — Cleaning, Joins, and Parquet</span>"
    ]
  },
  {
    "objectID": "lec9.html#instructor-notes-gotchas-to-watch-for",
    "href": "lec9.html#instructor-notes-gotchas-to-watch-for",
    "title": "9  Session 9 — Cleaning, Joins, and Parquet",
    "section": "9.7 Instructor notes / gotchas to watch for",
    "text": "9.7 Instructor notes / gotchas to watch for\n\nNullable ints: astype(\"Int64\") keeps NAs; plain int64 will fail if NAs exist.\nCategoricals & partitions: When reading partitioned Parquet, ticker may come back as object. Re‑cast to category after read if needed.\nCompression choice: zstd gives good ratio/speed; snappy is more ubiquitous.\nPrecision: float32 is fine for teaching; for production finance, consider float64 and explicit rounding.\n\n\n\n9.7.1 Optional (for your Makefile later)\nAdd quick targets:\n.PHONY: prices-parquet returns-parquet\nprices-parquet:  ## Clean raw prices and save processed Parquet(s)\n\\tpython - &lt;&lt;'PY'\nimport pandas as pd, glob, pathlib, numpy as np, re, json\nfrom pathlib import Path\n# (Paste the functions from the lab: standardize_columns, clean_prices, join_meta)\n# Then read raw -&gt; clean -&gt; write parquet as in the lab\nPY\n\nreturns-parquet: ## Build returns.parquet with r_1d + calendar features\n\\tpython - &lt;&lt;'PY'\nimport pandas as pd, numpy as np\np=\"data/processed/prices.parquet\"; r=pd.read_parquet(p).sort_values([\"ticker\",\"date\"])\nr[\"log_return\"]=r.groupby(\"ticker\")[\"adj_close\"].apply(lambda s: np.log(s/s.shift(1))).reset_index(level=0, drop=True)\nr[\"r_1d\"]=r.groupby(\"ticker\")[\"log_return\"].shift(-1)\nr[\"weekday\"]=r[\"date\"].dt.weekday.astype(\"int8\"); r[\"month\"]=r[\"date\"].dt.month.astype(\"int8\")\nr[[\"date\",\"ticker\",\"log_return\",\"r_1d\",\"weekday\",\"month\"]].to_parquet(\"data/processed/returns.parquet\", compression=\"zstd\", index=False)\nprint(\"Wrote data/processed/returns.parquet\")\nPY\nYou now have a clean, tidy Parquet foundation the later sessions (evaluation & modeling) can rely on.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Session 9 — Cleaning, Joins, and Parquet</span>"
    ]
  },
  {
    "objectID": "lec10.html",
    "href": "lec10.html",
    "title": "10  Rolling Windows, Resampling, and Leakage‑Safe Features",
    "section": "",
    "text": "11 Session 10 — Rolling Windows, Resampling, and Leakage‑Safe Features\nEducational use only — not trading advice. Python‑only. Colab + Drive assumed. If you don’t already have the repo and folders used below, the first cells create them.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Rolling Windows, Resampling, and Leakage‑Safe Features</span>"
    ]
  },
  {
    "objectID": "lec10.html#agenda-75-min",
    "href": "lec10.html#agenda-75-min",
    "title": "10  Rolling Windows, Resampling, and Leakage‑Safe Features",
    "section": "11.1 Agenda (75 min)",
    "text": "11.1 Agenda (75 min)\n\n(10 min) Slides: leakage‑free features; lags vs rolling; resampling patterns\n(10 min) Slides: common pitfalls (min_periods, alignment, mixed frequencies)\n(35 min) In‑class lab: load returns → build features → (optional) weekly aggregates → write features_v1.parquet\n(10 min) Wrap‑up + homework brief\n(10 min) Buffer",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Rolling Windows, Resampling, and Leakage‑Safe Features</span>"
    ]
  },
  {
    "objectID": "lec10.html#slide-talking-points",
    "href": "lec10.html#slide-talking-points",
    "title": "10  Rolling Windows, Resampling, and Leakage‑Safe Features",
    "section": "11.2 Slide talking points",
    "text": "11.2 Slide talking points\nFeature timing = everything\n\nPredict \\(r_{t+1}\\) using info up to and including time t.\nRule: compute any rolling stat at \\(t\\) from data \\(\\le t\\), then shift by 1 if that stat includes the current target variable.\n\nCore pandas patterns\n\nLags: s.shift(k) (past), never negative shifts.\nRolling: s.rolling(W, min_periods=W).agg(...) and then no extra shift if the rolling window ends at \\(t\\).\nExpanding: long‑memory features (e.g., expanding mean).\nEWM: s.ewm(span=W, adjust=False).mean() for decayed memory.\n\nResampling safely\n\nUse groupby('ticker').resample('W-FRI', on='date') then aggregate:\n\nOHLC: first/open, max/high, min/low, last/adj_close\nVolume: sum\nReturns: compound via np.log(prod(1+r)) or sum of log returns.\n\n\nDtypes\n\nticker = category; calendar ints int8; features float32 (fine for class).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Rolling Windows, Resampling, and Leakage‑Safe Features</span>"
    ]
  },
  {
    "objectID": "lec10.html#inclass-lab-colabfriendly",
    "href": "lec10.html#inclass-lab-colabfriendly",
    "title": "10  Rolling Windows, Resampling, and Leakage‑Safe Features",
    "section": "11.2 In‑class lab (Colab‑friendly)",
    "text": "11.2 In‑class lab (Colab‑friendly)\n\nRun each block as its own cell. Adjust REPO_NAME as needed.\n\n\n11.2.1 0) Setup\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"   # &lt;- change if needed\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, numpy as np, pandas as pd\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/raw\",\"data/processed\",\"reports\",\"scripts\",\"tests\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\n\n11.2.2 1) Load inputs or build small fallbacks\nfrom pathlib import Path\nrng = np.random.default_rng(0)\n\n# Fallback synthetic if missing\ndef make_synth_prices():\n    dates = pd.bdate_range(\"2022-01-03\", periods=300)\n    frames=[]\n    for tkr in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\"]:\n        base = 100 + rng.normal(0,1, size=len(dates)).cumsum()\n        d = pd.DataFrame({\n            \"date\": dates, \"ticker\": tkr,\n            \"adj_close\": np.maximum(base, 1.0).astype(\"float32\"),\n            \"volume\": rng.integers(1e6, 5e6, size=len(dates)).astype(\"int64\")\n        })\n        frames.append(d)\n    prices = pd.concat(frames, ignore_index=True)\n    prices[\"ticker\"] = prices[\"ticker\"].astype(\"category\")\n    prices.to_parquet(\"data/processed/prices.parquet\", index=False)\n    return prices\n\nppath = Path(\"data/processed/prices.parquet\")\nrpath = Path(\"data/processed/returns.parquet\")\n\nif ppath.exists():\n    prices = pd.read_parquet(ppath)\nelse:\n    prices = make_synth_prices()\n\n# Build returns if missing (from Session 9 logic)\nif rpath.exists():\n    returns = pd.read_parquet(rpath)\nelse:\n    df = prices.sort_values([\"ticker\",\"date\"]).copy()\n    df[\"log_return\"] = (df.groupby(\"ticker\")[\"adj_close\"]\n                        .apply(lambda s: np.log(s/s.shift(1))).reset_index(level=0, drop=True))\n    df[\"r_1d\"] = df.groupby(\"ticker\")[\"log_return\"].shift(-1)\n    df[\"weekday\"] = df[\"date\"].dt.weekday.astype(\"int8\")\n    df[\"month\"]   = df[\"date\"].dt.month.astype(\"int8\")\n    returns = df[[\"date\",\"ticker\",\"log_return\",\"r_1d\",\"weekday\",\"month\"]].copy()\n    returns[\"ticker\"] = returns[\"ticker\"].astype(\"category\")\n    returns.to_parquet(\"data/processed/returns.parquet\", index=False)\n\nprices.head(3), returns.head(3)\n\n\n11.2.3 2) Rolling, lag, expanding, ewm features (no leakage)\ndef build_features(ret: pd.DataFrame, windows=(5,10,20), add_rsi=True):\n    g = ret.sort_values([\"ticker\",\"date\"]).groupby(\"ticker\", group_keys=False)\n    out = ret.copy()\n\n    # Lags of log_return (past info)\n    for k in [1,2,3]:\n        out[f\"lag{k}\"] = g[\"log_return\"].shift(k)\n\n    # Rolling mean/std and z-score of returns using past W days **including today**,\n    # which is fine because target is r_{t+1}. No extra shift needed.\n    for W in windows:\n        rm = g[\"log_return\"].rolling(W, min_periods=W).mean()\n        rsd= g[\"log_return\"].rolling(W, min_periods=W).std()\n        out[f\"roll_mean_{W}\"] = rm.reset_index(level=0, drop=True)\n        out[f\"roll_std_{W}\"]  = rsd.reset_index(level=0, drop=True)\n        out[f\"zscore_{W}\"]    = (out[\"log_return\"] - out[f\"roll_mean_{W}\"]) / (out[f\"roll_std_{W}\"] + 1e-8)\n\n    # Expanding stats (from start to t): long-memory\n    out[\"exp_mean\"] = g[\"log_return\"].expanding(min_periods=20).mean().reset_index(level=0, drop=True)\n    out[\"exp_std\"]  = g[\"log_return\"].expanding(min_periods=20).std().reset_index(level=0, drop=True)\n\n    # Exponential weighted (decayed memory)\n    for W in [10,20]:\n        out[f\"ewm_mean_{W}\"] = g[\"log_return\"].apply(lambda s: s.ewm(span=W, adjust=False).mean())\n        out[f\"ewm_std_{W}\"]  = g[\"log_return\"].apply(lambda s: s.ewm(span=W, adjust=False).std())\n\n    # Optional RSI(14) using returns sign proxy (toy version)\n    if add_rsi:\n        def rsi14(s):\n            delta = s.diff()\n            up = delta.clip(lower=0).ewm(alpha=1/14, adjust=False).mean()\n            dn = (-delta.clip(upper=0)).ewm(alpha=1/14, adjust=False).mean()\n            rs = up / (dn + 1e-12)\n            return 100 - (100 / (1 + rs))\n        out[\"rsi_14\"] = g[\"adj_close\"].apply(rsi14) if \"adj_close\" in out else g[\"log_return\"].apply(rsi14)\n\n    # Cast dtypes\n    for c in out.columns:\n        if c not in [\"date\",\"ticker\",\"weekday\",\"month\"] and pd.api.types.is_float_dtype(out[c]):\n            out[c] = out[c].astype(\"float32\")\n    out[\"ticker\"] = out[\"ticker\"].astype(\"category\")\n    return out\n\n# Merge adj_close and volume into returns (if not already)\nret2 = returns.merge(prices[[\"ticker\",\"date\",\"adj_close\",\"volume\"]], on=[\"ticker\",\"date\"], how=\"left\")\nfeatures = build_features(ret2, windows=(5,10,20), add_rsi=True)\nfeatures.head(5)\n\n\n11.2.4 3) (Optional) Weekly resampling demo (OHLCV + returns)\n# Safe weekly resample per ticker, aggregating OHLCV and log returns\ndef weekly_ohlcv(df):\n    df = df.sort_values([\"ticker\",\"date\"]).copy()\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    res=[]\n    for tkr, g in df.groupby(\"ticker\"):\n        wk = (g.resample(\"W-FRI\", on=\"date\")\n              .agg({\"adj_close\":\"last\",\"volume\":\"sum\"}).dropna().reset_index())\n        wk[\"ticker\"] = tkr\n        # Weekly log return = log(adj_close_t / adj_close_{t-1})\n        wk = wk.sort_values(\"date\")\n        wk[\"wk_log_return\"] = np.log(wk[\"adj_close\"]/wk[\"adj_close\"].shift(1))\n        res.append(wk)\n    return pd.concat(res, ignore_index=True)\n\nweekly = weekly_ohlcv(prices[[\"ticker\",\"date\",\"adj_close\",\"volume\"]])\nweekly.head(5)\n\n\n11.2.5 4) Save features_v1.parquet (+ optional partition by ticker)\n# Select a compact set to start with\nkeep = [\"date\",\"ticker\",\"log_return\",\"r_1d\",\"weekday\",\"month\",\n        \"lag1\",\"lag2\",\"lag3\",\n        \"roll_mean_5\",\"roll_std_5\",\"zscore_5\",\n        \"roll_mean_10\",\"roll_std_10\",\"zscore_10\",\n        \"roll_mean_20\",\"roll_std_20\",\"zscore_20\",\n        \"ewm_mean_10\",\"ewm_std_10\",\"ewm_mean_20\",\"ewm_std_20\",\n        \"exp_mean\",\"exp_std\",\"rsi_14\",\"adj_close\",\"volume\"]\n\nkeep = [c for c in keep if c in features.columns]\nfv1 = features.loc[:, keep].dropna().sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nfv1[\"weekday\"] = fv1[\"weekday\"].astype(\"int8\")\nfv1[\"month\"]   = fv1[\"month\"].astype(\"int8\")\nfv1[\"ticker\"]  = fv1[\"ticker\"].astype(\"category\")\n\nfv1_path = \"data/processed/features_v1.parquet\"\nfv1.to_parquet(fv1_path, compression=\"zstd\", index=False)\nprint(\"Wrote:\", fv1_path, \"| rows:\", len(fv1), \"| cols:\", len(fv1.columns))\n\n# Optional partition\npart_dir = \"data/processed/features_v1_by_ticker\"\ntry:\n    fv1.to_parquet(part_dir, compression=\"zstd\", index=False, engine=\"pyarrow\", partition_cols=[\"ticker\"])\n    print(\"Wrote partitioned:\", part_dir)\nexcept TypeError:\n    print(\"Partition writing skipped (engine missing).\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Rolling Windows, Resampling, and Leakage‑Safe Features</span>"
    ]
  },
  {
    "objectID": "lec10.html#wrapup-what-to-emphasize",
    "href": "lec10.html#wrapup-what-to-emphasize",
    "title": "10  Rolling Windows, Resampling, and Leakage‑Safe Features",
    "section": "11.3 Wrap‑up (what to emphasize)",
    "text": "11.3 Wrap‑up (what to emphasize)\n\nFor next‑day targets \\(r_{t+1}\\), rolling stats up to t are fine; never use future rows.\nBe explicit about min_periods to avoid unstable early rows.\nKeep features small and typed; document your cookbook in the repo.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Rolling Windows, Resampling, and Leakage‑Safe Features</span>"
    ]
  },
  {
    "objectID": "lec10.html#homework-due-before-session-11",
    "href": "lec10.html#homework-due-before-session-11",
    "title": "10  Rolling Windows, Resampling, and Leakage‑Safe Features",
    "section": "11.4 Homework (due before Session 11)",
    "text": "11.4 Homework (due before Session 11)\nGoal: Add an automated leakage check and re‑run feature build.\n\n11.4.1 A. Script: scripts/build_features_v1.py\n#!/usr/bin/env python\nimport numpy as np, pandas as pd, pathlib\ndef build():\n    p = pathlib.Path(\"data/processed/returns.parquet\")\n    if not p.exists(): raise SystemExit(\"Missing returns.parquet — finish Session 9.\")\n    prices = pd.read_parquet(\"data/processed/prices.parquet\")\n    ret = pd.read_parquet(p)\n    ret2 = ret.merge(prices[[\"ticker\",\"date\",\"adj_close\",\"volume\"]], on=[\"ticker\",\"date\"], how=\"left\")\n    # (Paste the build_features() from class)\n    # ...\n    fv1 = build_features(ret2)\n    keep = [\"date\",\"ticker\",\"log_return\",\"r_1d\",\"weekday\",\"month\",\n            \"lag1\",\"lag2\",\"lag3\",\"roll_mean_20\",\"roll_std_20\",\"zscore_20\",\n            \"ewm_mean_20\",\"ewm_std_20\",\"exp_mean\",\"exp_std\",\"adj_close\",\"volume\"]\n    keep = [c for c in keep if c in fv1.columns]\n    fv1 = fv1[keep].dropna().sort_values([\"ticker\",\"date\"])\n    fv1.to_parquet(\"data/processed/features_v1.parquet\", compression=\"zstd\", index=False)\n    print(\"Wrote data/processed/features_v1.parquet\", fv1.shape)\nif __name__ == \"__main__\":\n    build()\nMake executable:\n%%bash\nchmod +x scripts/build_features_v1.py\npython scripts/build_features_v1.py\n\n\n11.4.2 B. Test: tests/test_no_lookahead.py\nimport pandas as pd, numpy as np\n\ndef test_features_no_lookahead():\n    df = pd.read_parquet(\"data/processed/features_v1.parquet\").sort_values([\"ticker\",\"date\"])\n    # For each ticker, recompute roll_mean_20 with an independent method and compare\n    for tkr, g in df.groupby(\"ticker\"):\n        s = g[\"log_return\"]\n        rm = s.rolling(20, min_periods=20).mean()\n        # Our feature should equal this rolling mean (within tol)\n        if \"roll_mean_20\" in g:\n            assert np.allclose(g[\"roll_mean_20\"].values, rm.values, equal_nan=True, atol=1e-7)\n        # r_1d must be the **lead** of log_return\n        assert g[\"r_1d\"].shift(1).iloc[21:].equals(g[\"log_return\"].iloc[21:])\nRun:\n%%bash\npytest -q",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Rolling Windows, Resampling, and Leakage‑Safe Features</span>"
    ]
  },
  {
    "objectID": "lec11.html",
    "href": "lec11.html",
    "title": "11  Session 11 — APIs with requests: Secrets, Retries, and Caching",
    "section": "",
    "text": "11.1 Learning goals\nStudents will be able to:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Session 11 — APIs with `requests`: Secrets, Retries, and Caching</span>"
    ]
  },
  {
    "objectID": "lec11.html#agenda-75-min",
    "href": "lec11.html#agenda-75-min",
    "title": "11  Session 11 — APIs with requests: Secrets, Retries, and Caching",
    "section": "11.2 Agenda (75 min)",
    "text": "11.2 Agenda (75 min)\n\n(10 min) Slides: anatomy of a GET; query params; JSON; status codes\n(10 min) Slides: secrets (python-dotenv), file layout (.env, .env.template), .gitignore\n(10 min) Slides: retries and caching patterns; idempotent design\n(35 min) In‑class lab: fetch FRED VIX (VIXCLS) + optional FEDFUNDS → cache → store in SQLite → join to daily features\n(10 min) Wrap‑up + homework",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Session 11 — APIs with `requests`: Secrets, Retries, and Caching</span>"
    ]
  },
  {
    "objectID": "lec11.html#slide-talking-points",
    "href": "lec11.html#slide-talking-points",
    "title": "11  Session 11 — APIs with requests: Secrets, Retries, and Caching",
    "section": "12.2 Slide talking points",
    "text": "12.2 Slide talking points\nRequests pattern\n\nSession + HTTPAdapter + Retry → robust.\nValidate: status code, content type; guard against partial data.\n\nSecrets\n\n.env.template committed; .env untracked.\nLoad with dotenv.load_dotenv(). Access via os.getenv(\"FRED_API_KEY\").\n\nCaching\n\nFile cache: key by URL+params hash.\nDB cache: cache (key TEXT PRIMARY KEY, value BLOB, fetched_at).\n\nAlignment\n\nAfter download, normalize to date and join on date.\nStore to SQLite table with a composite key (series_id, date).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Session 11 — APIs with `requests`: Secrets, Retries, and Caching</span>"
    ]
  },
  {
    "objectID": "lec11.html#inclass-lab",
    "href": "lec11.html#inclass-lab",
    "title": "11  Session 11 — APIs with requests: Secrets, Retries, and Caching",
    "section": "11.4 In‑class lab",
    "text": "11.4 In‑class lab\n\n11.4.1 0) Setup, folders, and templates\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, json, hashlib, time, sqlite3, pandas as pd, numpy as np\nfrom pathlib import Path\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\".cache/api\",\"data\",\"data/processed\",\"data/raw\"]:\n    Path(p).mkdir(parents=True, exist_ok=True)\n\n# .env template for secrets\nPath(\".env.template\").write_text(\"FRED_API_KEY=\\n\")\n# Ensure .gitignore has secrets & cache\ngi = Path(\".gitignore\")\nif gi.exists():\n    gi_txt = gi.read_text()\nelse:\n    gi_txt = \"\"\nfor line in [\".env\", \".cache/\", \"__pycache__/\"]:\n    if line not in gi_txt:\n        gi_txt += (\"\\n\" if not gi_txt.endswith(\"\\n\") else \"\") + line\ngi.write_text(gi_txt)\nprint(\"Ready. Fill your FRED key in a local .env (do not commit).\")\n\n\n11.4.2 1) Robust GET with retry + file cache\nimport os, requests\nfrom urllib3.util.retry import Retry\nfrom requests.adapters import HTTPAdapter\nfrom dotenv import load_dotenv\n\nload_dotenv()  # reads .env if present\n\ndef session_with_retry(total=3, backoff=0.5):\n    s = requests.Session()\n    retry = Retry(total=total, backoff_factor=backoff, status_forcelist=[429,500,502,503,504])\n    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n    s.headers.update({\"User-Agent\": \"dspt-class/1.0 (+edu)\"})\n    return s\n\ndef cache_key(url, params):\n    raw = url + \"?\" + \"&\".join(f\"{k}={params[k]}\" for k in sorted(params))\n    return hashlib.sha1(raw.encode()).hexdigest()\n\ndef cached_get(url, params, ttl_hours=24):\n    key = cache_key(url, params)\n    path = Path(f\".cache/api/{key}.json\")\n    if path.exists() and (time.time() - path.stat().st_mtime &lt; ttl_hours*3600):\n        return json.loads(path.read_text())\n    s = session_with_retry()\n    r = s.get(url, params=params, timeout=20)\n    r.raise_for_status()\n    data = r.json()\n    path.write_text(json.dumps(data))\n    return data\n\n\n11.4.3 2) Fetch VIX (VIXCLS) and FEDFUNDS from FRED; store to SQLite\nAPI_KEY = os.getenv(\"FRED_API_KEY\", \"\").strip()\nif not API_KEY:\n    print(\"WARNING: No FRED_API_KEY in .env; continuing with unauthenticated request may fail on FRED. Add your key to use in class.\")\n\nFRED_SERIES_URL = \"https://api.stlouisfed.org/fred/series/observations\"\n\ndef fred_series(series_id, start=\"2010-01-01\", end=None):\n    p = {\"series_id\":series_id, \"api_key\":API_KEY, \"file_type\":\"json\",\n         \"observation_start\":start}\n    if end is not None: p[\"observation_end\"]=end\n    data = cached_get(FRED_SERIES_URL, p, ttl_hours=24)\n    obs = data.get(\"observations\", [])\n    df = pd.DataFrame(obs)[[\"date\",\"value\"]]\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n    df[\"series_id\"] = series_id\n    return df.dropna()\n\nvix = fred_series(\"VIXCLS\", start=\"2015-01-01\")       # CBOE VIX\nfed = fred_series(\"FEDFUNDS\", start=\"2015-01-01\")     # Effective Fed Funds\n\n# Write to SQLite\ndb = sqlite3.connect(\"data/prices.db\")\ndb.execute(\"\"\"CREATE TABLE IF NOT EXISTS macro_series(\n    series_id TEXT NOT NULL, date TEXT NOT NULL, value REAL NOT NULL,\n    PRIMARY KEY(series_id, date))\"\"\")\nfor df in [vix, fed]:\n    df.to_sql(\"macro_series\", db, if_exists=\"append\", index=False)\ndb.commit(); db.close()\n\nvix.head(), fed.head()\n\n\n11.4.4 3) Join macro series to daily returns/features by date\n# Load features (build if missing)\nfrom pathlib import Path\nfvpath = Path(\"data/processed/features_v1.parquet\")\nif not fvpath.exists():\n    raise SystemExit(\"Missing features_v1.parquet — run Session 10 lab or homework.\")\n\nfv1 = pd.read_parquet(fvpath).sort_values([\"ticker\",\"date\"])\nmacro = pd.concat([vix.rename(columns={\"value\":\"vix\"}).drop(columns=\"series_id\"),\n                   fed.rename(columns={\"value\":\"fedfunds\"}).drop(columns=\"series_id\")], axis=0)\n# Pivot macro wide\nmacro_wide = (pd.concat([\n    vix.assign(var=\"vix\").rename(columns={\"value\":\"val\"}),\n    fed.assign(var=\"fedfunds\").rename(columns={\"value\":\"val\"})\n]) .pivot_table(index=\"date\", columns=\"var\", values=\"val\").reset_index())\n\nenriched = fv1.merge(macro_wide, on=\"date\", how=\"left\")\nenriched[[\"vix\",\"fedfunds\"]] = enriched[[\"vix\",\"fedfunds\"]].astype(\"float32\")\nenriched.to_parquet(\"data/processed/features_v1_ext.parquet\", compression=\"zstd\", index=False)\nprint(\"Wrote data/processed/features_v1_ext.parquet\", enriched.shape)\nenriched.head(5)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Session 11 — APIs with `requests`: Secrets, Retries, and Caching</span>"
    ]
  },
  {
    "objectID": "lec11.html#wrapup",
    "href": "lec11.html#wrapup",
    "title": "11  Session 11 — APIs with requests: Secrets, Retries, and Caching",
    "section": "11.5 Wrap‑up",
    "text": "11.5 Wrap‑up\n\nYou built a retrying, cached API client, stored macro data in SQLite, and aligned it by date.\nSecrets live in .env (never committed).\nEnriched features are saved for modeling later.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Session 11 — APIs with `requests`: Secrets, Retries, and Caching</span>"
    ]
  },
  {
    "objectID": "lec11.html#homework-due-before-session-12",
    "href": "lec11.html#homework-due-before-session-12",
    "title": "11  Session 11 — APIs with requests: Secrets, Retries, and Caching",
    "section": "11.6 Homework (due before Session 12)",
    "text": "11.6 Homework (due before Session 12)\nGoal: Add one more external series (your choice) via FRED and keep everything cached and reproducible.\n\n11.6.1 A. Script: scripts/get_macro.py\n#!/usr/bin/env python\nimport os, json, time, hashlib, pandas as pd, sqlite3\nfrom pathlib import Path\nimport requests\nfrom urllib3.util.retry import Retry\nfrom requests.adapters import HTTPAdapter\nfrom dotenv import load_dotenv\n\nload_dotenv()\nAPI_KEY = os.getenv(\"FRED_API_KEY\",\"\").strip()\nBASE = \"https://api.stlouisfed.org/fred/series/observations\"\n\ndef sess():\n    s = requests.Session()\n    s.headers.update({\"User-Agent\":\"dspt-class/1.0\"})\n    s.mount(\"https://\", HTTPAdapter(max_retries=Retry(total=3, backoff_factor=0.5,\n                                                      status_forcelist=[429,500,502,503,504])))\n    return s\n\ndef ckey(url, params):\n    raw = url + \"?\" + \"&\".join(f\"{k}={params[k]}\" for k in sorted(params))\n    return hashlib.sha1(raw.encode()).hexdigest()\n\ndef cached_get(url, params, ttl=86400):\n    key = ckey(url, params); p = Path(f\".cache/api/{key}.json\")\n    p.parent.mkdir(parents=True, exist_ok=True)\n    if p.exists() and (time.time() - p.stat().st_mtime &lt; ttl):\n        return json.loads(p.read_text())\n    r = sess().get(url, params=params, timeout=20); r.raise_for_status()\n    data = r.json(); p.write_text(json.dumps(data)); return data\n\ndef fetch_series(series_id, start=\"2015-01-01\"):\n    if not API_KEY: raise SystemExit(\"Set FRED_API_KEY in .env\")\n    params = {\"series_id\":series_id, \"api_key\":API_KEY, \"file_type\":\"json\", \"observation_start\":start}\n    data = cached_get(BASE, params)\n    df = pd.DataFrame(data[\"observations\"])[[\"date\",\"value\"]]\n    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.strftime(\"%Y-%m-%d\")  # store as TEXT YYYY-MM-DD\n    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n    df[\"series_id\"] = series_id\n    return df.dropna(subset=[\"value\"])\n\ndef ensure_schema(con):\n    con.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS macro_series(\n            series_id TEXT,\n            date      TEXT,\n            value     REAL,\n            PRIMARY KEY(series_id, date)\n        )\n    \"\"\")\n    # Optional: index on date for faster slicing across all series\n    con.execute(\"CREATE INDEX IF NOT EXISTS idx_macro_series_date ON macro_series(date)\")\n\ndef upsert_dataframe(con, df):\n    sql = \"\"\"\n        INSERT INTO macro_series(series_id, date, value)\n        VALUES (?, ?, ?)\n        ON CONFLICT(series_id, date) DO UPDATE SET\n            value = excluded.value\n    \"\"\"\n    rows = list(df[[\"series_id\",\"date\",\"value\"]].itertuples(index=False, name=None))\n    con.executemany(sql, rows)\n\ndef main(series_id, start=\"2015-01-01\"):\n    df = fetch_series(series_id, start=start)\n    con = sqlite3.connect(\"data/prices.db\")\n    try:\n        ensure_schema(con)\n        upsert_dataframe(con, df)\n        con.commit()\n    finally:\n        con.close()\n    print(f\"Upserted {series_id}: {len(df)} rows from {start}\")\n\nif __name__ == \"__main__\":\n    import argparse\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--series-id\", default=\"VIXCLS\")\n    ap.add_argument(\"--start\", default=\"2015-01-01\")\n    args, _ = ap.parse_known_args()\n    main(args.series_id, start=args.start)\nRun example:\n%%bash\nchmod +x scripts/get_macro.py\npython scripts/get_macro.py --series-id DGS10   # 10‑Year Treasury Constant Maturity Rate\n\n\n11.6.2 B. Enrich features with your new series\nimport pandas as pd, sqlite3\nfv = pd.read_parquet(\"data/processed/features_v1.parquet\")\ncon = sqlite3.connect(\"data/prices.db\")\nmacro = pd.read_sql_query(\"SELECT series_id, date, value FROM macro_series\", con, parse_dates=[\"date\"])\ncon.close()\nwide = macro.pivot_table(index=\"date\", columns=\"series_id\", values=\"value\").reset_index()\nout = fv.merge(wide, on=\"date\", how=\"left\")\nout.to_parquet(\"data/processed/features_v1_ext.parquet\", compression=\"zstd\", index=False)\nprint(\"Wrote features_v1_ext.parquet with extra series:\", out.shape)\n\n\n11.6.3 C. Short test: tests/test_macro_join.py\nimport pandas as pd\ndef test_enriched_has_macro():\n    df = pd.read_parquet(\"data/processed/features_v1_ext.parquet\")\n    assert \"date\" in df and \"ticker\" in df\n    assert df.filter(regex=\"^(VIXCLS|DGS10|FEDFUNDS)$\").shape[1] &gt;= 1\nRun:\n%%bash\npytest -q",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Session 11 — APIs with `requests`: Secrets, Retries, and Caching</span>"
    ]
  },
  {
    "objectID": "lec12.html",
    "href": "lec12.html",
    "title": "12  Session 12 — HTML Scraping: Ethics & Resilience",
    "section": "",
    "text": "12.0.1 Learning goals\nStudents will be able to:",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Session 12 — HTML Scraping: Ethics & Resilience</span>"
    ]
  },
  {
    "objectID": "lec12.html#agenda-75-min",
    "href": "lec12.html#agenda-75-min",
    "title": "12  Session 12 — HTML Scraping: Ethics & Resilience",
    "section": "12.1 Agenda (75 min)",
    "text": "12.1 Agenda (75 min)\n\n(10 min) Slides: ethics, robots, terms; caching, rate limits\n(10 min) Slides: stable selectors (ids, table headers), text cleanup, date parsing\n(35 min) In‑class lab: scrape a static sector table (Wikipedia S&P 500 components), map to your tickers, save data/static/sector_map.csv; merge into prices.parquet if missing\n(10 min) Wrap‑up + homework brief\n(10 min) Buffer",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Session 12 — HTML Scraping: Ethics & Resilience</span>"
    ]
  },
  {
    "objectID": "lec12.html#slide-talking-points",
    "href": "lec12.html#slide-talking-points",
    "title": "12  Session 12 — HTML Scraping: Ethics & Resilience",
    "section": "13.2 Slide talking points",
    "text": "13.2 Slide talking points\nEthics + resilience\n\nCheck robots.txt; identify disallow rules.\nSet a clear User‑Agent and sleep between requests.\nCache HTML locally; don’t hammer sites.\nExpect structure to change; write defensive code.\n\nParsing patterns\n\nPrefer table selectors; use read_html for well‑formed tables.\nClean headers → snake_case; drop footnotes; trim whitespace.\nNormalize keys (e.g., ticker symbols: map . ↔︎ - if needed).\n\nProvenance\n\nSave source_url, fetched_at, and a checksum alongside the CSV.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Session 12 — HTML Scraping: Ethics & Resilience</span>"
    ]
  },
  {
    "objectID": "lec12.html#inclass-lab",
    "href": "lec12.html#inclass-lab",
    "title": "12  Session 12 — HTML Scraping: Ethics & Resilience",
    "section": "12.3 In‑class lab",
    "text": "12.3 In‑class lab\n\nWe’ll scrape Wikipedia: List of S&P 500 companies (static table). If blocked, we fall back to pandas.read_html or a small local stub.\n\n\n12.3.1 0) Setup + robots check + HTML caching\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, requests, time, hashlib, pandas as pd, numpy as np\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\nfrom datetime import datetime\n\nos.chdir(REPO_DIR)\nfor p in [\".cache/html\",\"data/static\",\"reports\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\n\nUA = {\"User-Agent\": \"dspt-class/1.0 (+edu)\"}\nWIKI_URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n\ndef allowed_by_robots(base, path=\"/wiki/\"):\n    r = requests.get(urljoin(base, \"/robots.txt\"), headers=UA, timeout=20)\n    if r.status_code != 200: return True\n    lines = r.text.splitlines()\n    disallows = [ln.split(\":\")[1].strip() for ln in lines if ln.lower().startswith(\"disallow:\")]\n    return all(not path.startswith(d) for d in disallows)\n\nprint(\"Robots allows /wiki/?\", allowed_by_robots(\"https://en.wikipedia.org\"))\n\n\n12.3.2 1) Download (with cache) and parse the first big table\ndef get_html_cached(url, ttl_hours=24):\n    key = hashlib.sha1(url.encode()).hexdigest()\n    path = pathlib.Path(f\".cache/html/{key}.html\")\n    if path.exists() and (time.time() - path.stat().st_mtime &lt; ttl_hours * 3600):\n        return path.read_text()\n    r = requests.get(url, headers=UA, timeout=30)\n    r.raise_for_status()\n    path.write_text(r.text)\n    time.sleep(1.0)  # be polite\n    return r.text\n\nhtml = get_html_cached(WIKI_URL)\nsoup = BeautifulSoup(html, \"html.parser\")\n\n# Try soup table first; fallback to pandas.read_html\ntable = soup.find(\"table\", {\"id\":\"constituents\"}) or soup.find(\"table\", {\"class\":\"wikitable\"})\nif table is not None:\n    rows = []\n    headers = [th.get_text(strip=True) for th in table.find(\"tr\").find_all(\"th\")]\n    for tr in table.find_all(\"tr\")[1:]:\n        tds = [td.get_text(strip=True) for td in tr.find_all([\"td\",\"th\"])]\n        if len(tds) == len(headers):\n            rows.append(dict(zip(headers, tds)))\n    sp = pd.DataFrame(rows)\nelse:\n    sp = pd.read_html(html)[0]\n\nsp.head(3), sp.columns.tolist()\n\n\n12.3.3 2) Clean + normalize + keep only ticker ↔︎ sector\nimport re\ndef snake(s): \n    s = re.sub(r\"[^\\w\\s]\", \"_\", s)\n    s = re.sub(r\"\\s+\", \"_\", s.strip().lower())\n    return re.sub(r\"_+\", \"_\", s)\n\nsp.columns = [snake(c) for c in sp.columns]\ncand_cols = [c for c in sp.columns if \"symbol\" in c or \"security\" in c or \"sector\" in c]\nsp = sp.rename(columns={c:\"symbol\" for c in sp.columns if \"symbol\" in c or c==\"ticker\"})\nsp = sp.rename(columns={c:\"sector\" for c in sp.columns if \"sector\" in c})\nkeep = [c for c in [\"symbol\",\"sector\"] if c in sp.columns]\nsp = sp[keep].dropna().drop_duplicates()\nsp = sp.rename(columns={\"symbol\":\"ticker\"})\nsp[\"ticker\"] = sp[\"ticker\"].str.strip()\nsp[\"sector\"] = sp[\"sector\"].astype(\"category\")\n\n# Save with provenance\nsrc = {\"source_url\": WIKI_URL, \"fetched_at_utc\": datetime.utcnow().isoformat()+\"Z\"}\nsp.to_csv(\"data/static/sector_map.csv\", index=False)\nwith open(\"data/static/sector_map.provenance.json\",\"w\") as f:\n    import json; json.dump(src, f, indent=2)\nprint(\"Wrote data/static/sector_map.csv\", sp.shape)\nsp.head(5)\n\n\n12.3.4 3) Merge sector mapping into prices if missing sector\nfrom pathlib import Path\npp = Path(\"data/processed/prices.parquet\")\nif not pp.exists():\n    raise SystemExit(\"Need prices.parquet (Session 9).\")\n\nprices = pd.read_parquet(pp)\nif \"sector\" not in prices.columns or prices[\"sector\"].isna().all():\n    prices2 = prices.merge(sp, on=\"ticker\", how=\"left\")\n    prices2[\"sector\"] = prices2[\"sector\"].astype(\"category\")\n    prices2.to_parquet(\"data/processed/prices.parquet\", compression=\"zstd\", index=False)\n    print(\"Updated prices.parquet with sector column.\")\nelse:\n    print(\"Sector already present; no merge needed.\")",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Session 12 — HTML Scraping: Ethics & Resilience</span>"
    ]
  },
  {
    "objectID": "lec12.html#wrapup",
    "href": "lec12.html#wrapup",
    "title": "12  Session 12 — HTML Scraping: Ethics & Resilience",
    "section": "12.4 Wrap‑up",
    "text": "12.4 Wrap‑up\n\nYou scraped a static table politely (robots, throttle, cache) and extracted a tidy map.\nYou persisted provenance and used it to enrich your dataset.\nKeep scrapers small, cached, and resilient.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Session 12 — HTML Scraping: Ethics & Resilience</span>"
    ]
  },
  {
    "objectID": "lec12.html#homework-due-next-week",
    "href": "lec12.html#homework-due-next-week",
    "title": "12  Session 12 — HTML Scraping: Ethics & Resilience",
    "section": "12.5 Homework (due next week)",
    "text": "12.5 Homework (due next week)\nGoal: Document your web data provenance and generate a minimal data dictionary for the project.\n\n12.5.1 A. Provenance section (script that composes a Markdown file)\n# scripts/write_provenance.py\n#!/usr/bin/env python\nimport json, pandas as pd\nfrom pathlib import Path\nPath(\"reports\").mkdir(exist_ok=True)\n\nprovenance = []\nif Path(\"data/static/sector_map.provenance.json\").exists():\n    provenance.append(json.loads(Path(\"data/static/sector_map.provenance.json\").read_text()))\nelse:\n    provenance.append({\"source_url\":\"(none)\",\"fetched_at_utc\":\"(n/a)\"})\n\nmd = [\"# Data provenance\",\n      \"\",\n      \"## Web sources\",\n      \"\",\n      \"| Source | Fetched at |\",\n      \"|---|---|\"]\nfor p in provenance:\n    md.append(f\"| {p['source_url']} | {p['fetched_at_utc']} |\")\n\nPath(\"reports/provenance.md\").write_text(\"\\n\".join(md))\nprint(\"Wrote reports/provenance.md\")\nRun:\n%%bash\nchmod +x scripts/write_provenance.py\npython scripts/write_provenance.py\n\n\n12.5.2 B. Data dictionary generator\n# scripts/data_dictionary.py\n#!/usr/bin/env python\nimport pandas as pd\nfrom pathlib import Path\n\ndef describe_parquet(path):\n    df = pd.read_parquet(path)\n    dtypes = df.dtypes.astype(str).to_dict()\n    return pd.DataFrame({\"column\": list(dtypes.keys()), \"dtype\": list(dtypes.values())})\n\ndef main():\n    rows=[]\n    for path in [\"data/processed/prices.parquet\",\n                 \"data/processed/returns.parquet\",\n                 \"data/processed/features_v1.parquet\",\n                 \"data/processed/features_v1_ext.parquet\"]:\n        p = Path(path)\n        if p.exists():\n            df = describe_parquet(p)\n            df.insert(0, \"dataset\", p.name)\n            rows.append(df)\n    out = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=[\"dataset\",\"column\",\"dtype\"])\n    Path(\"reports\").mkdir(exist_ok=True)\n    out.to_csv(\"reports/data_dictionary.csv\", index=False)\n    print(\"Wrote reports/data_dictionary.csv\")\n\nif __name__ == \"__main__\":\n    main()\nRun:\n%%bash\nchmod +x scripts/data_dictionary.py\npython scripts/data_dictionary.py\n\n\n12.5.3 C. (Optional) Add a short Quarto page that includes both files\nCreate reports/data_overview.qmd and render in your next report.\n\n\n12.5.4 D. Quick tests\n# tests/test_dictionary_provenance.py\nimport os, pandas as pd\ndef test_provenance_and_dict():\n    assert os.path.exists(\"reports/provenance.md\")\n    assert os.path.exists(\"reports/data_dictionary.csv\")\n    df = pd.read_csv(\"reports/data_dictionary.csv\")\n    assert {\"dataset\",\"column\",\"dtype\"}.issubset(df.columns)\nRun:\n%%bash\npytest -q",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Session 12 — HTML Scraping: Ethics & Resilience</span>"
    ]
  },
  {
    "objectID": "lec12.html#instructor-tips-for-all-three-sessions",
    "href": "lec12.html#instructor-tips-for-all-three-sessions",
    "title": "12  Session 12 — HTML Scraping: Ethics & Resilience",
    "section": "13.6 Instructor tips (for all three sessions)",
    "text": "13.6 Instructor tips (for all three sessions)\n\nKeep a one‑page “no leakage” checklist handy and point to it often.\nFor Session 11, have a prepared .env with a working FRED key to avoid classroom delays.\nFor Session 12, if Wikipedia blocks requests, switch to pandas.read_html (shown) or use a small pre‑saved HTML in data/static/ to demonstrate parsing.\n\nThese three sessions carry you from solid feature engineering → external data integration → web scraping with ethics, setting up a strong foundation for the testing/CI weeks that follow.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Session 12 — HTML Scraping: Ethics & Resilience</span>"
    ]
  },
  {
    "objectID": "lec13.html",
    "href": "lec13.html",
    "title": "13  Session 13: pytest + Data Validation",
    "section": "",
    "text": "13.1 Session 13 — pytest + Data Validation\nAssumptions: You completed Session 9–12 and have data/processed/features_v1.parquet (or features_v1_ext.parquet). If a file is missing, the lab provides a small synthetic fallback so tests still run. Goal today: Make it hard to ship bad data by adding precise, fast tests.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13: pytest + Data Validation</span>"
    ]
  },
  {
    "objectID": "lec13.html#session-13-pytest-data-validation-75-min",
    "href": "lec13.html#session-13-pytest-data-validation-75-min",
    "title": "13  Session 13: pytest + Data Validation",
    "section": "",
    "text": "13.1.1 Learning goals\nBy the end of class, students can:\n\nWrite fast, high‑signal tests for data pipelines (shapes, dtypes, nulls, no look‑ahead).\nValidate a DataFrame with Pandera (schema + value checks) or custom checks only.\nUse logging effectively and capture logs in tests.\nRun tests in Colab / locally and prepare for CI in Session 14.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13: pytest + Data Validation</span>"
    ]
  },
  {
    "objectID": "lec13.html#agenda-75-min",
    "href": "lec13.html#agenda-75-min",
    "title": "13  Session 13: pytest + Data Validation",
    "section": "13.2 Agenda (75 min)",
    "text": "13.2 Agenda (75 min)\n\n(10 min) Slides: What to test (and not), “data tests” vs unit tests, speed budget\n(10 min) Slides: Pandera schemas & custom checks; tolerance and stability\n(10 min) Slides: Logging basics (logging, levels, handlers); testing logs with caplog\n(35 min) In‑class lab: add tests/test_features.py (+ optional Pandera test), fixtures, config; run & fix\n(10 min) Wrap‑up + homework briefing",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13: pytest + Data Validation</span>"
    ]
  },
  {
    "objectID": "lec13.html#slides-talking-points-drop-into-your-deck",
    "href": "lec13.html#slides-talking-points-drop-into-your-deck",
    "title": "13  Session 13",
    "section": "13.3 Slides / talking points (drop into your deck)",
    "text": "13.3 Slides / talking points (drop into your deck)\n\n13.3.1 What to test (fast, crisp)\n\nContract tests for data:\n\nSchema: required columns exist; dtypes sane (ticker categorical, calendar ints).\nNulls: no NAs in training‑critical cols.\nSemantics: r_1d is lead of log_return; rolling features computed from past only.\nKeys: no duplicate (ticker, date); dates strictly increasing within ticker.\n\nKeep tests under ~5s total (CI budget). Avoid long recomputations; sample/take head.\n\n\n\n13.3.2 Pandera vs custom checks\n\nPandera: declarative schema; optional dependency; good for column existence + ranges.\nCustom: essential for domain logic (look‑ahead bans, exact rolling formulas).\n\n\n\n13.3.3 Logging basics\n\nUse logging.getLogger(__name__); set level via env (LOGLEVEL=INFO).\nLog counts, ranges, and any data drops inside build scripts.\nIn tests: use caplog to assert a warning is emitted for suspicious conditions.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13</span>"
    ]
  },
  {
    "objectID": "lec13.html#inclass-lab-35-min",
    "href": "lec13.html#inclass-lab-35-min",
    "title": "13  Session 13: pytest + Data Validation",
    "section": "13.3 In‑class lab (35 min)",
    "text": "13.3 In‑class lab (35 min)\n\nRun each block as its own Colab cell. Adjust REPO_NAME as needed.\n\n\n13.3.1 0) Setup: mount & folders\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"  # &lt;- change if needed\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/processed\",\"tests\",\"scripts\",\"reports\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\n\n13.3.2 1) (Optional) Install test‑time helpers (Pandera)\n!pip -q install pytest pandera pyarrow\n\n\n13.3.3 2) Put a tiny logging helper in your repo (used by build scripts & tests)\n# scripts/logsetup.py\nfrom __future__ import annotations\nimport logging, os\n\ndef setup_logging(name: str = \"dspt\"):\n    level = os.getenv(\"LOGLEVEL\", \"INFO\").upper()\n    logger = logging.getLogger(name)\n    if not logger.handlers:\n        handler = logging.StreamHandler()\n        fmt = \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\"\n        handler.setFormatter(logging.Formatter(fmt))\n        logger.addHandler(handler)\n    logger.setLevel(level)\n    return logger\n\n\n13.3.4 3) Create pytest config and a fixture (with safe fallback data)\n# pytest.ini\nfrom pathlib import Path\nPath(\"pytest.ini\").write_text(\"\"\"[pytest]\naddopts = -q\ntestpaths = tests\nfilterwarnings =\n    ignore::FutureWarning\n\"\"\")\n\n# tests/conftest.py\nfrom pathlib import Path\nimport pandas as pd, numpy as np, pytest\n\ndef _synth_features():\n    # minimal synthetic features for 3 tickers, 60 days\n    rng = np.random.default_rng(0)\n    dates = pd.bdate_range(\"2023-01-02\", periods=60)\n    frames=[]\n    for t in [\"AAPL\",\"MSFT\",\"GOOGL\"]:\n        ret = rng.normal(0, 0.01, size=len(dates)).astype(\"float32\")\n        adj = 100 * np.exp(np.cumsum(ret))\n        df = pd.DataFrame({\n            \"date\": dates,\n            \"ticker\": t,\n            \"adj_close\": adj.astype(\"float32\"),\n            \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n        })\n        # next-day label\n        df[\"r_1d\"] = df[\"log_return\"].shift(-1)\n        # rolling\n        df[\"roll_mean_20\"] = df[\"log_return\"].rolling(20, min_periods=20).mean()\n        df[\"roll_std_20\"]  = df[\"log_return\"].rolling(20, min_periods=20).std()\n        df[\"zscore_20\"]    = (df[\"log_return\"]-df[\"roll_mean_20\"])/(df[\"roll_std_20\"]+1e-8)\n        df[\"weekday\"] = df[\"date\"].dt.weekday.astype(\"int8\")\n        df[\"month\"]   = df[\"date\"].dt.month.astype(\"int8\")\n        frames.append(df)\n    out = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)\n    out[\"ticker\"] = out[\"ticker\"].astype(\"category\")\n    return out\n\n@pytest.fixture(scope=\"session\")\ndef features_df():\n    p = Path(\"data/processed/features_v1.parquet\")\n    if p.exists():\n        df = pd.read_parquet(p)\n        # Ensure expected minimal cols exist (compute light ones if missing)\n        if \"weekday\" not in df: df[\"weekday\"] = pd.to_datetime(df[\"date\"]).dt.weekday.astype(\"int8\")\n        if \"month\" not in df:   df[\"month\"] = pd.to_datetime(df[\"date\"]).dt.month.astype(\"int8\")\n        return df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    # fallback\n    return _synth_features().sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n\n\n13.3.5 4) High‑value tests: shapes, nulls, look‑ahead ban (as requested)\n# tests/test_features.py\nimport numpy as np, pandas as pd\nimport pytest\n\nREQUIRED_COLS = [\"date\",\"ticker\",\"log_return\",\"r_1d\",\"weekday\",\"month\"]\n\ndef test_required_columns_present(features_df):\n    missing = [c for c in REQUIRED_COLS if c not in features_df.columns]\n    assert not missing, f\"Missing required columns: {missing}\"\n\ndef test_key_no_duplicates(features_df):\n    dup = features_df[[\"ticker\",\"date\"]].duplicated().sum()\n    assert dup == 0, f\"Found {dup} duplicate (ticker,date) rows\"\n\ndef test_sorted_within_ticker(features_df):\n    for tkr, g in features_df.groupby(\"ticker\"):\n        assert g[\"date\"].is_monotonic_increasing, f\"Dates not sorted for {tkr}\"\n\ndef test_nulls_in_critical_columns(features_df):\n    crit = [\"log_return\",\"r_1d\"]\n    na = features_df[crit].isna().sum().to_dict()\n    assert all(v == 0 for v in na.values()), f\"NAs in critical cols: {na}\"\n\ndef test_calendar_dtypes(features_df):\n    assert str(features_df[\"weekday\"].dtype) in (\"int8\",\"Int8\"), \"weekday should be compact int\"\n    assert str(features_df[\"month\"].dtype)   in (\"int8\",\"Int8\"), \"month should be compact int\"\n\ndef test_ticker_is_categorical(features_df):\n    # allow object if reading from some parquet engines, but prefer category\n    assert features_df[\"ticker\"].dtype.name in (\"category\",\"CategoricalDtype\",\"object\")\n\ndef test_r1d_is_lead_of_log_return(features_df):\n    for tkr, g in features_df.groupby(\"ticker\"):\n        # r_1d at t equals log_return at t+1\n        assert g[\"r_1d\"].iloc[:-1].equals(g[\"log_return\"].iloc[1:]), f\"Lead/lag mismatch for {tkr}\"\n\n@pytest.mark.parametrize(\"W\", [20])\ndef test_rolling_mean_matches_definition(features_df, W):\n    if f\"roll_mean_{W}\" not in features_df.columns:\n        pytest.skip(f\"roll_mean_{W} not present\")\n    for tkr, g in features_df.groupby(\"ticker\"):\n        s = g[\"log_return\"]\n        rm = s.rolling(W, min_periods=W).mean()\n        # compare only where defined\n        mask = ~rm.isna()\n        diff = (g[f\"roll_mean_{W}\"][mask] - rm[mask]).abs().max()\n        assert float(diff) &lt;= 1e-7, f\"roll_mean_{W} mismatch for {tkr} (max diff {diff})\"\n\n\n13.3.6 5) Optional Pandera schema test (declarative)\n# tests/test_schema_pandera.py\nimport pytest, pandas as pd, numpy as np\ntry:\n    import pandera.pandas as pa\n    from pandera import Column, Check, DataFrameSchema\nexcept Exception:\n    pytest.skip(\"pandera not installed\", allow_module_level=True)\n\nschema = pa.DataFrameSchema({\n    \"date\":     Column(pa.DateTime, nullable=False),\n    \"ticker\":   Column(pa.String,  nullable=False, coerce=True, checks=Check.str_length(1, 12)),\n    \"log_return\": Column(pa.Float, nullable=False,\n                         checks=Check(lambda s: np.isfinite(s).all(), error=\"log_return must be finite\")),\n    \"r_1d\":       Column(pa.Float, nullable=False,\n                         checks=Check(lambda s: np.isfinite(s).all(), error=\"r_1d must be finite\")),\n    \"weekday\":  Column(pa.Int8, checks=Check.isin(range(7))),                 # 0..6\n    \"month\":    Column(pa.Int8, checks=Check.in_range(1, 12, inclusive=\"both\")),\n})\n\ndef test_schema_validate(features_df):\n    # Cast ticker to string for schema validation; categorical is ok → string\n    df = features_df.copy()\n    df[\"ticker\"] = df[\"ticker\"].astype(str)\n    schema.validate(df[[\"date\",\"ticker\",\"log_return\",\"r_1d\",\"weekday\",\"month\"]])\n\n\n13.3.7 6) Logging test: assert a warning is emitted on duplicates (toy demo)\n# tests/test_logging.py\nimport logging, pandas as pd, numpy as np, pytest\nfrom scripts.logsetup import setup_logging\n\ndef check_for_duplicates(df, logger=None):\n    logger = logger or setup_logging(\"dspt\")\n    dups = df[[\"ticker\",\"date\"]].duplicated().sum()\n    if dups &gt; 0:\n        logger.warning(\"Found %d duplicate (ticker,date) rows\", dups)\n    return dups\n\ndef test_duplicate_warning(caplog):\n    caplog.set_level(logging.WARNING)\n    df = pd.DataFrame({\"ticker\":[\"AAPL\",\"AAPL\"], \"date\":pd.to_datetime([\"2024-01-02\",\"2024-01-02\"])})\n    dups = check_for_duplicates(df)\n    assert dups == 1\n    assert any(\"duplicate\" in rec.message for rec in caplog.records)\n\n\n13.3.8 7) Run tests now\n!pytest -q\n\nIf a test fails on your real data, fix your pipeline (e.g., regenerate features_v1.parquet) and re‑run. Do not relax the test without understanding the failure.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13: pytest + Data Validation</span>"
    ]
  },
  {
    "objectID": "lec13.html#wrapup-10-min",
    "href": "lec13.html#wrapup-10-min",
    "title": "13  Session 13: pytest + Data Validation",
    "section": "13.4 Wrap‑up (10 min)",
    "text": "13.4 Wrap‑up (10 min)\n\nYou now have tests that fail loudly if labels leak, required columns/keys break, or schemas drift.\nPandera provides a declarative baseline; custom tests encode your domain logic.\nLogging helps you debug data issues; you can assert on log messages in tests.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13: pytest + Data Validation</span>"
    ]
  },
  {
    "objectID": "lec13.html#homework-due-before-session-14",
    "href": "lec13.html#homework-due-before-session-14",
    "title": "13  Session 13: pytest + Data Validation",
    "section": "13.5 Homework (due before Session 14)",
    "text": "13.5 Homework (due before Session 14)\nGoal: Create a Health Check notebook that prints key diagnostics and is easy to include in your Quarto report.\n\n13.5.1 Part A — Build a reusable health module\n# scripts/health.py\nfrom __future__ import annotations\nimport pandas as pd, numpy as np, json\nfrom pathlib import Path\n\ndef df_health(df: pd.DataFrame) -&gt; dict:\n    out = {}\n    out[\"rows\"] = int(len(df))\n    out[\"cols\"] = int(df.shape[1])\n    out[\"date_min\"] = str(pd.to_datetime(df[\"date\"]).min().date())\n    out[\"date_max\"] = str(pd.to_datetime(df[\"date\"]).max().date())\n    out[\"tickers\"]  = int(df[\"ticker\"].nunique())\n    # Null counts (top 10)\n    na = df.isna().sum().sort_values(ascending=False)\n    out[\"nulls\"] = na[na&gt;0].head(10).to_dict()\n    # Duplicates\n    out[\"dup_key_rows\"] = int(df[[\"ticker\",\"date\"]].duplicated().sum())\n    # Example numeric ranges for core cols\n    for c in [x for x in [\"log_return\",\"r_1d\",\"roll_std_20\"] if x in df.columns]:\n        s = pd.to_numeric(df[c], errors=\"coerce\")\n        out[f\"{c}_min\"] = float(np.nanmin(s))\n        out[f\"{c}_max\"] = float(np.nanmax(s))\n    return out\n\ndef write_health_report(in_parquet=\"data/processed/features_v1.parquet\",\n                        out_json=\"reports/health.json\", out_md=\"reports/health.md\"):\n    p = Path(in_parquet)\n    if not p.exists():\n        raise SystemExit(f\"Missing {in_parquet}.\")\n    df = pd.read_parquet(p)\n    h = df_health(df)\n    Path(out_json).write_text(json.dumps(h, indent=2))\n    # Render a small Markdown summary\n    lines = [\n        \"# Data Health Summary\",\n        \"\",\n        f\"- Rows: **{h['rows']}**; Cols: **{h['cols']}**; Tickers: **{h['tickers']}**\",\n        f\"- Date range: **{h['date_min']} → {h['date_max']}**\",\n        f\"- Duplicate (ticker,date) rows: **{h['dup_key_rows']}**\",\n    ]\n    if h.get(\"nulls\"):\n        lines += [\"\", \"## Top Null Counts\", \"\"]\n        lines += [f\"- **{k}**: {v}\" for k,v in h[\"nulls\"].items()]\n    Path(out_md).write_text(\"\\n\".join(lines))\n    print(\"Wrote\", out_json, \"and\", out_md)\nRun once to generate the files:\n!python scripts/health.py\n\n\n13.5.2 Part B — Health Check notebook (reports/health.ipynb)\nCreate a new notebook reports/health.ipynb with two cells:\nCell 1 (setup):\n%load_ext autoreload\n%autoreload 2\nfrom scripts.health import write_health_report\nwrite_health_report()  # writes reports/health.json and reports/health.md\nCell 2 (display in notebook):\nfrom pathlib import Path\nprint(Path(\"reports/health.md\").read_text())\n\nCommit the notebook. It will be light and re‑usable. You’ll include its output in Quarto below.\n\n\n\n13.5.3 Part C — Include health output in your Quarto report\nIn reports/eda.qmd, add a section:\n## Data Health (auto-generated)\n\n::: {#384d252a .cell execution_count=1}\n``` {.python .cell-code}\nfrom pathlib import Path\nprint(Path(\"reports/health.md\").read_text())\n```\n:::\n\nRender EDA: quarto render reports/eda.qmd\n\n\n13.5.4 Part D — Add a Makefile target and a quick test\nMakefile append:\n.PHONY: health test\nhealth: ## Generate health.json and health.md from the current features parquet\n    python scripts/health.py\n\npytest:\n    pytest -q\n\ntest: pytest\nTest that health files exist:\n# tests/test_health_outputs.py\nimport os, json\n\ndef test_health_files_exist():\n    assert os.path.exists(\"reports/health.json\")\n    assert os.path.exists(\"reports/health.md\")\n    # json is valid\n    import json\n    json.load(open(\"reports/health.json\"))\nRun:\n%%bash\nmake health\npytest -q -k health",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13: pytest + Data Validation</span>"
    ]
  },
  {
    "objectID": "lec13.html#instructor-checklist-before-class",
    "href": "lec13.html#instructor-checklist-before-class",
    "title": "13  Session 13",
    "section": "13.7 Instructor checklist (before class)",
    "text": "13.7 Instructor checklist (before class)\n\nEnsure features_v1.parquet exists or the fixture’s synthetic fallback works.\nDry‑run pytest -q in a fresh runtime; keep total time &lt; 5s.\nPrepare 2–3 “expected failures” you can toggle (e.g., edit one feature column to NaN) to show tests catching bugs.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13</span>"
    ]
  },
  {
    "objectID": "lec13.html#emphasize-while-teaching",
    "href": "lec13.html#emphasize-while-teaching",
    "title": "13  Session 13",
    "section": "13.8 Emphasize while teaching",
    "text": "13.8 Emphasize while teaching\n\nFast tests only for CI; keep heavy, long recomputations out.\nNo look‑ahead and unique (ticker,date) are non‑negotiable contracts.\nLogging is a first‑class tool—tests can assert on warnings you emit.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13</span>"
    ]
  },
  {
    "objectID": "lec13.html#grading-passrevise",
    "href": "lec13.html#grading-passrevise",
    "title": "13  Session 13: pytest + Data Validation",
    "section": "13.7 Grading (pass/revise)",
    "text": "13.7 Grading (pass/revise)\n\ntests/test_features.py present with shapes, nulls, look‑ahead ban (and rolling check).\nTests pass locally (pytest -q).\nreports/health.ipynb and reports/health.md/.json exist and integrate into eda.qmd.\nMakefile health and test targets work.\n\nYou now have a safety net around your data. In Session 14, we’ll enforce style with pre‑commit and bring your tests to GitHub Actions CI.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13: pytest + Data Validation</span>"
    ]
  },
  {
    "objectID": "lec14.html",
    "href": "lec14.html",
    "title": "14  pre‑commit & GitHub Actions CI",
    "section": "",
    "text": "14.1 Session 14 — pre‑commit & GitHub Actions CI\nAssumptions: You completed Session 13 and have a repo in Drive (e.g., unified-stocks-teamX) with a small test suite (pytest) and Parquet data present locally. Colab + Drive workflow assumed. Goals today: Make code quality and basic data tests automatic and repeatable in CI.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>pre‑commit & GitHub Actions CI</span>"
    ]
  },
  {
    "objectID": "lec14.html#session-14-precommit-github-actions-ci-75-min",
    "href": "lec14.html#session-14-precommit-github-actions-ci-75-min",
    "title": "14  pre‑commit & GitHub Actions CI",
    "section": "",
    "text": "14.1.1 Learning goals\nStudents will be able to:\n\nConfigure pre‑commit to run Black, Ruff (lint + import sort), and nbstripout on every commit.\nKeep commits clean and notebook outputs stripped.\nAdd a fast GitHub Actions CI workflow that runs pre‑commit hooks and pytest on each PR.\nKeep CI runtime under ~3–4 minutes with caching and a lean dependency set.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>pre‑commit & GitHub Actions CI</span>"
    ]
  },
  {
    "objectID": "lec14.html#agenda-75-min",
    "href": "lec14.html#agenda-75-min",
    "title": "14  pre‑commit & GitHub Actions CI",
    "section": "14.2 Agenda (75 min)",
    "text": "14.2 Agenda (75 min)\n\n(10 min) Slides: why pre‑commit; the “quality gate”; anatomy of a fast CI\n(10 min) Slides: Black vs Ruff; when nbstripout matters; what belongs in CI\n(35 min) In‑class lab: configure pre‑commit (Black, Ruff, nbstripout) → run locally → add CI workflow → local dry‑run\n(10 min) Wrap‑up + homework briefing\n(10 min) Buffer",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>pre‑commit & GitHub Actions CI</span>"
    ]
  },
  {
    "objectID": "lec14.html#main-points",
    "href": "lec14.html#main-points",
    "title": "14  pre‑commit & GitHub Actions CI",
    "section": "14.3 Main points",
    "text": "14.3 Main points\nWhy pre‑commit?\n\nPrevent “drive‑by” problems before they enter history: unformatted code, stray notebook outputs, trailing whitespace.\nHooks run locally on commit, then again in CI for defense‑in‑depth.\n\nBlack & Ruff\n\nBlack: opinionated formatter → consistent diffs; no bikeshedding.\nRuff: very fast linter (flake8 family), plus import sorting; can also fix many issues (--fix).\nYou can use both (common) or let Ruff handle formatting too; we’ll use both for clarity.\n\nnbstripout\n\nRemove cell outputs from notebooks to keep diffs small, avoid binary bloat, and reduce CI time.\nTwo patterns: pre‑commit hook (recommended) and/or git filter (nbstripout --install).\n\nCI scope (fast!)\n\nLint + tests only; no heavy training in CI.\nCache dependencies; pin Python (3.11+).\nKeep tests deterministic and &lt; ~5s (already done in Session 13).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>pre‑commit & GitHub Actions CI</span>"
    ]
  },
  {
    "objectID": "lec14.html#inclass-lab-35-min-colabfriendly",
    "href": "lec14.html#inclass-lab-35-min-colabfriendly",
    "title": "14  pre‑commit & GitHub Actions CI",
    "section": "14.4 In‑class lab (35 min, Colab‑friendly)",
    "text": "14.4 In‑class lab (35 min, Colab‑friendly)\n\nRun each block as its own Colab cell. Update REPO_NAME to your repo. The cells create and modify files inside your repo.\n\n\n14.4.1 0) Mount Drive & go to repo\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"  # &lt;- change if needed\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\".github/workflows\",\"tests\",\"scripts\",\"reports\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\n\n14.4.2 1) Install tools locally (for this Colab runtime)\n!pip -q install pre-commit black ruff nbstripout pytest\n\n\n14.4.3 2) Add tool config to pyproject.toml (Black + Ruff)\n\nIf you don’t have a pyproject.toml, this cell will create a minimal one; otherwise it appends/updates sections.\n\nfrom pathlib import Path\nimport textwrap, re\n\npyproj = Path(\"pyproject.toml\")\nexisting = pyproj.read_text() if pyproj.exists() else \"\"\n\ndef upsert(section_header, body):\n    global existing\n    pattern = rf\"(?ms)^\\[{re.escape(section_header)}\\]\\s*.*?(?=^\\[|\\Z)\"\n    if re.search(pattern, existing):\n        existing = re.sub(pattern, f\"[{section_header}]\\n{body}\\n\", existing)\n    else:\n        existing += f\"\\n[{section_header}]\\n{body}\\n\"\n\n# Black\nupsert(\"tool.black\", textwrap.dedent(\"\"\"\nline-length = 88\ntarget-version = [\"py311\"]\n\"\"\").strip())\n\n# Ruff (modern layout)\nupsert(\"tool.ruff\", textwrap.dedent(\"\"\"\nline-length = 88\ntarget-version = \"py311\"\n\"\"\").strip())\n\nupsert(\"tool.ruff.lint\", textwrap.dedent(\"\"\"\nselect = [\"E\",\"F\",\"I\"]  # flake8 errors, pyflakes, import sort\nignore = [\"E501\"]       # let Black handle line length\n\"\"\").strip())\n\nupsert(\"tool.ruff.lint.isort\", textwrap.dedent(\"\"\"\nknown-first-party = [\"projectname\"]\n\"\"\").strip())\n\npyproj.write_text(existing.strip()+\"\\n\")\nprint(pyproj.read_text())\n\n\n14.4.4 3) Create .pre-commit-config.yaml with hooks (Black, Ruff, nbstripout)\n\nVersions below are stable at time of writing—feel free to bump later.\n\nfrom pathlib import Path\ncfg = Path(\".pre-commit-config.yaml\")\ncfg.write_text(\"\"\"repos:\n  - repo: https://github.com/psf/black\n    rev: 24.4.2\n    hooks:\n      - id: black\n        language_version: python3.11\n\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.5.0\n    hooks:\n      - id: ruff\n        args: [--fix, --exit-non-zero-on-fix]\n      - id: ruff-format\n\n  - repo: https://github.com/kynan/nbstripout\n    rev: 0.7.1\n    hooks:\n      - id: nbstripout\n        files: \\\\.ipynb$\n\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.6.0\n    hooks:\n      - id: end-of-file-fixer\n      - id: trailing-whitespace\n      - id: check-yaml\n      - id: check-added-large-files\n\"\"\")\nprint(cfg.read_text())\n\n\n14.4.5 4) Install the local git hook & run on all files\n!pre-commit install\n!pre-commit run --all-files\n\nThe first run will download hook toolchains (Black, Ruff, etc.), format files, and strip notebook outputs. Commit changes after verifying.\n\n\n\n14.4.6 5) (Optional) Also install git filter for nbstripout\n\nThis is an extra layer; pre‑commit hook above already strips outputs. Use this to guarantee outputs are removed even when bypassing pre‑commit.\n\n!nbstripout --install --attributes .gitattributes\nprint(open(\".gitattributes\").read())\n\n\n14.4.7 6) Add a tiny “bad style” file to see hooks in action\nfrom pathlib import Path\np = Path(\"scripts/bad_style.py\")\np.write_text(\"import os,sys\\n\\n\\ndef add(a,b):\\n  return(a +  b)\\n\")\nprint(\"Wrote:\", p)\n\n# Run hooks just on this file\n!pre-commit run --files scripts/bad_style.py\nprint(open(\"scripts/bad_style.py\").read())\n\nYou should see Black and Ruff fix spacing/imports; trailing whitespace hooks may also fire.\n\n\n\n14.4.8 7) Add a fast GitHub Actions CI workflow (.github/workflows/ci.yml)\n\nThis runs pre‑commit and your tests on Ubuntu with Python 3.11, with pip caching.\n\nfrom pathlib import Path\nwf = Path(\".github/workflows/ci.yml\")\nwf.write_text(\"\"\"name: CI\non:\n  push:\n    branches: [ main, master, develop ]\n  pull_request:\n    branches: [ main, master, develop ]\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\njobs:\n  build:\n    runs-on: ubuntu-latest\n    timeout-minutes: 10\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n          cache: 'pip'\n          cache-dependency-path: |\n            requirements.txt\n            pyproject.toml\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n          pip install pre-commit pytest\n\n      # Run pre-commit (Black, Ruff, nbstripout, etc.)\n      - name: pre-commit\n        uses: pre-commit/action@v3.0.1\n\n      # Run tests (fast only)\n      - name: pytest\n        run: pytest -q --maxfail=1\n\"\"\")\nprint(wf.read_text())\n\n\n14.4.9 8) Add a Makefile convenience (optional but nice)\nfrom pathlib import Path\nmk = Path(\"Makefile\")\ntext = mk.read_text() if mk.exists() else \"\"\nif \"lint\" not in text:\n    text += \"\"\"\n\n.PHONY: lint test ci-local\nlint: ## Run pre-commit hooks on all files\n\\tpre-commit run --all-files\n\ntest: ## Run fast tests\n\\tpytest -q --maxfail=1\n\nci-local: lint test ## Simulate CI locally\n\"\"\"\n    mk.write_text(text)\nprint(mk.read_text())",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>pre‑commit & GitHub Actions CI</span>"
    ]
  },
  {
    "objectID": "lec14.html#wrapup-10-min",
    "href": "lec14.html#wrapup-10-min",
    "title": "14  pre‑commit & GitHub Actions CI",
    "section": "14.5 Wrap‑up (10 min)",
    "text": "14.5 Wrap‑up (10 min)\n\nYou configured pre‑commit with Black, Ruff (lint + import sort), and nbstripout to keep the repo clean.\nYou added a fast CI that runs the same hooks plus pytest on every PR.\nCI time stays small due to caching and a lean dependency set; tests are fast by design (Session 13).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>pre‑commit & GitHub Actions CI</span>"
    ]
  },
  {
    "objectID": "lec14.html#homework-due-before-next-session",
    "href": "lec14.html#homework-due-before-next-session",
    "title": "14  pre‑commit & GitHub Actions CI",
    "section": "14.6 Homework (due before next session)",
    "text": "14.6 Homework (due before next session)\nGoal: Prove the workflow works end‑to‑end with a green PR from a fresh clone.\n\n14.6.1 Part A — Fresh‑clone smoke test (local)\n# On your laptop or a new Colab session:\ngit clone https://github.com/YOUR_USER/unified-stocks-teamX.git\ncd unified-stocks-teamX\npython -m pip install -U pip\npip install pre-commit pytest\npre-commit install\npre-commit run --all-files\npytest -q --maxfail=1\n\n\n14.6.2 Part B — Open a PR that turns CI green\n\nCreate a branch and make a tiny, style‑breaking change, then commit and let pre‑commit fix it automatically.\ngit checkout -b chore/ci-badge-and-hooks\necho \"# Tiny edit  \" &gt;&gt; README.md   # trailing spaces (will be fixed)\ngit add -A\ngit commit -m \"chore: add CI badge + enable pre-commit hooks\"\ngit push -u origin chore/ci-badge-and-hooks\nAdd a CI badge to README.md:\n![CI](https://github.com/YOUR_USER/unified-stocks-teamX/actions/workflows/ci.yml/badge.svg)\nOpen a Pull Request on GitHub. Verify that:\n\nThe pre‑commit step passes.\npytest passes.\nTotal runtime is &lt; ~3–4 minutes.\n\nMerge once green. (If red, fix locally; do not disable hooks.)\n\n\n\n14.6.3 Part C — (Optional) Tune Ruff + Black to your taste\n\nIn pyproject.toml, try:\n[tool.black]\nline-length = 100\n\n[tool.ruff]\nline-length = 100\n\n[tool.ruff.lint]\nselect = [\"E\",\"F\",\"I\",\"B\"]  # enable flake8-bugbear\nignore = [\"E501\"]\nRun pre-commit run --all-files and ensure CI remains green.\n\n\n\n14.6.4 Part D — (Optional) Add notebook QA without executing them\n\nAdd nbqa to run Ruff on notebooks (markdown & code cells):\n# append to .pre-commit-config.yaml\n- repo: https://github.com/nbQA-dev/nbQA\n  rev: 1.8.5\n  hooks:\n    - id: nbqa-ruff\n      args: [--fix]\n      additional_dependencies: [ruff==0.5.0]\nRe‑install hooks and run pre-commit run --all-files.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>pre‑commit & GitHub Actions CI</span>"
    ]
  },
  {
    "objectID": "lec14.html#reference-checklist-for-grading",
    "href": "lec14.html#reference-checklist-for-grading",
    "title": "14  pre‑commit & GitHub Actions CI",
    "section": "14.7 Reference checklist (for grading)",
    "text": "14.7 Reference checklist (for grading)\n\n.pre-commit-config.yaml present with Black, Ruff, nbstripout.\npyproject.toml includes [tool.black] and [tool.ruff] sections.\n.github/workflows/ci.yml runs pre‑commit and pytest with Python 3.11 and pip caching.\nmake lint, make test, make ci-local work (if you added them).\nA PR was opened and CI is green; README has the CI badge.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>pre‑commit & GitHub Actions CI</span>"
    ]
  },
  {
    "objectID": "lec14.html#instructor-tips-gotchas",
    "href": "lec14.html#instructor-tips-gotchas",
    "title": "14  pre‑commit & GitHub Actions CI",
    "section": "14.8 Instructor tips / gotchas",
    "text": "14.8 Instructor tips / gotchas\n\nIf pre‑commit says “no files to check” for nbstripout, ensure your file matcher files: \\.ipynb$ is correct and that notebooks are tracked.\nIf Ruff conflicts with Black on formatting: keep Black as the authority, disable E501 in Ruff, and let Ruff handle imports (I) and errors (E, F).\nCI failures from missing deps: ensure your requirements.txt (or pyproject.toml with [project.dependencies]) includes pandas, pyarrow, and pytest if your tests read Parquet.\nKeep CI lean: no data downloads or training; use fixtures and tiny synthetic datasets (Session 13 pattern).\n\nYou now have an automated quality gate—style, lint, and tests run locally and in CI—so your future PRs start green and stay green.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>pre‑commit & GitHub Actions CI</span>"
    ]
  },
  {
    "objectID": "lec15.html",
    "href": "lec15.html",
    "title": "15  Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)",
    "section": "",
    "text": "15.1 Session 15 — Framing & Metrics",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)</span>"
    ]
  },
  {
    "objectID": "lec15.html#session-15-framing-metrics-75-min",
    "href": "lec15.html#session-15-framing-metrics-75-min",
    "title": "15  Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)",
    "section": "",
    "text": "15.1.1 Learning goals\nBy the end of class, students can:\n\nSpecify forecast horizon \\(H\\), step (stride), and choose between expanding vs sliding rolling‑origin evaluation with an embargo gap.\nImplement a date‑based splitter that yields (train_idx, val_idx) for all tickers at once.\nCompute MAE, sMAPE, MASE (with a proper training‑window scale), and aggregate per‑ticker and across tickers (macro vs micro/weighted).\nProduce a tidy CSV of baseline results to serve as your course’s ground truth.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)</span>"
    ]
  },
  {
    "objectID": "lec15.html#agenda-75-min",
    "href": "lec15.html#agenda-75-min",
    "title": "15  Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)",
    "section": "15.2 Agenda (75 min)",
    "text": "15.2 Agenda (75 min)\n\n(10 min) Slides: forecasting setup — horizon \\(H\\), step, rolling‑origin (expanding vs sliding), embargo\n(10 min) Slides: metrics — MAE, sMAPE, MASE; aggregation across tickers (macro vs micro/weighted)\n(35 min) In‑class lab: implement a date‑based splitter → compute naive & seasonal‑naive baselines → MAE/sMAPE/MASE per split/ticker → save reports\n(10 min) Wrap‑up & homework brief\n(10 min) Buffer",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)</span>"
    ]
  },
  {
    "objectID": "lec15.html#slides-talking-points-add-these-bullets-to-your-deck",
    "href": "lec15.html#slides-talking-points-add-these-bullets-to-your-deck",
    "title": "15  Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)",
    "section": "15.3 Slides / talking points (add these bullets to your deck)",
    "text": "15.3 Slides / talking points (add these bullets to your deck)\n\n15.3.1 Framing the forecast\n\nTarget: next‑day log return \\(r_{t+1}\\) (you built this as r_1d).\nHorizon \\(H\\): 1 business day.\nStep (stride): how far the origin moves forward each split (e.g., 63 trading days ≈ a quarter).\nRolling‑origin schemes\n\nExpanding: train start fixed; train grows over time.\nSliding (rolling): fixed‑length train window slides forward.\n\nEmbargo: small gap (e.g., 5 days) between train end and validation start to avoid adjacency leakage.\n\n\n\n15.3.2 Metrics (scalar, easy to compare)\n\nMAE: \\(\\frac{1}{n}\\sum |y - \\hat{y}|\\) — robust & interpretable.\nsMAPE: \\(\\frac{2}{n}\\sum \\frac{|y - \\hat{y}|}{(|y| + |\\hat{y}| + \\epsilon)}\\) — scale‑free, safe for near‑zero returns with \\(\\epsilon\\).\nMASE: \\(\\text{MASE}=\\frac{\\text{MAE}_\\text{model}}{\\text{MAE}_\\text{naive (train)}}\\) — &lt;1 means better than naive.\n\nFor seasonality \\(s\\), the naive comparator predicts \\(y_{t+1} \\approx y_{t+1-s}\\) (we’ll use \\(s=5\\) for day‑of‑week seasonality on business days).\nScale is computed on the training window only, per ticker.\n\n\n\n\n15.3.3 Aggregation across tickers\n\nPer‑ticker metrics first → then aggregate.\nMacro average: mean of per‑ticker metrics (each ticker equal weight).\nMicro/weighted: pool all rows (or weight tickers by sample count); for MAE, pooled MAE equals sample‑count weighted average of per‑ticker MAEs.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)</span>"
    ]
  },
  {
    "objectID": "lec15.html#inclass-lab-35-min-colabfriendly",
    "href": "lec15.html#inclass-lab-35-min-colabfriendly",
    "title": "15  Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)",
    "section": "15.3 In‑class lab (35 min, Colab‑friendly)",
    "text": "15.3 In‑class lab (35 min, Colab‑friendly)\n\nRun each block as its own cell. Adjust REPO_NAME if needed.\n\n\n15.3.1 0) Setup & fallback data\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"  # &lt;- change to your repo name\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, numpy as np, pandas as pd\nfrom pathlib import Path\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/raw\",\"data/processed\",\"reports\",\"scripts\",\"tests\"]:\n    Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\n# Load returns or create a tiny fallback\nrpath = Path(\"data/processed/returns.parquet\")\nif rpath.exists():\n    returns = pd.read_parquet(rpath)\nelse:\n    # Fallback synthetic returns for 5 tickers, 320 business days\n    rng = np.random.default_rng(0)\n    dates = pd.bdate_range(\"2022-01-03\", periods=320)\n    frames=[]\n    for tkr in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\"]:\n        eps = rng.normal(0, 0.012, size=len(dates)).astype(\"float32\")\n        adj = 100*np.exp(np.cumsum(eps))\n        df = pd.DataFrame({\n            \"date\": dates,\n            \"ticker\": tkr,\n            \"adj_close\": adj.astype(\"float32\"),\n            \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n        })\n        df[\"r_1d\"] = df[\"log_return\"].shift(-1)\n        df[\"weekday\"] = df[\"date\"].dt.weekday.astype(\"int8\")\n        df[\"month\"]   = df[\"date\"].dt.month.astype(\"int8\")\n        frames.append(df)\n    returns = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)\n    returns[\"ticker\"] = returns[\"ticker\"].astype(\"category\")\n    returns.to_parquet(rpath, index=False)\n\n# Standardize\nreturns[\"date\"] = pd.to_datetime(returns[\"date\"])\nreturns = returns.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nreturns[\"ticker\"] = returns[\"ticker\"].astype(\"category\")\nreturns.head()\n\n\n15.3.2 1) Rolling‑origin date splitter (expanding windows + embargo)\nimport numpy as np, pandas as pd\n\ndef make_rolling_origin_splits(dates: pd.Series,\n                               train_min=252,   # ~1y of trading days\n                               val_size=63,     # ~1 quarter\n                               step=63,\n                               embargo=5):\n    \"\"\"Return a list of (train_start, train_end, val_start, val_end) date tuples.\"\"\"\n    u = np.array(sorted(pd.to_datetime(dates.unique())))\n    n = len(u)\n    splits=[]\n    i = train_min - 1\n    while True:\n        if i &gt;= n: break\n        tr_start, tr_end = u[0], u[i]\n        vs_idx = i + embargo + 1\n        ve_idx = vs_idx + val_size - 1\n        if ve_idx &gt;= n: break\n        splits.append((tr_start, tr_end, u[vs_idx], u[ve_idx]))\n        i += step\n    return splits\n\ndef splits_to_indices(df, split):\n    \"\"\"Map a date split to index arrays for the full multi-ticker frame.\"\"\"\n    a,b,c,d = split\n    tr_idx = df.index[(df[\"date\"]&gt;=a) & (df[\"date\"]&lt;=b)].to_numpy()\n    va_idx = df.index[(df[\"date\"]&gt;=c) & (df[\"date\"]&lt;=d)].to_numpy()\n    # sanity: embargo =&gt; last train date &lt; first val date\n    assert b &lt; c\n    return tr_idx, va_idx\n\nsplits = make_rolling_origin_splits(returns[\"date\"], train_min=252, val_size=63, step=63, embargo=5)\nlen(splits), splits[:2]\n\n\n15.3.3 2) Metrics & baseline predictors (naive and seasonal‑naive)\nfrom typing import Dict, Tuple\n\ndef mae(y, yhat): \n    y = np.asarray(y); yhat = np.asarray(yhat); \n    return float(np.mean(np.abs(y - yhat)))\n\ndef smape(y, yhat, eps=1e-8):\n    y = np.asarray(y); yhat = np.asarray(yhat)\n    return float(np.mean(2.0*np.abs(y - yhat)/(np.abs(y)+np.abs(yhat)+eps)))\n\ndef mase(y_true, y_pred, y_train_true, y_train_naive):\n    # Scale = MAE of comparator (naive) on TRAIN only; add tiny epsilon\n    scale = mae(y_train_true, y_train_naive) + 1e-12\n    return float(mae(y_true, y_pred) / scale)\n\ndef add_baseline_preds(df: pd.DataFrame, seasonality:int=5) -&gt; pd.DataFrame:\n    \"\"\"\n    For each ticker:\n      - naive predicts r_{t+1} ≈ log_return_t (s=1)\n      - seasonal naive (s) predicts r_{t+1} ≈ log_return_{t+1-s}  =&gt; shift(s-1)\n    Adds columns: yhat_naive, yhat_s{s}\n    \"\"\"\n    out = df.copy()\n    out[\"yhat_naive\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda s: s)  # s=1\n    if seasonality &lt;= 1:\n        out[\"yhat_s\"] = out[\"yhat_naive\"]\n    else:\n        out[\"yhat_s\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda s: s.shift(seasonality-1))\n    return out\n\n\n15.3.4 3) Evaluate baselines across first 2 splits (fast in class)\n# Precompute predictions over the entire frame once (safe: uses only past values via shift)\nseasonality = 5  # business-day weekly\npreds_all = add_baseline_preds(returns, seasonality=seasonality)\n\ndef per_ticker_metrics(df_val, df_train, method=\"naive\") -&gt; pd.DataFrame:\n    \"\"\"\n    Compute per-ticker MAE, sMAPE, MASE for the chosen method ('naive' or 's').\n    MASE scale uses TRAIN window and the same comparator as method.\n    \"\"\"\n    rows=[]\n    col = \"yhat_naive\" if method==\"naive\" else \"yhat_s\"\n    for tkr, g in df_val.groupby(\"ticker\"):\n        gv = g.dropna(subset=[\"r_1d\", col])\n        if len(gv)==0: \n            continue\n        # TRAIN scale (per ticker)\n        gt = df_train[df_train[\"ticker\"]==tkr].dropna(subset=[\"r_1d\"])\n        if method==\"naive\":\n            gt_pred = gt[\"log_return\"]  # s=1\n        else:\n            gt_pred = gt[\"log_return\"].shift(seasonality-1)\n        gt_clean = gt.dropna(subset=[\"r_1d\"]).copy()\n        gt_pred = gt_pred.loc[gt_clean.index]\n        gt_clean = gt_clean.dropna(subset=[\"r_1d\"])\n        # Align indices\n        y_tr = gt_clean[\"r_1d\"].to_numpy()\n        yhat_tr_naive = gt_pred.to_numpy()\n        # VAL metrics\n        y = gv[\"r_1d\"].to_numpy()\n        yhat = gv[col].to_numpy()\n        rows.append({\n            \"ticker\": tkr,\n            \"n\": int(len(y)),\n            \"mae\": mae(y,yhat),\n            \"smape\": smape(y,yhat),\n            \"mase\": mase(y, yhat, y_tr, yhat_tr_naive),\n        })\n    return pd.DataFrame(rows)\n\ndef aggregate_across_tickers(per_ticker_df: pd.DataFrame) -&gt; Dict[str,float]:\n    if per_ticker_df.empty:\n        return {\"macro_mae\":np.nan,\"macro_smape\":np.nan,\"macro_mase\":np.nan,\n                \"micro_mae\":np.nan,\"micro_smape\":np.nan,\"micro_mase\":np.nan}\n    # Macro = unweighted mean across tickers\n    macro = per_ticker_df[[\"mae\",\"smape\",\"mase\"]].mean().to_dict()\n    # Micro/weighted by n (pooled)\n    w = per_ticker_df[\"n\"].to_numpy()\n    micro = {\n        \"micro_mae\": float(np.average(per_ticker_df[\"mae\"], weights=w)),\n        \"micro_smape\": float(np.average(per_ticker_df[\"smape\"], weights=w)),\n        \"micro_mase\": float(np.average(per_ticker_df[\"mase\"], weights=w)),\n    }\n    return {f\"macro_{k}\": float(v) for k,v in macro.items()} | micro\n\n# Run on 2 splits in class; you can expand later\nimport pathlib, json\npathlib.Path(\"reports\").mkdir(exist_ok=True)\nrows=[]\nfor sid, split in enumerate(splits[:2], start=1):\n    a,b,c,d = split\n    tr_idx, va_idx = splits_to_indices(returns, split)\n    tr = preds_all.loc[tr_idx].copy()\n    va = preds_all.loc[va_idx].copy()\n    # Per-ticker metrics for two baselines\n    pt_naive = per_ticker_metrics(va, tr, method=\"naive\")\n    pt_s     = per_ticker_metrics(va, tr, method=\"s\")\n    agg_naive = aggregate_across_tickers(pt_naive)\n    agg_s     = aggregate_across_tickers(pt_s)\n    # Save per-split, per-ticker\n    pt_naive.to_csv(f\"reports/baseline_naive_split{sid}.csv\", index=False)\n    pt_s.to_csv(f\"reports/baseline_s{seasonality}_split{sid}.csv\", index=False)\n    rows.append({\n        \"split\": sid,\n        \"train_range\": f\"{a.date()}→{b.date()}\",\n        \"val_range\": f\"{c.date()}→{d.date()}\",\n        \"method\": \"naive\", **agg_naive\n    })\n    rows.append({\n        \"split\": sid,\n        \"train_range\": f\"{a.date()}→{b.date()}\",\n        \"val_range\": f\"{c.date()}→{d.date()}\",\n        \"method\": f\"s{seasonality}\", **agg_s\n    })\n\nsummary = pd.DataFrame(rows)\nsummary.to_csv(\"reports/baselines_rollingorigin_summary.csv\", index=False)\nsummary\n\n\n15.3.5 4) Quick sanity assertions (no overlap; embargo honored)\ndef check_no_overlap(df, split):\n    a,b,c,d = split\n    assert b &lt; c, f\"Embargo violation: train_end {b} &gt;= val_start {c}\"\n    tr_idx, va_idx = splits_to_indices(df, split)\n    assert set(tr_idx).isdisjoint(set(va_idx))\n    return True\n\nall(check_no_overlap(returns, s) for s in splits[:2]), len(summary)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)</span>"
    ]
  },
  {
    "objectID": "lec15.html#wrapup-10-min",
    "href": "lec15.html#wrapup-10-min",
    "title": "15  Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)",
    "section": "15.5 Wrap‑up (10 min)",
    "text": "15.5 Wrap‑up (10 min)\n\nYou now have a date‑based rolling‑origin splitter with an embargo, and baseline metrics that set a credible reference.\nMASE uses a training‑window naive as scale (per ticker), so you can read “&lt;1 is better than naive” at a glance.\nAggregation: report both macro (per‑ticker average) and micro/weighted (pooled).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)</span>"
    ]
  },
  {
    "objectID": "lec15.html#homework-due-before-session-16",
    "href": "lec15.html#homework-due-before-session-16",
    "title": "15  Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)",
    "section": "15.5 Homework (due before Session 16)",
    "text": "15.5 Homework (due before Session 16)\nGoal: Build a small CLI to reproduce these baselines over all splits, then generate per‑ticker & aggregated tables.\n\n15.5.1 Part A — Script: scripts/baselines_eval.py\n#!/usr/bin/env python\nfrom __future__ import annotations\nimport argparse, numpy as np, pandas as pd\nfrom pathlib import Path\n\ndef mae(y,yhat): return float(np.mean(np.abs(np.asarray(y)-np.asarray(yhat))))\ndef smape(y,yhat,eps=1e-8):\n    y = np.asarray(y); yhat = np.asarray(yhat)\n    return float(np.mean(2*np.abs(y-yhat)/(np.abs(y)+np.abs(yhat)+eps)))\ndef mase(y_true, y_pred, y_train_true, y_train_naive):\n    return float(mae(y_true, y_pred) / (mae(y_train_true, y_train_naive)+1e-12))\n\ndef make_splits(dates, train_min, val_size, step, embargo):\n    u = np.array(sorted(pd.to_datetime(dates.unique()))); n=len(u); out=[]; i=train_min-1\n    while True:\n        if i&gt;=n: break\n        a,b = u[0], u[i]; vs = i + embargo + 1; ve = vs + val_size - 1\n        if ve&gt;=n: break\n        out.append((a,b,u[vs],u[ve])); i += step\n    return out\n\ndef add_preds(df, s):\n    out = df.copy()\n    out[\"yhat_naive\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda x: x)\n    out[\"yhat_s\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda x: x.shift(s-1)) if s&gt;1 else out[\"yhat_naive\"]\n    return out\n\ndef per_ticker(df_val, df_train, method, s):\n    col = \"yhat_naive\" if method==\"naive\" else \"yhat_s\"\n    rows=[]\n    for tkr, g in df_val.groupby(\"ticker\"):\n        gv = g.dropna(subset=[\"r_1d\", col])\n        if len(gv)==0: continue\n        gt = df_train[df_train[\"ticker\"]==tkr].dropna(subset=[\"r_1d\"])\n        gt_pred = gt[\"log_return\"] if method==\"naive\" else gt[\"log_return\"].shift(s-1)\n        gt_pred = gt_pred.loc[gt.index]\n        y_tr = gt[\"r_1d\"].to_numpy(); yhat_tr = gt_pred.to_numpy()\n        y = gv[\"r_1d\"].to_numpy(); yhat = gv[col].to_numpy()\n        rows.append({\"ticker\":tkr,\"n\":int(len(y)),\n                     \"mae\": mae(y,yhat),\n                     \"smape\": smape(y,yhat),\n                     \"mase\": mase(y,yhat,y_tr,yhat_tr)})\n    return pd.DataFrame(rows)\n\ndef agg(pt):\n    if pt.empty: return {\"macro_mae\":np.nan,\"macro_smape\":np.nan,\"macro_mase\":np.nan,\n                         \"micro_mae\":np.nan,\"micro_smape\":np.nan,\"micro_mase\":np.nan}\n    macro = pt[[\"mae\",\"smape\",\"mase\"]].mean().to_dict()\n    w = pt[\"n\"].to_numpy()\n    micro = {\n        \"micro_mae\": float(np.average(pt[\"mae\"], weights=w)),\n        \"micro_smape\": float(np.average(pt[\"smape\"], weights=w)),\n        \"micro_mase\": float(np.average(pt[\"mase\"], weights=w)),\n    }\n    return {f\"macro_{k}\": float(v) for k,v in macro.items()} | micro\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--returns\", default=\"data/processed/returns.parquet\")\n    ap.add_argument(\"--seasonality\", type=int, default=5)\n    ap.add_argument(\"--train-min\", type=int, default=252)\n    ap.add_argument(\"--val-size\", type=int, default=63)\n    ap.add_argument(\"--step\", type=int, default=63)\n    ap.add_argument(\"--embargo\", type=int, default=5)\n    ap.add_argument(\"--out-summary\", default=\"reports/baselines_rollingorigin_summary.csv\")\n    ap.add_argument(\"--out-per-ticker\", default=\"reports/baselines_per_ticker_split{sid}_{method}.csv\")\n    # ap.add_argument(\"-f\", help=argparse.SUPPRESS)  # added fix to work in Colab\n    # args = ap.parse_args()  # not working in colab, but would if paired with previous line or use the folowing single fix\n    args, _ = ap.parse_known_args() # fix \n\n    df = pd.read_parquet(args.returns).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    splits = make_splits(df[\"date\"], args.train_min, args.val_size, args.step, args.embargo)\n    pred = add_preds(df, args.seasonality)\n\n    rows=[]\n    for sid, (a,b,c,d) in enumerate(splits, start=1):\n        tr = pred[(pred[\"date\"]&gt;=a)&(pred[\"date\"]&lt;=b)]\n        va = pred[(pred[\"date\"]&gt;=c)&(pred[\"date\"]&lt;=d)]\n        for method in [\"naive\",\"s\"]:\n            pt = per_ticker(va, tr, method, args.seasonality)\n            Path(\"reports\").mkdir(exist_ok=True)\n            pt.to_csv(args.out_per_ticker.format(sid=sid, method=method), index=False)\n            rows.append({\"split\":sid,\"train_range\":f\"{a.date()}→{b.date()}\",\"val_range\":f\"{c.date()}→{d.date()}\",\n                         \"method\":\"naive\" if method==\"naive\" else f\"s{args.seasonality}\", **agg(pt)})\n    pd.DataFrame(rows).to_csv(args.out_summary, index=False)\n    print(\"Wrote\", args.out_summary, \"and per-ticker CSVs.\")\n\nif __name__ == \"__main__\":\n    main()\nMake executable & run:\n%%bash\nchmod +x scripts/baselines_eval.py\npython scripts/baselines_eval.py --seasonality 5\n\n\n15.5.2 Part B — Plot a tiny, informative results figure\nimport pandas as pd, matplotlib.pyplot as plt, pathlib\npathlib.Path(\"docs/figs\").mkdir(parents=True, exist_ok=True)\n\nsummary = pd.read_csv(\"reports/baselines_rollingorigin_summary.csv\")\nplt.figure(figsize=(6,3.5))\nfor method, g in summary.groupby(\"method\"):\n    plt.plot(g[\"split\"], g[\"micro_mae\"], marker=\"o\", label=f\"{method} micro MAE\")\nplt.xlabel(\"Split\"); plt.ylabel(\"MAE\"); plt.title(\"Baseline MAE across splits\")\nplt.legend(); plt.tight_layout()\nplt.savefig(\"docs/figs/baselines_mae_splits.png\", dpi=200)\n\"Saved docs/figs/baselines_mae_splits.png\"\n\n\n15.5.3 Part C — Add a quick test to protect the splitter\n# tests/test_rolling_splitter.py\nimport pandas as pd, numpy as np\nfrom datetime import timedelta\n\ndef make_splits(dates, train_min, val_size, step, embargo):\n    u = np.array(sorted(pd.to_datetime(dates.unique()))); n=len(u); out=[]; i=train_min-1\n    while True:\n        if i&gt;=n: break\n        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve&gt;=n: break\n        out.append((a,b,u[vs],u[ve])); i+=step\n    return out\n\ndef test_embargo_and_order():\n    dates = pd.bdate_range(\"2024-01-01\", periods=400)\n    s = make_splits(pd.Series(dates), 252, 63, 63, 5)\n    assert all(b &lt; c for (a,b,c,d) in s), \"Embargo/order violated\"\n    # Splits should move forward\n    assert len(s) &gt;= 2 and s[1][1] &gt; s[0][1]\nRun:\n%%bash\npytest -q -k rolling_splitter\n\n\n15.5.4 Part D — (Optional) Makefile targets\n.PHONY: baselines\nbaselines: ## Evaluate naive & seasonal-naive baselines across all splits\n\\tpython scripts/baselines_eval.py --seasonality 5",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)</span>"
    ]
  },
  {
    "objectID": "lec15.html#instructor-checklist-before-class",
    "href": "lec15.html#instructor-checklist-before-class",
    "title": "15  Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)",
    "section": "15.7 Instructor checklist (before class)",
    "text": "15.7 Instructor checklist (before class)\n\nEnsure returns.parquet exists or fallback works.\nBe ready to whiteboard why the seasonal naïve for daily data uses s=5.\nEmphasize MASE scale from TRAIN and macro vs micro aggregation.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)</span>"
    ]
  },
  {
    "objectID": "lec15.html#emphasize-while-teaching",
    "href": "lec15.html#emphasize-while-teaching",
    "title": "15  Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)",
    "section": "15.8 Emphasize while teaching",
    "text": "15.8 Emphasize while teaching\n\nDefine the problem first (H, step, splits); metrics only make sense after framing.\nMASE &lt; 1 ⇒ better than naïve; report both macro & micro.\nEmbargo helps mitigate adjacency leakage; keep it small but nonzero.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)</span>"
    ]
  },
  {
    "objectID": "lec15.html#grading-passrevise",
    "href": "lec15.html#grading-passrevise",
    "title": "15  Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)",
    "section": "15.7 Grading (pass/revise)",
    "text": "15.7 Grading (pass/revise)\n\nRolling‑origin splitter implemented and used (train/val ranges printed).\nReports written: baselines_rollingorigin_summary.csv and per‑ticker CSVs per split & method.\nMetrics include MAE, sMAPE, MASE; aggregation includes macro and micro.\nA test asserts basic splitter properties (no overlap; forward progress).\n\nYou now have clear framing and metrics for your project. In Session 16, you’ll fit classical baselines (e.g., lags‑only linear, ARIMA/ETS quick sketches) and log them in the same results table schema.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)</span>"
    ]
  },
  {
    "objectID": "lec16.html",
    "href": "lec16.html",
    "title": "16  Session 16",
    "section": "",
    "text": "16.1 Session 16 — Classical baselines (75 min)\nBelow is a complete lecture package for Session 16 — Classical Baselines (75 minutes). It includes a timed agenda, slide talking points, a Colab‑friendly in‑class lab with copy‑paste code, and homework with copy‑paste code. In class you’ll train a lags‑only linear regressor per ticker and compare it to the naive and seasonal‑naive baselines from Session 15. You’ll also see a short, optional ARIMA demo and log results in a consistent schema for future comparison.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Session 16</span>"
    ]
  },
  {
    "objectID": "lec16.html#session-16-classical-baselines-75-min",
    "href": "lec16.html#session-16-classical-baselines-75-min",
    "title": "16  Session 16",
    "section": "",
    "text": "16.1.1 Learning goals\nBy the end of class, students can:\n\nFit a per‑ticker lags‑only linear regressor to predict next‑day log return \\(r_{t+1}\\).\nEvaluate models with MAE, sMAPE, MASE using the rolling‑origin splits (with embargo) from Session 15.\nLog results in a consistent table schema for per‑ticker and split‑level summaries.\nUnderstand ARIMA at a glance and its common pitfalls (optional demo).",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Session 16</span>"
    ]
  },
  {
    "objectID": "lec16.html#agenda-75-min",
    "href": "lec16.html#agenda-75-min",
    "title": "16  Session 16",
    "section": "16.2 Agenda (75 min)",
    "text": "16.2 Agenda (75 min)\n\n(10 min) Slides: where classical models fit; pitfalls with ARIMA; cross‑sectional regressors\n(10 min) Slides: results table schema & comparison to baselines\n(35 min) In‑class lab: train per‑ticker Linear (lags‑only) → evaluate across 2 splits → compare to naive/seasonal‑naive → log CSVs\n(10 min) Wrap‑up + homework brief\n(10 min) Buffer",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Session 16</span>"
    ]
  },
  {
    "objectID": "lec16.html#slides-talking-points",
    "href": "lec16.html#slides-talking-points",
    "title": "16  Session 16",
    "section": "16.3 Slides / talking points",
    "text": "16.3 Slides / talking points\n\n16.3.1 Why “classical” now?\n\nCreates a credible, strong baseline against naive that’s still transparent.\nSupports fast iteration and helps you debug feature definitions before deep models.\n\n\n\n16.3.2 Lags‑only linear regressor\n\nFeatures at time \\(t\\): lag1, lag2, lag3 (i.e., past returns), optionally a few stable stats (roll_std_20, zscore_20).\nTarget: r_1d (next‑day log return).\nFit per ticker to avoid cross‑sectional leakage for now.\n\n\n\n16.3.3 ARIMA 60‑second pitfall tour\n\nStationarity: fit on returns, not prices (unless differencing).\nEvaluation: re‑fit only on train; generate one‑step‑ahead forecasts on val, updating state without peeking.\nOver‑differencing & mis‑specified seasonal terms → bad bias.\nComputational cost grows with grid search; keep demo tiny.\n\n\n\n16.3.4 Results table schema (consistent across sessions)\n\nPer‑split summary: split, train_range, val_range, model, macro_mae, macro_smape, macro_mase, micro_mae, micro_smape, micro_mase\nPer‑ticker metrics: split, ticker, n, model, mae, smape, mase",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Session 16</span>"
    ]
  },
  {
    "objectID": "lec16.html#inclass-lab-35-min-colabfriendly",
    "href": "lec16.html#inclass-lab-35-min-colabfriendly",
    "title": "16  Session 16",
    "section": "16.4 In‑class lab (35 min, Colab‑friendly)",
    "text": "16.4 In‑class lab (35 min, Colab‑friendly)\n\nRun each block as its own cell. Update REPO_NAME if needed.\n\n\n16.4.1 0) Setup & data (with fallbacks)\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"  # &lt;- change if needed\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, numpy as np, pandas as pd\nfrom pathlib import Path\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/raw\",\"data/processed\",\"reports\",\"models\",\"scripts\",\"tests\"]:\n    Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\n# Load returns; if missing, synthesize\nrpath = Path(\"data/processed/returns.parquet\")\nif rpath.exists():\n    returns = pd.read_parquet(rpath)\nelse:\n    rng = np.random.default_rng(0)\n    dates = pd.bdate_range(\"2022-01-03\", periods=360)\n    rows=[]\n    for t in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\"]:\n        eps = rng.normal(0,0.012, size=len(dates)).astype(\"float32\")\n        adj = 100*np.exp(np.cumsum(eps))\n        df = pd.DataFrame({\n            \"date\": dates, \"ticker\": t,\n            \"adj_close\": adj.astype(\"float32\"),\n            \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n        })\n        df[\"r_1d\"] = df[\"log_return\"].shift(-1)\n        df[\"weekday\"] = df[\"date\"].dt.weekday.astype(\"int8\")\n        df[\"month\"]   = df[\"date\"].dt.month.astype(\"int8\")\n        rows.append(df)\n    returns = pd.concat(rows, ignore_index=True).dropna().reset_index(drop=True)\n    returns[\"ticker\"] = returns[\"ticker\"].astype(\"category\")\n    returns.to_parquet(rpath, index=False)\n\n# Load features_v1 or derive minimal lags from returns if missing\nfpath = Path(\"data/processed/features_v1.parquet\")\nif fpath.exists():\n    feats = pd.read_parquet(fpath)\nelse:\n    # Minimal lags derived just from returns\n    feats = returns.sort_values([\"ticker\",\"date\"]).copy()\n    for k in [1,2,3]:\n        feats[f\"lag{k}\"] = feats.groupby(\"ticker\")[\"log_return\"].shift(k)\n    feats = feats.dropna(subset=[\"lag1\",\"lag2\",\"lag3\",\"r_1d\"]).reset_index(drop=True)\n\n# Harmonize\nfeats[\"date\"] = pd.to_datetime(feats[\"date\"])\nfeats[\"ticker\"] = feats[\"ticker\"].astype(\"category\")\nfeats = feats.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nfeats.head()\n\n\n16.4.2 1) Rolling‑origin date splits (reuse Session 15 logic)\ndef make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    splits=[]; i = train_min-1; n=len(u)\n    while True:\n        if i&gt;=n: break\n        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve&gt;=n: break\n        splits.append((a,b,u[vs],u[ve])); i+=step\n    return splits\n\nsplits = make_rolling_origin_splits(feats[\"date\"], 252, 63, 63, 5)\nlen(splits), splits[:2]\n\n\n16.4.3 2) Metrics & baselines (from Session 15)\ndef mae(y, yhat): \n    y = np.asarray(y); yhat = np.asarray(yhat); \n    return float(np.mean(np.abs(y - yhat)))\n\ndef smape(y, yhat, eps=1e-8):\n    y = np.asarray(y); yhat = np.asarray(yhat)\n    return float(np.mean(2.0*np.abs(y - yhat)/(np.abs(y)+np.abs(yhat)+eps)))\n\ndef mase(y_true, y_pred, y_train_true, y_train_naive):\n    scale = mae(y_train_true, y_train_naive) + 1e-12\n    return float(mae(y_true, y_pred)/scale)\n\ndef add_baseline_preds(df: pd.DataFrame, seasonality:int=5) -&gt; pd.DataFrame:\n    out = df.copy()\n    out[\"yhat_naive\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda s: s)\n    out[\"yhat_s\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda s: s.shift(seasonality-1)) if seasonality&gt;1 else out[\"yhat_naive\"]\n    return out\n\n\n16.4.4 3) Per‑ticker lags‑only LinearRegression (fit only on each split’s TRAIN)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\n\n# Choose features (lags only for in-class lab)\nXCOLS = [c for c in [\"lag1\",\"lag2\",\"lag3\"] if c in feats.columns]\nassert XCOLS, \"No lag features found. Ensure features_v1 or fallback creation ran.\"\n\ndef fit_predict_lin_lags(train_df, val_df):\n    \"\"\"Fit per-ticker pipeline(StandardScaler, LinearRegression) on TRAIN; predict on VAL.\"\"\"\n    preds=[]\n    for tkr, tr in train_df.groupby(\"ticker\"):\n        va = val_df[val_df[\"ticker\"]==tkr]\n        if len(tr)==0 or len(va)==0: \n            continue\n        pipe = Pipeline([(\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n                         (\"lr\", LinearRegression())])\n        pipe.fit(tr[XCOLS].values, tr[\"r_1d\"].values)\n        yhat = pipe.predict(va[XCOLS].values)\n        out = va[[\"date\",\"ticker\",\"r_1d\",\"log_return\"]].copy()\n        out[\"yhat_linlags\"] = yhat.astype(\"float32\")\n        preds.append(out)\n    return pd.concat(preds, ignore_index=True) if preds else pd.DataFrame(columns=[\"date\",\"ticker\",\"r_1d\",\"log_return\",\"yhat_linlags\"])\n\n\n16.4.5 4) Evaluate across the first 2 splits; compare to naive/seasonal‑naive\nseasonality = 5\nfeats_baseline = add_baseline_preds(feats, seasonality=seasonality)\n\ndef per_ticker_metrics(df_val_pred, df_train, method_col):\n    rows=[]\n    for tkr, gv in df_val_pred.groupby(\"ticker\"):\n        if method_col not in gv: \n            continue\n        gv = gv.dropna(subset=[\"r_1d\", method_col])\n        if len(gv)==0: \n            continue\n        # TRAIN scale for MASE\n        gt = df_train[df_train[\"ticker\"]==tkr].dropna(subset=[\"r_1d\"])\n        gt_naive = gt[\"log_return\"] if \"yhat_s\" not in method_col else gt[\"log_return\"].shift(seasonality-1)\n        gt_naive = gt_naive.loc[gt.index]\n        rows.append({\n            \"ticker\": tkr,\n            \"n\": int(len(gv)),\n            \"mae\": mae(gv[\"r_1d\"], gv[method_col]),\n            \"smape\": smape(gv[\"r_1d\"], gv[method_col]),\n            \"mase\": mase(gv[\"r_1d\"], gv[method_col], gt[\"r_1d\"], gt_naive),\n        })\n    return pd.DataFrame(rows)\n\ndef summarize_split(feats_frame, sid, split, save_prefix=\"linlags\"):\n    a,b,c,d = split\n    tr = feats_frame[(feats_frame[\"date\"]&gt;=a)&(feats_frame[\"date\"]&lt;=b)].copy()\n    va = feats_frame[(feats_frame[\"date\"]&gt;=c)&(feats_frame[\"date\"]&lt;=d)].copy()\n    # Predictions\n    val_pred = fit_predict_lin_lags(tr, va)\n    # Attach baseline preds on val slice\n    va_base = add_baseline_preds(va, seasonality=seasonality)\n    val_pred = val_pred.merge(va_base[[\"date\",\"ticker\",\"yhat_naive\",\"yhat_s\"]], on=[\"date\",\"ticker\"], how=\"left\")\n\n    # Per-ticker metrics\n    pt_lin = per_ticker_metrics(val_pred, tr, \"yhat_linlags\"); pt_lin[\"model\"] = \"lin_lags\"\n    pt_nav = per_ticker_metrics(val_pred.rename(columns={\"yhat_naive\":\"yhat_linlags\"}), tr, \"yhat_linlags\"); pt_nav[\"model\"]=\"naive\"\n    pt_sea = per_ticker_metrics(val_pred.rename(columns={\"yhat_s\":\"yhat_linlags\"}), tr, \"yhat_linlags\"); pt_sea[\"model\"]=f\"s{seasonality}\"\n\n    # Save per-ticker\n    out_pt = pd.concat([pt_lin.assign(split=sid), pt_nav.assign(split=sid), pt_sea.assign(split=sid)], ignore_index=True)\n    out_pt.to_csv(f\"reports/{save_prefix}_per_ticker_split{sid}.csv\", index=False)\n\n    # Aggregate\n    def agg(df):\n        if df.empty: \n            return {\"macro_mae\":np.nan,\"macro_smape\":np.nan,\"macro_mase\":np.nan,\"micro_mae\":np.nan,\"micro_smape\":np.nan,\"micro_mase\":np.nan}\n        macro = df[[\"mae\",\"smape\",\"mase\"]].mean().to_dict()\n        w = df[\"n\"].to_numpy()\n        micro = {\"micro_mae\": float(np.average(df[\"mae\"], weights=w)),\n                 \"micro_smape\": float(np.average(df[\"smape\"], weights=w)),\n                 \"micro_mase\": float(np.average(df[\"mase\"], weights=w))}\n        return {f\"macro_{k}\":float(v) for k,v in macro.items()} | micro\n\n    rows=[]\n    for name, pt in [(\"lin_lags\", pt_lin), (\"naive\", pt_nav), (f\"s{seasonality}\", pt_sea)]:\n        rows.append({\"split\":sid, \"train_range\": f\"{a.date()}→{b.date()}\",\n                     \"val_range\": f\"{c.date()}→{d.date()}\",\n                     \"model\":name, **agg(pt)})\n    return pd.DataFrame(rows)\n\n# Run on first 2 splits in class\nsummary_frames=[]\nfor sid, split in enumerate(splits[:2], start=1):\n    sf = summarize_split(feats_baseline, sid, split, save_prefix=\"linlags\")\n    summary_frames.append(sf)\n\nsummary = pd.concat(summary_frames, ignore_index=True)\nsummary.to_csv(\"reports/linlags_summary_splits12.csv\", index=False)\nsummary\n\n\n16.4.6 5) (Optional) Tiny ARIMA demo on one ticker for the first split\n# Optional: quick ARIMA(1,0,0) demo predicting r_{t+1} on val for a single ticker\ntry:\n    from statsmodels.tsa.arima.model import ARIMA\n    import warnings; warnings.filterwarnings(\"ignore\")\n    a,b,c,d = splits[0]\n    tkr = feats[\"ticker\"].cat.categories[0]\n    tr = feats[(feats[\"ticker\"]==tkr) & (feats[\"date\"]&gt;=a) & (feats[\"date\"]&lt;=b)]\n    va = feats[(feats[\"ticker\"]==tkr) & (feats[\"date\"]&gt;=c) & (feats[\"date\"]&lt;=d)]\n    # Fit on TRAIN returns only (endog = log_return). Predict one-step ahead for VAL dates.\n    model = ARIMA(tr[\"log_return\"].to_numpy(), order=(1,0,0))\n    res = model.fit()\n    # Forecast length = len(va), one-step-ahead with dynamic=False updates internally\n    # (For strict no-peek rolling one-step, loop and append val true values; here we keep demo simple.)\n    fc = res.forecast(steps=len(va))\n    arima_mae = mae(va[\"r_1d\"], fc)  # compare against next-day return\n    float(arima_mae)\nexcept Exception as e:\n    print(\"ARIMA demo skipped:\", e)\n\n⚠️ ARIMA is optional and slow on large loops. If you try it per ticker/per split, keep the dataset tiny.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Session 16</span>"
    ]
  },
  {
    "objectID": "lec16.html#wrapup-10-min",
    "href": "lec16.html#wrapup-10-min",
    "title": "16  Session 16",
    "section": "16.5 Wrap‑up (10 min)",
    "text": "16.5 Wrap‑up (10 min)\n\nYou trained a per‑ticker lags‑only linear model and compared it fairly to naive and seasonal‑naive using the same splits and MASE scale (from the train window).\nYou logged results in a stable schema that you’ll reuse for future models (LSTM / Transformer).\nARIMA can be illustrative but is often fragile + slower; treat it as optional for your project scale.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Session 16</span>"
    ]
  },
  {
    "objectID": "lec16.html#homework-due-before-next-session",
    "href": "lec16.html#homework-due-before-next-session",
    "title": "16  Session 16",
    "section": "16.6 Homework (due before next session)",
    "text": "16.6 Homework (due before next session)\nGoal: 1) Run the linear lags baseline across all splits; 2) Write your first model card (Quarto) for the classical baseline.\n\n16.6.1 Part A — CLI script to evaluate Linear‑Lags across all splits\n# scripts/eval_linlags.py\n#!/usr/bin/env python\nfrom __future__ import annotations\nimport argparse, numpy as np, pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom pathlib import Path\n\ndef mae(y,yhat): return float(np.mean(np.abs(np.asarray(y)-np.asarray(yhat))))\ndef smape(y,yhat,eps=1e-8):\n    y = np.asarray(y); yhat = np.asarray(yhat)\n    return float(np.mean(2*np.abs(y-yhat)/(np.abs(y)+np.abs(yhat)+eps)))\ndef mase(y_true, y_pred, y_train_true, y_train_naive):\n    return float(mae(y_true, y_pred) / (mae(y_train_true, y_train_naive)+1e-12))\n\ndef make_splits(dates, train_min, val_size, step, embargo):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    splits=[]; i=train_min-1; n=len(u)\n    while True:\n        if i&gt;=n: break\n        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve&gt;=n: break\n        splits.append((a,b,u[vs],u[ve])); i+=step\n    return splits\n\ndef add_baselines(df, seasonality):\n    out = df.copy()\n    out[\"yhat_naive\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda s: s)\n    out[\"yhat_s\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda s: s.shift(seasonality-1)) if seasonality&gt;1 else out[\"yhat_naive\"]\n    return out\n\ndef fit_predict_lin(train_df, val_df, xcols):\n    from sklearn.linear_model import LinearRegression\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.pipeline import Pipeline\n    preds=[]\n    for tkr, tr in train_df.groupby(\"ticker\"):\n        va = val_df[val_df[\"ticker\"]==tkr]\n        if len(tr)==0 or len(va)==0: continue\n        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LinearRegression())])\n        pipe.fit(tr[xcols].values, tr[\"r_1d\"].values)\n        yhat = pipe.predict(va[xcols].values)\n        out = va[[\"date\",\"ticker\",\"r_1d\",\"log_return\"]].copy()\n        out[\"yhat_linlags\"] = yhat\n        preds.append(out)\n    return pd.concat(preds, ignore_index=True) if preds else pd.DataFrame()\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--features\", default=\"data/processed/features_v1.parquet\")\n    ap.add_argument(\"--seasonality\", type=int, default=5)\n    ap.add_argument(\"--train-min\", type=int, default=252)\n    ap.add_argument(\"--val-size\", type=int, default=63)\n    ap.add_argument(\"--step\", type=int, default=63)\n    ap.add_argument(\"--embargo\", type=int, default=5)\n    ap.add_argument(\"--xcols\", nargs=\"+\", default=[\"lag1\",\"lag2\",\"lag3\"])\n    ap.add_argument(\"--out-summary\", default=\"reports/linlags_summary.csv\")\n    ap.add_argument(\"--out-per-ticker\", default=\"reports/linlags_per_ticker_split{sid}.csv\")\n    args = ap.parse_args()\n\n    df = pd.read_parquet(args.features).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    df[\"ticker\"] = df[\"ticker\"].astype(\"category\")\n    splits = make_splits(df[\"date\"], args.train_min, args.val_size, args.step, args.embargo)\n    df = add_baselines(df, args.seasonality)\n\n    rows=[]\n    for sid, (a,b,c,d) in enumerate(splits, start=1):\n        tr = df[(df[\"date\"]&gt;=a)&(df[\"date\"]&lt;=b)]\n        va = df[(df[\"date\"]&gt;=c)&(df[\"date\"]&lt;=d)]\n        val_pred = fit_predict_lin(tr, va, args.xcols)\n        va = va.merge(val_pred[[\"date\",\"ticker\",\"yhat_linlags\"]], on=[\"date\",\"ticker\"], how=\"left\")\n        # per-ticker\n        pts=[]\n        for tkr, gv in va.groupby(\"ticker\"):\n            gv = gv.dropna(subset=[\"r_1d\",\"yhat_linlags\"])\n            if len(gv)==0: continue\n            gt = tr[tr[\"ticker\"]==tkr].dropna(subset=[\"r_1d\"])\n            gt_naive = gt[\"log_return\"]  # scale comparator for MASE\n            pts.append({\"ticker\":tkr,\"n\":int(len(gv)),\n                        \"mae\": mae(gv[\"r_1d\"], gv[\"yhat_linlags\"]),\n                        \"smape\": smape(gv[\"r_1d\"], gv[\"yhat_linlags\"]),\n                        \"mase\": mase(gv[\"r_1d\"], gv[\"yhat_linlags\"], gt[\"r_1d\"], gt_naive)})\n        pt = pd.DataFrame(pts)\n        Path(\"reports\").mkdir(exist_ok=True)\n        pt.assign(split=sid, model=\"lin_lags\").to_csv(args.out_per_ticker.format(sid=sid), index=False)\n\n        # aggregate\n        if not pt.empty:\n            macro = pt[[\"mae\",\"smape\",\"mase\"]].mean().to_dict()\n            w = pt[\"n\"].to_numpy()\n            micro = {\"micro_mae\": float(np.average(pt[\"mae\"], weights=w)),\n                     \"micro_smape\": float(np.average(pt[\"smape\"], weights=w)),\n                     \"micro_mase\": float(np.average(pt[\"mase\"], weights=w))}\n        else:\n            macro = {\"mae\":np.nan,\"smape\":np.nan,\"mase\":np.nan}\n            micro = {\"micro_mae\":np.nan,\"micro_smape\":np.nan,\"micro_mase\":np.nan}\n        rows.append({\"split\":sid,\"train_range\":f\"{a.date()}→{b.date()}\",\"val_range\":f\"{c.date()}→{d.date()}\",\n                     \"model\":\"lin_lags\", \"macro_mae\":float(macro[\"mae\"]), \"macro_smape\":float(macro[\"smape\"]), \"macro_mase\":float(macro[\"mase\"]),\n                     **micro})\n\n    pd.DataFrame(rows).to_csv(args.out_summary, index=False)\n    print(\"Wrote\", args.out_summary)\n\nif __name__ == \"__main__\":\n    main()\nMake executable & run:\n%%bash\nchmod +x scripts/eval_linlags.py\npython scripts/eval_linlags.py --xcols lag1 lag2 lag3\n\n\n16.6.2 Part B — Quarto Model Card for the Linear‑Lags baseline\nCreate docs/model_card_linear.qmd:\n---\ntitle: \"Model Card — Linear Lags (Per‑Ticker)\"\nformat:\n  html:\n    theme: cosmo\n    toc: true\nparams:\n  model_name: \"Linear Lags (per‑ticker)\"\n  data: \"features_v1.parquet\"\n---\n\n&gt; **Educational use only — not trading advice.** Predicts next‑day log return \\(r_{t+1}\\) using past lags.\n\n## Overview\n\n- **Model:** Per‑ticker linear regression with features: `lag1`, `lag2`, `lag3`.\n- **Data:** `features_v1.parquet` (Session 10).  \n- **Splits:** Expanding, quarterly val, 5‑day embargo (Session 15).  \n- **Baselines:** Naive and seasonal‑naive \\(s=5\\).\n\n## Metrics (across splits)\n\n::: {#50e1d667 .cell execution_count=1}\n````` {.python .cell-code}\nimport pandas as pd\ndf = pd.read_csv(\"reports/linlags_summary.csv\")\ndf",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Session 16</span>"
    ]
  },
  {
    "objectID": "lec16.html#discussion",
    "href": "lec16.html#discussion",
    "title": "16  Session 16",
    "section": "16.7 Discussion",
    "text": "16.7 Discussion\n\nAssumptions: Linear relation to recent returns; stationarity at return level.\nStrengths: Fast, interpretable, leakage‑resistant with proper splits.\nFailure modes: Regime shifts; volatility spikes; nonlinearity.\nEthics: Educational; not suitable for trading.\n\n\nRender (if Quarto is available):\n```bash\nquarto render docs/model_card_linear.qmd\n\n16.7.1 Part C — Quick test to safeguard results shape\n# tests/test_linlags_results.py\nimport pandas as pd, os\n\ndef test_linlags_summary_exists_and_columns():\n    assert os.path.exists(\"reports/linlags_summary.csv\")\n    df = pd.read_csv(\"reports/linlags_summary.csv\")\n    need = {\"split\",\"model\",\"macro_mae\",\"micro_mae\"}\n    assert need.issubset(df.columns)\n:::\nRun:\n%%bash\npytest -q -k linlags_results\n\n\n16.7.2 Part D — (Optional) Extend features or add Ridge\n\nTry --xcols lag1 lag2 lag3 roll_std_20 zscore_20 (if present in features_v1).\nSwap LinearRegression for Ridge(alpha=1.0); log and compare.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Session 16</span>"
    ]
  },
  {
    "objectID": "lec16.html#instructor-checklist-before-class",
    "href": "lec16.html#instructor-checklist-before-class",
    "title": "16  Session 16",
    "section": "16.8 Instructor checklist (before class)",
    "text": "16.8 Instructor checklist (before class)\n\nVerify features_v1.parquet has lag1..lag3 or the fallback cell creates them.\nDry‑run the 2‑split demo; ensure total runtime &lt; 5–6 minutes.\nOptionally prepare an ARIMA demo on one ticker to illustrate pitfalls.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Session 16</span>"
    ]
  },
  {
    "objectID": "lec16.html#emphasize-while-teaching",
    "href": "lec16.html#emphasize-while-teaching",
    "title": "16  Session 16",
    "section": "16.9 Emphasize while teaching",
    "text": "16.9 Emphasize while teaching\n\nKeep splits identical across models for fair comparison.\nMASE &lt; 1 ⇒ your model beats naive on train‑scale; report macro & micro.\nLinear lags are a transparent baseline—use them to validate your entire pipeline.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Session 16</span>"
    ]
  },
  {
    "objectID": "lec16.html#grading-passrevise",
    "href": "lec16.html#grading-passrevise",
    "title": "16  Session 16",
    "section": "16.10 Grading (pass/revise)",
    "text": "16.10 Grading (pass/revise)\n\nscripts/eval_linlags.py runs and writes reports/linlags_summary.csv + per‑ticker CSVs.\nModel card exists and renders (locally or in CI artifact).\nTests for results table shape pass.\nResults show a reasonable comparison against naive/seasonal‑naive.\n\nYou now have a solid classical baseline with a reproducible evaluation and reporting workflow—perfect for benchmarking upcoming neural models.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Session 16</span>"
    ]
  },
  {
    "objectID": "lec17.html",
    "href": "lec17.html",
    "title": "17  Session 17 — Feature Timing, Biases & Leakage",
    "section": "",
    "text": "17.1 Session 17 — Feature Timing, Biases & Leakage (75 min)\nBelow is a complete lecture package for Session 17 — Feature Timing, Biases & Leakage (75 minutes). It includes a timed agenda, slide talking points, a Colab‑friendly in‑class lab with copy‑paste code, and homework with copy‑paste code. In class you’ll freeze a static ticker universe (avoid survivorship bias), formalize label definitions (t+1 and multi‑step), and add a leakage test suite that fails if any feature at time t uses information from t+1 or later.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Session 17 — Feature Timing, Biases & Leakage</span>"
    ]
  },
  {
    "objectID": "lec17.html#session-17-feature-timing-biases-leakage-75-min",
    "href": "lec17.html#session-17-feature-timing-biases-leakage-75-min",
    "title": "17  Session 17 — Feature Timing, Biases & Leakage",
    "section": "",
    "text": "17.1.1 Learning goals\nBy the end of class, students can:\n\nExplain and avoid look‑ahead and survivorship biases.\nFreeze and use a static ticker universe chosen from the train window (not the whole history).\nDefine labels correctly (e.g., t+1 and t+5) and verify them with tests.\nAdd leakage tests that recompute trusted features and fail on any future‑peek.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Session 17 — Feature Timing, Biases & Leakage</span>"
    ]
  },
  {
    "objectID": "lec17.html#agenda-75-min",
    "href": "lec17.html#agenda-75-min",
    "title": "17  Session 17 — Feature Timing, Biases & Leakage",
    "section": "17.2 Agenda (75 min)",
    "text": "17.2 Agenda (75 min)\n\n(10 min) Slides: what leakage looks like; examples; how it sneaks in\n(10 min) Slides: survivorship bias (today’s constituents ≠ past reality); freezing a universe\n(10 min) Slides: label definitions (t+1, multi‑step) and alignment rules\n(35 min) In‑class lab:\n\nFreeze a static universe from the first split’s train window\nAdd leakage tests that recompute known‑good features\nAdd multi‑step labels (e.g., t+5) with tests\n\n(10 min) Wrap‑up & homework brief",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Session 17 — Feature Timing, Biases & Leakage</span>"
    ]
  },
  {
    "objectID": "lec17.html#slides-talking-points-drop-into-your-deck",
    "href": "lec17.html#slides-talking-points-drop-into-your-deck",
    "title": "17  Session 17 — Feature Timing, Biases & Leakage",
    "section": "17.3 Slides / talking points (drop into your deck)",
    "text": "17.3 Slides / talking points (drop into your deck)\n\n17.3.1 What is data leakage?\n\nLook‑ahead leakage: using any info from t+1 or later to compute features at t or to scale/normalize train and validation together.\nCommon culprits: shift(-1) in features, global scaling fit on full data, forward‑fill across split boundaries, using today’s close to predict today’s close.\n\n\n\n17.3.2 Survivorship bias\n\nUsing today’s index membership to pick tickers for the past ⇒ drops delisted/removed names ⇒ optimistically biased results.\nCure: freeze a static universe from the training window (e.g., all tickers with ≥ 252 observations by the end of the first train window). Save it and filter by it for all future experiments.\n\n\n\n17.3.3 Label definitions (be explicit)\n\nt+1 log return: r_1d = log_return.shift(-1) per ticker (your Session‑9 label).\nt+5 log return (multi‑step): r_5d = log_return.shift(-1) + … + log_return.shift(-5) per ticker.\nRules: labels come from future; features come from ≤ t. Splits with embargo reduce adjacency leakage.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Session 17 — Feature Timing, Biases & Leakage</span>"
    ]
  },
  {
    "objectID": "lec17.html#inclass-lab-35-min-colabfriendly",
    "href": "lec17.html#inclass-lab-35-min-colabfriendly",
    "title": "17  Session 17 — Feature Timing, Biases & Leakage",
    "section": "17.4 In‑class lab (35 min, Colab‑friendly)",
    "text": "17.4 In‑class lab (35 min, Colab‑friendly)\n\nRun each block as its own cell. Update REPO_NAME as needed.\n\n\n17.4.1 0) Setup & load data (with fallbacks)\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"   # &lt;- change if needed\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, numpy as np, pandas as pd\nfrom pathlib import Path\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/raw\",\"data/processed\",\"data/static\",\"reports\",\"scripts\",\"tests\"]:\n    Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\n# Load returns or synthesize a small fallback\nrpath = Path(\"data/processed/returns.parquet\")\nif rpath.exists():\n    returns = pd.read_parquet(rpath)\nelse:\n    rng = np.random.default_rng(0)\n    dates = pd.bdate_range(\"2022-01-03\", periods=360)\n    rows=[]\n    for t in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\",\"TSLA\",\"META\",\"NFLX\"]:\n        eps = rng.normal(0,0.012,size=len(dates)).astype(\"float32\")\n        adj = 100*np.exp(np.cumsum(eps))\n        df = pd.DataFrame({\n            \"date\": dates, \"ticker\": t,\n            \"adj_close\": adj.astype(\"float32\"),\n            \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n        })\n        df[\"r_1d\"] = df[\"log_return\"].shift(-1)\n        df[\"weekday\"] = df[\"date\"].dt.weekday.astype(\"int8\")\n        df[\"month\"]   = df[\"date\"].dt.month.astype(\"int8\")\n        rows.append(df)\n    returns = pd.concat(rows, ignore_index=True).dropna().reset_index(drop=True)\n    returns[\"ticker\"] = returns[\"ticker\"].astype(\"category\")\n    returns.to_parquet(rpath, index=False)\n\n# Load features_v1 or construct minimal lags for tests\nfpath = Path(\"data/processed/features_v1.parquet\")\nif fpath.exists():\n    feats = pd.read_parquet(fpath).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nelse:\n    feats = returns.sort_values([\"ticker\",\"date\"]).copy()\n    for k in [1,2,3]:\n        feats[f\"lag{k}\"] = feats.groupby(\"ticker\")[\"log_return\"].shift(k)\n    feats[\"roll_mean_20\"] = feats.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).mean().reset_index(level=0, drop=True)\n    feats[\"roll_std_20\"]  = feats.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)\n    feats[\"zscore_20\"]    = (feats[\"log_return\"] - feats[\"roll_mean_20\"]) / (feats[\"roll_std_20\"] + 1e-8)\n    feats = feats.dropna().reset_index(drop=True)\n\n# Harmonize types\nreturns[\"date\"] = pd.to_datetime(returns[\"date\"])\nfeats[\"date\"]   = pd.to_datetime(feats[\"date\"])\nreturns[\"ticker\"] = returns[\"ticker\"].astype(\"category\")\nfeats[\"ticker\"]   = feats[\"ticker\"].astype(\"category\")\nreturns = returns.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nfeats   = feats.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nreturns.head(3), feats.head(3)\n\n\n17.4.2 1) Freeze a static universe from the first split’s train window\nimport numpy as np, pandas as pd\n\ndef make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    i = train_min - 1; splits=[]\n    while True:\n        if i &gt;= len(u): break\n        a,b = u[0], u[i]\n        vs = i + embargo + 1\n        ve = vs + val_size - 1\n        if ve &gt;= len(u): break\n        splits.append((a,b,u[vs],u[ve]))\n        i += step\n    return splits\n\nsplits = make_rolling_origin_splits(returns[\"date\"], train_min=252, val_size=63, step=63, embargo=5)\nassert len(splits) &gt;= 1, \"Not enough history for a first split.\"\na,b,c,d = splits[0]\nprint(\"First train window:\", a.date(), \"→\", b.date())\n\n# Eligible = tickers with at least train_min rows by train_end (b)\ntrain_slice = returns[(returns[\"date\"]&gt;=a) & (returns[\"date\"]&lt;=b)]\ncounts = train_slice.groupby(\"ticker\").size()\neligible = counts[counts &gt;= 252].index.sort_values()\nuniverse = pd.DataFrame({\"ticker\": eligible})\nuniv_name = f\"data/static/universe_{b.date()}.csv\"\nuniverse.to_csv(univ_name, index=False)\nprint(\"Saved static universe:\", univ_name, \"| tickers:\", len(universe))\nuniverse.head()\n\nFrom now on, filter your data to universe before modeling/evaluation.\n\n\n\n17.4.3 2) Apply the static universe to your features\nfeats_static = feats[feats[\"ticker\"].isin(set(universe[\"ticker\"]))].copy()\nfeats_static.to_parquet(\"data/processed/features_v1_static.parquet\", compression=\"zstd\", index=False)\nprint(\"Wrote data/processed/features_v1_static.parquet\", feats_static.shape)\n\n\n17.4.4 3) Add leakage tests that recompute trusted features & compare\nCreate a high‑value test file that fails if any feature depends on future rows.\n# tests/test_leakage_features.py\nfrom __future__ import annotations\nimport numpy as np, pandas as pd\nimport pytest\n\nSAFE_ROLL = 20\n\n@pytest.fixture(scope=\"session\")\ndef df():\n    import pandas as pd\n    import pathlib\n    p = pathlib.Path(\"data/processed/features_v1_static.parquet\")\n    if not p.exists():\n        p = pathlib.Path(\"data/processed/features_v1.parquet\")\n    df = pd.read_parquet(p).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    return df\n\ndef test_label_definition_r1d(df):\n    for tkr, g in df.groupby(\"ticker\"):\n        assert g[\"r_1d\"].iloc[:-1].equals(g[\"log_return\"].iloc[1:]), f\"r_1d mismatch for {tkr}\"\n\ndef _recompute_safe(g: pd.DataFrame) -&gt; pd.DataFrame:\n    # Recompute causal features using only &lt;= t information\n    out = pd.DataFrame(index=g.index)\n    s = g[\"log_return\"]\n    out[\"lag1\"] = s.shift(1)\n    out[\"lag2\"] = s.shift(2)\n    out[\"lag3\"] = s.shift(3)\n    rm = s.rolling(SAFE_ROLL, min_periods=SAFE_ROLL).mean()\n    rs = s.rolling(SAFE_ROLL, min_periods=SAFE_ROLL).std()\n    out[\"roll_mean_20\"] = rm\n    out[\"roll_std_20\"]  = rs\n    out[\"zscore_20\"]    = (s - rm) / (rs + 1e-8)\n    # EWM & expanding if present\n    out[\"exp_mean\"] = s.expanding(min_periods=SAFE_ROLL).mean()\n    out[\"exp_std\"]  = s.expanding(min_periods=SAFE_ROLL).std()\n    out[\"ewm_mean_20\"] = s.ewm(span=20, adjust=False).mean()\n    out[\"ewm_std_20\"]  = s.ewm(span=20, adjust=False).std()\n    # RSI(14) if adj_close present\n    if \"adj_close\" in g:\n        delta = g[\"adj_close\"].diff()\n        up = delta.clip(lower=0).ewm(alpha=1/14, adjust=False).mean()\n        dn = (-delta.clip(upper=0)).ewm(alpha=1/14, adjust=False).mean()\n        rs = up / (dn + 1e-12)\n        out[\"rsi_14\"] = 100 - (100/(1+rs))\n    return out\n\n@pytest.mark.parametrize(\"col\", [\"lag1\",\"lag2\",\"lag3\",\"roll_mean_20\",\"roll_std_20\",\"zscore_20\",\"exp_mean\",\"exp_std\",\"ewm_mean_20\",\"ewm_std_20\",\"rsi_14\"])\ndef test_features_match_causal_recompute(df, col):\n    if col not in df.columns:\n        pytest.skip(f\"{col} not present\")\n    # Compare per ticker to avoid cross-group alignment issues\n    for tkr, g in df.groupby(\"ticker\", sort=False):\n        ref = _recompute_safe(g)\n        if col not in ref.columns: \n            continue\n        a = g[col].to_numpy()\n        b = ref[col].to_numpy()\n        # Allow NaNs at the start; compare where both finite\n        mask = np.isfinite(a) & np.isfinite(b)\n        if mask.sum() == 0: \n            continue\n        diff = np.nanmax(np.abs(a[mask] - b[mask]))\n        assert float(diff) &lt;= 1e-6, f\"{col} deviates from causal recompute for {tkr}: max |Δ|={diff}\"\n\ndef test_no_feature_equals_target(df):\n    y = df[\"r_1d\"].to_numpy()\n    for col in df.select_dtypes(include=[\"float32\",\"float64\"]).columns:\n        if col in {\"r_1d\",\"log_return\"}: \n            continue\n        x = df[col].to_numpy()\n        # Proportion of exact equality (within tiny tol) should not be high\n        eq = np.isfinite(x) & np.isfinite(y) & (np.abs(x - y) &lt; 1e-12)\n        assert eq.mean() &lt; 0.8, f\"Suspicious: feature {col} equals target too often\"\nRun tests now:\n!pytest -q tests/test_leakage_features.py\n\nIf a test fails, fix the pipeline, don’t weaken the test.\n\n\n\n17.4.5 4) Add multi‑step labels (e.g., t+5) and tests\n# scripts/make_multistep_labels.py\nfrom __future__ import annotations\nimport pandas as pd, numpy as np\nfrom pathlib import Path\n\ndef make_multistep(in_parquet=\"data/processed/returns.parquet\", horizons=(5,)):\n    df = pd.read_parquet(in_parquet).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    for H in horizons:\n        # r_Hd = sum of next H log returns: shift(-1) ... shift(-H)\n        s = df.groupby(\"ticker\")[\"log_return\"]\n        acc = None\n        for h in range(1, H+1):\n            sh = s.shift(-h)\n            acc = sh if acc is None else (acc + sh)\n        df[f\"r_{H}d\"] = acc\n    out = df\n    Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n    out.to_parquet(\"data/processed/returns_multistep.parquet\", compression=\"zstd\", index=False)\n    print(\"Wrote data/processed/returns_multistep.parquet\", out.shape)\n\nif __name__ == \"__main__\":\n    make_multistep()\nRun it:\n!python scripts/make_multistep_labels.py\nAdd a test for label correctness:\n# tests/test_labels_multistep.py\nimport pandas as pd, numpy as np\n\ndef test_r5d_definition():\n    df = pd.read_parquet(\"data/processed/returns_multistep.parquet\").sort_values([\"ticker\",\"date\"])\n    if \"r_5d\" not in df.columns:\n        return\n    for tkr, g in df.groupby(\"ticker\"):\n        lr = g[\"log_return\"]\n        r5 = sum(lr.shift(-h) for h in range(1,6))\n        diff = (g[\"r_5d\"] - r5).abs().max()\n        assert float(diff) &lt; 1e-10, f\"r_5d misdefined for {tkr} (max |Δ|={diff})\"\nRun:\n!pytest -q tests/test_labels_multistep.py",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Session 17 — Feature Timing, Biases & Leakage</span>"
    ]
  },
  {
    "objectID": "lec17.html#wrapup-10-min",
    "href": "lec17.html#wrapup-10-min",
    "title": "17  Session 17 — Feature Timing, Biases & Leakage",
    "section": "17.5 Wrap‑up (10 min)",
    "text": "17.5 Wrap‑up (10 min)\n\nStatic universe removes survivorship bias: pick tickers with adequate history by train end and stick to them.\nLabel definitions must be explicit and tested (t+1, t+5).\nLeakage tests recompute causal features and compare—if you accidentally used shift(-1) or cross‑split fills, tests fail.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Session 17 — Feature Timing, Biases & Leakage</span>"
    ]
  },
  {
    "objectID": "lec17.html#homework-due-before-session-18",
    "href": "lec17.html#homework-due-before-session-18",
    "title": "17  Session 17 — Feature Timing, Biases & Leakage",
    "section": "17.6 Homework (due before Session 18)",
    "text": "17.6 Homework (due before Session 18)\nGoal: Document your evaluation protocol and ship a concise “leakage & bias” memo, plus a one‑command audit.\n\n17.6.1 Part A — Generate a protocol memo (reports/eval_protocol.md)\n# scripts/write_eval_protocol.py\nfrom __future__ import annotations\nimport pandas as pd, numpy as np\nfrom pathlib import Path\nfrom datetime import date\n\ndef make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    i = train_min - 1; out=[]\n    while True:\n        if i &gt;= len(u): break\n        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve &gt;= len(u): break\n        out.append((a,b,u[vs],u[ve])); i += step\n    return out\n\ndef main():\n    ret = pd.read_parquet(\"data/processed/returns.parquet\").sort_values([\"ticker\",\"date\"])\n    splits = make_rolling_origin_splits(ret[\"date\"])\n    a,b,c,d = splits[0]\n    # Universe info\n    univ_files = sorted(Path(\"data/static\").glob(\"universe_*.csv\"))\n    univ = univ_files[-1] if univ_files else None\n    univ_count = pd.read_csv(univ).shape[0] if univ else ret[\"ticker\"].nunique()\n    md = []\n    md += [\"# Evaluation Protocol (Leakage‑Aware)\", \"\"]\n    md += [\"**Date:** \" + date.today().isoformat(), \"\"]\n    md += [\"## Splits\", f\"- Train window (split 1): **{a.date()} → {b.date()}**\",\n           f\"- Embargo: **5** business days\", f\"- Validation window: **{c.date()} → {d.date()}**\",\n           f\"- Step between origins: **63** business days\", \"\"]\n    md += [\"## Static Universe\", f\"- Universe file: **{univ.name if univ else '(none)'}**\",\n           f\"- Count: **{univ_count}** tickers\", \n           \"- Selection rule: tickers with ≥252 obs by first train end; fixed for all splits.\", \"\"]\n    md += [\"## Labels\", \"- `r_1d` = next‑day log return `log_return.shift(-1)` per ticker.\",\n           \"- `r_5d` (if used) = sum of `log_return.shift(-1..-5)`.\", \"\"]\n    md += [\"## Leakage Controls\",\n           \"- Features computed from ≤ t only (rolling/ewm/expanding without negative shifts).\",\n           \"- No forward‑fill across split boundaries; embargo = 5 days.\",\n           \"- Scalers/normalizers fit on TRAIN only.\",\n           \"- Tests: `tests/test_leakage_features.py`, `tests/test_labels_multistep.py`.\", \"\"]\n    md += [\"## Caveats\",\n           \"- Educational dataset; not investment advice.\",\n           \"- Survivorship minimized via static universe; still subject to data vendor quirks.\", \"\"]\n    Path(\"reports\").mkdir(parents=True, exist_ok=True)\n    Path(\"reports/eval_protocol.md\").write_text(\"\\n\".join(md))\n    print(\"Wrote reports/eval_protocol.md\")\n\nif __name__ == \"__main__\":\n    main()\nRun:\n!python scripts/write_eval_protocol.py\n\n\n17.6.2 Part B — One‑command leakage audit target\nAppend to your Makefile:\n.PHONY: leakage-audit\nleakage-audit: ## Run leakage & label tests; write eval protocol\n\\tpytest -q tests/test_leakage_features.py tests/test_labels_multistep.py\n\\tpython scripts/write_eval_protocol.py\nThen run:\nmake leakage-audit\n\n\n17.6.3 Part C — Short memo (1–2 pages max)\n\nOpen reports/eval_protocol.md and add two paragraphs in your own words:\n\nWhy these splits and embargo are credible for your task.\nWhere leakage could still hide (e.g., future macro revisions, implicit target leakage), and how you’d detect it.\n\n\n\nSubmit the updated reports/eval_protocol.md and a screenshot of make leakage-audit passing.\n\n\n\n17.6.4 Part D — (Optional) Quarto inclusion\nAdd this to your Quarto report:\n## Evaluation Protocol (Leakage‑Aware)\n\n::: {#6ef59865 .cell execution_count=1}\n````` {.python .cell-code}\nfrom pathlib import Path\nprint(Path(\"reports/eval_protocol.md\").read_text())\n````` :::",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Session 17 — Feature Timing, Biases & Leakage</span>"
    ]
  },
  {
    "objectID": "lec17.html#instructor-checklist-before-class",
    "href": "lec17.html#instructor-checklist-before-class",
    "title": "17  Session 17 — Feature Timing, Biases & Leakage",
    "section": "17.7 Instructor checklist (before class)",
    "text": "17.7 Instructor checklist (before class)\n\nEnsure returns.parquet and features_v1.parquet exist or fallback works.\n\nIntentionally create a leaked feature (e.g., lag1 = log_return.shift(-1)) on your copy to show tests failing, then fix.\n\nDecide an anchor date policy for universe freeze; today’s lab uses first split’s train end.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Session 17 — Feature Timing, Biases & Leakage</span>"
    ]
  },
  {
    "objectID": "lec17.html#emphasize-while-teaching",
    "href": "lec17.html#emphasize-while-teaching",
    "title": "17  Session 17 — Feature Timing, Biases & Leakage",
    "section": "17.8 Emphasize while teaching",
    "text": "17.8 Emphasize while teaching\n\nDefine labels first, then prove features are causal (≤ t).\n\nFreezing the universe is small effort with big impact on credibility.\n\nTests are your guardrails—if they go red, don’t relax them; fix the pipeline.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Session 17 — Feature Timing, Biases & Leakage</span>"
    ]
  },
  {
    "objectID": "lec17.html#grading-passrevise",
    "href": "lec17.html#grading-passrevise",
    "title": "17  Session 17 — Feature Timing, Biases & Leakage",
    "section": "17.9 Grading (pass/revise)",
    "text": "17.9 Grading (pass/revise)\n\ndata/static/universe_YYYY-MM-DD.csv created; features_v1_static.parquet filtered by it.\n\nLeakage tests present and green on the clean pipeline; red if you inject a future‑peek.\n\nreports/eval_protocol.md exists and includes student commentary.\n\nmake leakage-audit runs without errors.\n\nYou now have a credibility layer on top of your data pipeline—ready to analyze regimes and calibration next (Session 18). ```",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Session 17 — Feature Timing, Biases & Leakage</span>"
    ]
  },
  {
    "objectID": "lec18.html",
    "href": "lec18.html",
    "title": "18  Session 18 — Walk‑forward + Regime Analysi",
    "section": "",
    "text": "18.1 Session 18 — Walk‑forward + Regime Analysis (75 min)\nBelow is a complete lecture package for Session 18 — Walk‑forward + Regime Analysis (75 minutes). It includes a timed agenda, slide talking points, a Colab‑friendly in‑class lab with copy‑paste code, and homework with copy‑paste code. You’ll add volatility regimes to your rolling‑origin evaluation (with embargo), compute metrics by regime, and produce calibration plots that reveal where baselines over/under‑predict.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Session 18 — Walk‑forward + Regime Analysi</span>"
    ]
  },
  {
    "objectID": "lec18.html#session-18-walkforward-regime-analysis-75-min",
    "href": "lec18.html#session-18-walkforward-regime-analysis-75-min",
    "title": "18  Session 18 — Walk‑forward + Regime Analysi",
    "section": "",
    "text": "18.1.1 Learning goals\nBy the end of class, students can:\n\nUse embargoed rolling‑origin splits (Session 15) and apply a static universe (Session 17) consistently.\nConstruct volatility regimes (low/med/high) from rolling volatility computed causally (≤ t), and set regime thresholds using training‑only data per split.\nEvaluate MAE, sMAPE, MASE by regime, with macro and micro aggregation.\nMake calibration plots (binned predicted vs. realized returns) by regime and interpret them.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Session 18 — Walk‑forward + Regime Analysi</span>"
    ]
  },
  {
    "objectID": "lec18.html#agenda-75-min",
    "href": "lec18.html#agenda-75-min",
    "title": "18  Session 18 — Walk‑forward + Regime Analysi",
    "section": "18.2 Agenda (75 min)",
    "text": "18.2 Agenda (75 min)\n\n(10 min) Slides: walk‑forward recap (expanding vs sliding), embargo; regime intuition\n(10 min) Slides: defining regimes (rolling std), training‑only thresholds, leakage pitfalls\n(35 min) In‑class lab: add regime labels (train‑only quantiles) → evaluate naive & linear‑lags by regime → calibration plots\n(10 min) Wrap‑up + homework brief\n(10 min) Buffer / Q&A",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Session 18 — Walk‑forward + Regime Analysi</span>"
    ]
  },
  {
    "objectID": "lec18.html#slide-talking-points-paste-into-your-deck",
    "href": "lec18.html#slide-talking-points-paste-into-your-deck",
    "title": "18  Session 18 — Walk‑forward + Regime Analysi",
    "section": "18.3 Slide talking points (paste into your deck)",
    "text": "18.3 Slide talking points (paste into your deck)\n\n18.3.1 Why regime analysis?\n\nModel error is not uniform. Many models fail during high‑volatility periods.\nReporting one global metric hides when/where models break.\nRegime‑aware metrics guide feature/model design and risk controls.\n\n\n\n18.3.2 Splits & embargo refresher\n\nRolling‑origin, expanding: train grows, validation moves forward.\nEmbargo: gap (e.g., 5 business days) between train end and val start to reduce adjacency leakage.\n\n\n\n18.3.3 Defining volatility regimes (avoid leakage)\n\nUse rolling standard deviation of returns (e.g., roll_std_20) computed up to and including t.\nThresholds: choose quantiles (e.g., 33% and 66%) on TRAIN ONLY for each split; label both train & val using those fixed thresholds.\nCategories: low, med, high. Treat labels as categorical dtypes.\n\n\n\n18.3.4 Metrics & calibration by regime\n\nCompute MAE, sMAPE, MASE within each regime. Aggregate macro/micro.\nCalibration (point forecasts): bin predictions into deciles; plot mean predicted vs. mean realized per bin.\n\nPerfect calibration ⇒ points on the 45° line.\nPlot one figure overall and one per regime.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Session 18 — Walk‑forward + Regime Analysi</span>"
    ]
  },
  {
    "objectID": "lec18.html#inclass-lab-35-min-colabfriendly",
    "href": "lec18.html#inclass-lab-35-min-colabfriendly",
    "title": "18  Session 18 — Walk‑forward + Regime Analysi",
    "section": "18.4 In‑class lab (35 min, Colab‑friendly)",
    "text": "18.4 In‑class lab (35 min, Colab‑friendly)\n\nRun each block as its own cell. Adjust REPO_NAME to your repo name.\n\n\n18.4.1 0) Setup & load (with safe fallbacks)\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"   # &lt;- change if needed\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, numpy as np, pandas as pd, json\nfrom pathlib import Path\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/raw\",\"data/processed\",\"data/static\",\"reports\",\"scripts\",\"tests\",\"docs/figs\"]:\n    Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\n# Load returns; synthesize if missing\nrpath = Path(\"data/processed/returns.parquet\")\nif rpath.exists():\n    returns = pd.read_parquet(rpath)\nelse:\n    rng = np.random.default_rng(0)\n    dates = pd.bdate_range(\"2022-01-03\", periods=360)\n    frames=[]\n    for t in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\"]:\n        eps = rng.normal(0,0.012,size=len(dates)).astype(\"float32\")\n        adj = 100*np.exp(np.cumsum(eps))\n        df = pd.DataFrame({\n            \"date\": dates, \"ticker\": t,\n            \"adj_close\": adj.astype(\"float32\"),\n            \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n        })\n        df[\"r_1d\"] = df[\"log_return\"].shift(-1)\n        df[\"weekday\"] = df[\"date\"].dt.weekday.astype(\"int8\")\n        df[\"month\"]   = df[\"date\"].dt.month.astype(\"int8\")\n        frames.append(df)\n    returns = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)\n    returns[\"ticker\"] = returns[\"ticker\"].astype(\"category\")\n    returns.to_parquet(rpath, index=False)\n\n# Load features or generate minimal set with rolling std (causal)\nfpath = Path(\"data/processed/features_v1.parquet\")\nif fpath.exists():\n    feats = pd.read_parquet(fpath)\n    if \"roll_std_20\" not in feats.columns:\n        # ensure we have rolling volatility\n        feats = feats.sort_values([\"ticker\",\"date\"])\n        feats[\"roll_std_20\"] = feats.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)\nelse:\n    feats = returns.sort_values([\"ticker\",\"date\"]).copy()\n    for k in [1,2,3]:\n        feats[f\"lag{k}\"] = feats.groupby(\"ticker\")[\"log_return\"].shift(k)\n    feats[\"roll_std_20\"] = feats.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)\n\n# If static universe exists from Session 17, apply it\nuniv_files = sorted(Path(\"data/static\").glob(\"universe_*.csv\"))\nif univ_files:\n    univ = pd.read_csv(univ_files[-1])[\"ticker\"].astype(str)\n    feats = feats[feats[\"ticker\"].astype(str).isin(set(univ))]\n    returns = returns[returns[\"ticker\"].astype(str).isin(set(univ))]\n\n# Harmonize types & sort\nfor df in (returns, feats):\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    df[\"ticker\"] = df[\"ticker\"].astype(\"category\")\nfeats = feats.dropna(subset=[\"log_return\"]).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nreturns = returns.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nfeats.head(3)\n\n\n18.4.2 1) Rolling‑origin splits (expanding) with embargo\nimport numpy as np, pandas as pd\n\ndef make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    splits=[]; i=train_min-1; n=len(u)\n    while True:\n        if i&gt;=n: break\n        a,b = u[0], u[i]\n        vs = i + embargo + 1\n        ve = vs + val_size - 1\n        if ve&gt;=n: break\n        splits.append((a,b,u[vs],u[ve]))\n        i += step\n    return splits\n\nsplits = make_rolling_origin_splits(feats[\"date\"], train_min=252, val_size=63, step=63, embargo=5)\nprint(\"Num splits:\", len(splits))\nsplits[:2]\n\n\n18.4.3 2) Regime thresholds from training‑only (quantiles of rolling vol)\ndef regime_thresholds(train_df, vol_col=\"roll_std_20\", q_low=0.33, q_high=0.66):\n    v = train_df[vol_col].dropna().to_numpy()\n    if len(v) &lt; 100:  # defensive: small train\n        q_low, q_high = 0.4, 0.8\n    return float(np.quantile(v, q_low)), float(np.quantile(v, q_high))\n\ndef label_regime(df, vol_col, lo, hi):\n    # low: &lt;= lo, high: &gt;= hi, else med; NaNs -&gt; 'unknown'\n    out = df.copy()\n    vc = out[vol_col]\n    regime = pd.Series(pd.Categorical([\"unknown\"]*len(out), categories=[\"low\",\"med\",\"high\",\"unknown\"]), index=out.index)\n    regime[(vc.notna()) & (vc &lt;= lo)] = \"low\"\n    regime[(vc.notna()) & (vc &gt; lo) & (vc &lt; hi)] = \"med\"\n    regime[(vc.notna()) & (vc &gt;= hi)] = \"high\"\n    out[\"regime\"] = regime.astype(\"category\")\n    return out\n\n# Demonstrate on first split in class\na,b,c,d = splits[0]\ntr = feats[(feats[\"date\"]&gt;=a) & (feats[\"date\"]&lt;=b)]\nva = feats[(feats[\"date\"]&gt;=c) & (feats[\"date\"]&lt;=d)]\nlo, hi = regime_thresholds(tr, \"roll_std_20\", 0.33, 0.66)\ntr_lab = label_regime(tr, \"roll_std_20\", lo, hi)\nva_lab = label_regime(va, \"roll_std_20\", lo, hi)\nprint({\"lo\": lo, \"hi\": hi}, tr_lab[\"regime\"].value_counts().to_dict(), va_lab[\"regime\"].value_counts().to_dict())\n\n\n18.4.4 3) Baseline predictions (naive & linear‑lags per ticker, fit on TRAIN only)\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\n# features we will use for linear baseline\nXCOLS = [c for c in [\"lag1\",\"lag2\",\"lag3\"] if c in feats.columns]\nif not XCOLS:\n    # create lags on the fly (causal)\n    feats = feats.sort_values([\"ticker\",\"date\"]).copy()\n    for k in [1,2,3]:\n        feats[f\"lag{k}\"] = feats.groupby(\"ticker\")[\"log_return\"].shift(k)\n    XCOLS = [\"lag1\",\"lag2\",\"lag3\"]\n\ndef fit_predict_lin_per_ticker(train_df, val_df):\n    preds=[]\n    for tkr, trk in train_df.groupby(\"ticker\"):\n        vak = val_df[val_df[\"ticker\"]==tkr]\n        if len(trk)==0 or len(vak)==0: continue\n        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LinearRegression())])\n        pipe.fit(trk[XCOLS].dropna().values, trk.dropna(subset=XCOLS)[\"r_1d\"].values)\n        yhat = pipe.predict(vak[XCOLS].fillna(0).values)\n        out = vak[[\"date\",\"ticker\",\"r_1d\",\"log_return\",\"regime\"]].copy()\n        out[\"yhat_lin\"] = yhat.astype(\"float32\")\n        preds.append(out)\n    return pd.concat(preds, ignore_index=True) if preds else pd.DataFrame()\n\ndef add_naive_preds(df):\n    out = df.copy()\n    out[\"yhat_naive\"] = out[\"log_return\"]  # r_{t+1} ~ log_return_t\n    return out\n\ntr_lab2 = add_naive_preds(tr_lab)\nva_lab2 = add_naive_preds(va_lab)\nval_lin = fit_predict_lin_per_ticker(tr_lab2, va_lab2)\nval = va_lab2.merge(val_lin[[\"date\",\"ticker\",\"yhat_lin\"]], on=[\"date\",\"ticker\"], how=\"left\")\nval.head(3)\n\n\n18.4.5 4) Metrics by regime (MAE, sMAPE, MASE; macro & micro)\ndef mae(y, yhat): \n    y = np.asarray(y); yhat = np.asarray(yhat)\n    return float(np.mean(np.abs(y - yhat)))\n\ndef smape(y,yhat,eps=1e-8):\n    y = np.asarray(y); yhat = np.asarray(yhat)\n    return float(np.mean(2.0*np.abs(y-yhat)/(np.abs(y)+np.abs(yhat)+eps)))\n\ndef mase(y_true, y_pred, y_train_true, y_train_naive):\n    scale = mae(y_train_true, y_train_naive) + 1e-12\n    return float(mae(y_true,y_pred)/scale)\n\ndef per_regime_metrics(val_df, train_df, pred_col):\n    rows=[]\n    for reg, g in val_df.groupby(\"regime\"):\n        if reg == \"unknown\" or len(g)==0: \n            continue\n        # build per-ticker MASE scales from TRAIN\n        per_t = []\n        for tkr, gv in g.groupby(\"ticker\"):\n            gt = train_df[train_df[\"ticker\"]==tkr].dropna(subset=[\"r_1d\"])\n            if len(gt)==0: continue\n            m = {\n                \"ticker\": tkr,\n                \"n\": int(gv[\"r_1d\"].notna().sum()),\n                \"mae\": mae(gv[\"r_1d\"], gv[pred_col]),\n                \"smape\": smape(gv[\"r_1d\"], gv[pred_col]),\n                \"mase\": mase(gv[\"r_1d\"], gv[pred_col], gt[\"r_1d\"], gt[\"log_return\"]),\n                \"regime\": reg\n            }\n            per_t.append(m)\n        per_t = pd.DataFrame(per_t)\n        if per_t.empty: \n            continue\n        # macro (mean of per-ticker)\n        macro = per_t[[\"mae\",\"smape\",\"mase\"]].mean().to_dict()\n        # micro (weighted by n)\n        w = per_t[\"n\"].to_numpy()\n        micro = {\n            \"micro_mae\": float(np.average(per_t[\"mae\"], weights=w)),\n            \"micro_smape\": float(np.average(per_t[\"smape\"], weights=w)),\n            \"micro_mase\": float(np.average(per_t[\"mase\"], weights=w)),\n        }\n        rows.append({\"regime\":reg, **{f\"macro_{k}\":float(v) for k,v in macro.items()}, **micro})\n    return pd.DataFrame(rows)\n\nmet_naive = per_regime_metrics(val, tr_lab2, \"yhat_naive\")\nmet_lin   = per_regime_metrics(val.dropna(subset=[\"yhat_lin\"]), tr_lab2, \"yhat_lin\")\nprint(\"NAIVE by regime:\\n\", met_naive)\nprint(\"\\nLIN-LAGS by regime:\\n\", met_lin)\n# Save\npd.concat([\n    met_naive.assign(model=\"naive\"),\n    met_lin.assign(model=\"lin_lags\")\n], ignore_index=True).to_csv(\"reports/regime_metrics_split1.csv\", index=False)\n\n\n18.4.6 5) Calibration plots overall and by regime (binned)\nimport matplotlib.pyplot as plt\nimport numpy as np, pandas as pd, pathlib\n\ndef calibration_by_bins(df, pred_col, y_col=\"r_1d\", n_bins=10):\n    d = df.dropna(subset=[pred_col, y_col]).copy()\n    d[\"bin\"] = pd.qcut(d[pred_col], q=n_bins, duplicates=\"drop\")\n    grp = d.groupby(\"bin\").agg(\n        mean_pred=(pred_col, \"mean\"),\n        mean_true=(y_col, \"mean\"),\n        count=(y_col, \"size\")\n    ).reset_index()\n    return grp\n\n# Overall calibration (lin_lags) on validation slice\ncal_overall = calibration_by_bins(val.dropna(subset=[\"yhat_lin\"]), \"yhat_lin\", \"r_1d\", n_bins=10)\n\nplt.figure(figsize=(5,4))\nplt.plot(cal_overall[\"mean_pred\"], cal_overall[\"mean_true\"], marker=\"o\")\nlim = max(abs(cal_overall[\"mean_pred\"]).max(), abs(cal_overall[\"mean_true\"]).max())\nplt.plot([-lim, lim], [-lim, lim], linestyle=\"--\")\nplt.xlabel(\"Mean predicted (bin)\"); plt.ylabel(\"Mean realized (bin)\")\nplt.title(\"Calibration (overall) — lin_lags\")\nplt.tight_layout()\nplt.savefig(\"docs/figs/calibration_overall_lin.png\", dpi=160)\n\"Saved docs/figs/calibration_overall_lin.png\"\n\n# By regime\nplt.figure(figsize=(6.5,4.5))\nfor i, reg in enumerate([\"low\",\"med\",\"high\"], start=1):\n    g = val[(val[\"regime\"]==reg) & (val[\"yhat_lin\"].notna())]\n    if len(g) &lt; 50: \n        continue\n    cal = calibration_by_bins(g, \"yhat_lin\", \"r_1d\", n_bins=6)\n    plt.plot(cal[\"mean_pred\"], cal[\"mean_true\"], marker=\"o\", label=reg)\nlim = 0.02  # small returns\nplt.plot([-lim, lim], [-lim, lim], linestyle=\"--\")\nplt.xlabel(\"Mean predicted (bin)\"); plt.ylabel(\"Mean realized (bin)\")\nplt.title(\"Calibration by regime — lin_lags\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"docs/figs/calibration_by_regime_lin.png\", dpi=160)\n\"Saved docs/figs/calibration_by_regime_lin.png\"",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Session 18 — Walk‑forward + Regime Analysi</span>"
    ]
  },
  {
    "objectID": "lec18.html#wrapup-10-min-key-points-to-emphasize",
    "href": "lec18.html#wrapup-10-min-key-points-to-emphasize",
    "title": "18  Session 18 — Walk‑forward + Regime Analysi",
    "section": "18.5 Wrap‑up (10 min) — key points to emphasize",
    "text": "18.5 Wrap‑up (10 min) — key points to emphasize\n\nRegime thresholds must be set on TRAIN ONLY each split to avoid leakage.\nReport by‑regime metrics alongside overall metrics; show macro & micro.\nCalibration plots (binned predicted vs. realized) quickly show systematic bias; compare regimes.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Session 18 — Walk‑forward + Regime Analysi</span>"
    ]
  },
  {
    "objectID": "lec18.html#homework-due-before-session-19",
    "href": "lec18.html#homework-due-before-session-19",
    "title": "18  Session 18 — Walk‑forward + Regime Analysi",
    "section": "18.6 Homework (due before Session 19)",
    "text": "18.6 Homework (due before Session 19)\nGoal: Produce a full regime‑aware evaluation across all splits for naive and linear‑lags models and include the figures in your Quarto report.\n\n18.6.1 A. Script: scripts/regime_eval.py — run across all splits\n#!/usr/bin/env python\nfrom __future__ import annotations\nimport argparse, json, numpy as np, pandas as pd\nfrom pathlib import Path\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\ndef make_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    splits=[]; i=train_min-1; n=len(u)\n    while True:\n        if i&gt;=n: break\n        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve&gt;=n: break\n        splits.append((a,b,u[vs],u[ve])); i+=step\n    return splits\n\ndef regime_thresholds(train_df, vol_col=\"roll_std_20\", q_low=0.33, q_high=0.66):\n    v = train_df[vol_col].dropna().to_numpy()\n    if len(v) &lt; 100:\n        q_low, q_high = 0.4, 0.8\n    return float(np.quantile(v, q_low)), float(np.quantile(v, q_high))\n\ndef label_regime(df, vol_col, lo, hi):\n    out = df.copy()\n    vc = out[vol_col]\n    reg = pd.Series(pd.Categorical([\"unknown\"]*len(out), categories=[\"low\",\"med\",\"high\",\"unknown\"]), index=out.index)\n    reg[(vc.notna()) & (vc &lt;= lo)] = \"low\"\n    reg[(vc.notna()) & (vc &gt; lo) & (vc &lt; hi)] = \"med\"\n    reg[(vc.notna()) & (vc &gt;= hi)] = \"high\"\n    out[\"regime\"] = reg.astype(\"category\")\n    return out\n\ndef add_naive(df):\n    out = df.copy()\n    out[\"yhat_naive\"] = out[\"log_return\"]\n    return out\n\ndef fit_lin(tr, va, xcols):\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.linear_model import LinearRegression\n    preds=[]\n    for tkr, trk in tr.groupby(\"ticker\"):\n        vak = va[va[\"ticker\"]==tkr]\n        if len(trk)==0 or len(vak)==0: continue\n        Xtr = trk.dropna(subset=xcols); \n        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LinearRegression())])\n        pipe.fit(Xtr[xcols].values, Xtr[\"r_1d\"].values)\n        yhat = pipe.predict(vak[xcols].fillna(0).values)\n        out = vak[[\"date\",\"ticker\",\"r_1d\",\"log_return\",\"regime\"]].copy()\n        out[\"yhat_lin\"] = yhat\n        preds.append(out)\n    return pd.concat(preds, ignore_index=True) if preds else pd.DataFrame()\n\ndef mae(y, yhat): y=np.asarray(y); yhat=np.asarray(yhat); return float(np.mean(np.abs(y-yhat)))\ndef smape(y,yhat,eps=1e-8):\n    y=np.asarray(y); yhat=np.asarray(yhat); return float(np.mean(2*np.abs(y-yhat)/(np.abs(y)+np.abs(yhat)+eps)))\ndef mase(y_true, y_pred, y_train_true, y_train_naive):\n    return float(mae(y_true, y_pred)/(mae(y_train_true, y_train_naive)+1e-12))\n\ndef per_regime_metrics(val_df, train_df, pred_col):\n    rows=[]\n    for reg, g in val_df.groupby(\"regime\"):\n        if reg==\"unknown\" or len(g)==0: continue\n        per=[]\n        for tkr, gv in g.groupby(\"ticker\"):\n            gt = train_df[train_df[\"ticker\"]==tkr].dropna(subset=[\"r_1d\"])\n            if len(gt)==0: continue\n            per.append({\"ticker\":tkr,\"n\":int(gv[\"r_1d\"].notna().sum()),\n                        \"mae\": mae(gv[\"r_1d\"], gv[pred_col]),\n                        \"smape\": smape(gv[\"r_1d\"], gv[pred_col]),\n                        \"mase\": mase(gv[\"r_1d\"], gv[pred_col], gt[\"r_1d\"], gt[\"log_return\"]),\n                        \"regime\": reg})\n        pt = pd.DataFrame(per)\n        if pt.empty: continue\n        macro = pt[[\"mae\",\"smape\",\"mase\"]].mean().to_dict()\n        w = pt[\"n\"].to_numpy()\n        micro = {\"micro_mae\": float(np.average(pt[\"mae\"], weights=w)),\n                 \"micro_smape\": float(np.average(pt[\"smape\"], weights=w)),\n                 \"micro_mase\": float(np.average(pt[\"mase\"], weights=w))}\n        rows.append({\"regime\":reg, **{f\"macro_{k}\":float(v) for k,v in macro.items()}, **micro})\n    return pd.DataFrame(rows)\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--features\", default=\"data/processed/features_v1.parquet\")\n    ap.add_argument(\"--train-min\", type=int, default=252)\n    ap.add_argument(\"--val-size\", type=int, default=63)\n    ap.add_argument(\"--step\", type=int, default=63)\n    ap.add_argument(\"--embargo\", type=int, default=5)\n    ap.add_argument(\"--vol-col\", default=\"roll_std_20\")\n    ap.add_argument(\"--xcols\", nargs=\"+\", default=[\"lag1\",\"lag2\",\"lag3\"])\n    ap.add_argument(\"--out-summary\", default=\"reports/regime_summary.csv\")\n    args = ap.parse_args()\n\n    df = pd.read_parquet(args.features).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    # Ensure vol col exists\n    if args.vol_col not in df.columns:\n        df[args.vol_col] = df.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)\n\n    # Build lags if missing\n    for k in [1,2,3]:\n        col = f\"lag{k}\"\n        if col not in df.columns:\n            df[col] = df.groupby(\"ticker\")[\"log_return\"].shift(k)\n\n    splits = make_splits(df[\"date\"], args.train_min, args.val_size, args.step, args.embargo)\n    Path(\"reports\").mkdir(parents=True, exist_ok=True)\n    thresh_rec = {}\n\n    rows=[]\n    for sid,(a,b,c,d) in enumerate(splits, start=1):\n        tr = df[(df[\"date\"]&gt;=a)&(df[\"date\"]&lt;=b)]\n        va = df[(df[\"date\"]&gt;=c)&(df[\"date\"]&lt;=d)]\n        lo, hi = regime_thresholds(tr, args.vol_col)\n        thresh_rec[sid] = {\"lo\":lo, \"hi\":hi, \"train_range\":f\"{a.date()}→{b.date()}\"}\n        trL = label_regime(tr, args.vol_col, lo, hi)\n        vaL = label_regime(va, args.vol_col, lo, hi)\n\n        # predictions\n        trN, vaN = add_naive(trL), add_naive(vaL)\n        val_lin = fit_lin(trN, vaN, args.xcols)\n        vaN = vaN.merge(val_lin[[\"date\",\"ticker\",\"yhat_lin\"]], on=[\"date\",\"ticker\"], how=\"left\")\n\n        # metrics\n        m_naive = per_regime_metrics(vaN, trN, \"yhat_naive\").assign(split=sid, model=\"naive\")\n        m_lin   = per_regime_metrics(vaN.dropna(subset=[\"yhat_lin\"]), trN, \"yhat_lin\").assign(split=sid, model=\"lin_lags\")\n\n        out = pd.concat([m_naive, m_lin], ignore_index=True)\n        out.to_csv(f\"reports/regime_metrics_split{sid}.csv\", index=False)\n        rows.append(out)\n\n    pd.concat(rows, ignore_index=True).to_csv(args.out_summary, index=False)\n    Path(\"reports/regime_thresholds.json\").write_text(json.dumps(thresh_rec, indent=2))\n    print(\"Wrote\", args.out_summary, \"and per-split CSVs; thresholds saved to reports/regime_thresholds.json\")\n\nif __name__ == \"__main__\":\n    main()\nRun:\n%%bash\nchmod +x scripts/regime_eval.py\npython scripts/regime_eval.py\n\n\n18.6.2 B. Plot summary figures for your report\nimport pandas as pd, matplotlib.pyplot as plt, pathlib\npathlib.Path(\"docs/figs\").mkdir(parents=True, exist_ok=True)\n\ndf = pd.read_csv(\"reports/regime_summary.csv\")\n# Micro MAE by regime per model\npivot = df.pivot_table(index=[\"split\",\"regime\"], columns=\"model\", values=\"micro_mae\")\nplt.figure(figsize=(6,4))\nfor model in pivot.columns:\n    plt.plot(pivot.xs(\"low\", level=\"regime\").index, pivot.xs(\"low\", level=\"regime\")[model], marker=\"o\", label=f\"{model} — low\")\n    plt.plot(pivot.xs(\"high\", level=\"regime\").index, pivot.xs(\"high\", level=\"regime\")[model], marker=\"s\", label=f\"{model} — high\")\nplt.xlabel(\"Split\"); plt.ylabel(\"Micro MAE\")\nplt.title(\"Micro MAE by regime (low vs high)\")\nplt.legend(); plt.tight_layout()\nplt.savefig(\"docs/figs/regime_micro_mae.png\", dpi=160)\n\"Saved docs/figs/regime_micro_mae.png\"\n\n\n18.6.3 C. Add to Quarto report\nIn reports/eda.qmd (or a new reports/regime.qmd), include:\n## Regime‑aware Results\n\n::: {#8f00d8aa .cell execution_count=1}\n````` {.python .cell-code}\nimport pandas as pd\ndf = pd.read_csv(\"reports/regime_summary.csv\")\ndf.sort_values([\"split\",\"model\",\"regime\"]).head(12)\n\n\n\n\nRender:\n```bash\nquarto render reports/eda.qmd\n\n\n18.6.4 D. Quick test to protect train‑only thresholds\n# tests/test_regime_thresholds.py\nimport json, pandas as pd\n\ndef test_thresholds_exist_and_train_range():\n    data = json.load(open(\"reports/regime_thresholds.json\"))\n    assert len(data) &gt;= 1\n    # basic sanity: low &lt; high\n    for sid, rec in data.items():\n        assert float(rec[\"lo\"]) &lt; float(rec[\"hi\"])\n        assert \"→\" in rec[\"train_range\"]\n:::\nRun:\n%%bash\npytest -q -k regime_thresholds",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Session 18 — Walk‑forward + Regime Analysi</span>"
    ]
  },
  {
    "objectID": "lec18.html#instructor-checklist-before-class",
    "href": "lec18.html#instructor-checklist-before-class",
    "title": "18  Session 18 — Walk‑forward + Regime Analysi",
    "section": "18.7 Instructor checklist (before class)",
    "text": "18.7 Instructor checklist (before class)\n\nEnsure features_v1.parquet contains roll_std_20 (or let lab compute it).\nKeep the in‑class run to the first split to finish inside the time box; homework runs all splits.\nHave a slide explaining why thresholds must not be learned on validation/test.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Session 18 — Walk‑forward + Regime Analysi</span>"
    ]
  },
  {
    "objectID": "lec18.html#emphasize-while-teaching",
    "href": "lec18.html#emphasize-while-teaching",
    "title": "18  Session 18 — Walk‑forward + Regime Analysi",
    "section": "18.8 Emphasize while teaching",
    "text": "18.8 Emphasize while teaching\n\nRegime thresholds are part of your training‑time state—store them (JSON) and do not recompute from validation/test.\nReport both macro and micro metrics by regime so small/large tickers don’t dominate silently.\nUse calibration plots to diagnose systematic bias; e.g., under‑prediction in high vol.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Session 18 — Walk‑forward + Regime Analysi</span>"
    ]
  },
  {
    "objectID": "lec18.html#grading-passrevise",
    "href": "lec18.html#grading-passrevise",
    "title": "18  Session 18 — Walk‑forward + Regime Analysi",
    "section": "18.9 Grading (pass/revise)",
    "text": "18.9 Grading (pass/revise)\n\nscripts/regime_eval.py runs and writes reports/regime_summary.csv + per‑split CSVs.\nThresholds saved to reports/regime_thresholds.json.\nFigures exist under docs/figs/ and are embedded in the report.\nShort tests pass; report includes a paragraph discussing results by regime.\n\nYou now have a regime‑aware evaluation layered on your rolling‑origin pipeline—perfect preparation for Session 19, where you’ll implement PyTorch datasets and a minimal training loop.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Session 18 — Walk‑forward + Regime Analysi</span>"
    ]
  },
  {
    "objectID": "lec19.html",
    "href": "lec19.html",
    "title": "19  Session 19 — Tensors, Datasets, Training Loop",
    "section": "",
    "text": "19.1 Session 19 — Tensors, Datasets, Training Loop (75 min)\nBelow is a complete lecture package for Session 19 — Tensors, Datasets, Training Loop (75 minutes). It includes a timed agenda, slide talking points, a Colab‑friendly in‑class lab with copy‑paste code, and homework with copy‑paste code. You will build a windowed multi‑ticker dataset for next‑day return prediction, write a minimal PyTorch training loop with early stopping, and save/load checkpoints. We’ll keep it CPU‑friendly and optionally accelerate with GPU/Mixed Precision on Colab.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Session 19 — Tensors, Datasets, Training Loop</span>"
    ]
  },
  {
    "objectID": "lec19.html#session-19-tensors-datasets-training-loop-75-min",
    "href": "lec19.html#session-19-tensors-datasets-training-loop-75-min",
    "title": "19  Session 19 — Tensors, Datasets, Training Loop",
    "section": "",
    "text": "19.1.1 Learning goals\nBy the end of class, students can:\n\nCreate a windowed sequence dataset across multiple tickers with shape (B, T, F) → predict r_1d at time t+1.\nUse Dataset/DataLoader correctly: pin memory, worker seeding, and efficient slicing.\nWrite a minimal training loop with early stopping, AMP (mixed precision on GPU), and checkpoint save/load.\nProduce a tidy validation metrics CSV to compare later models.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Session 19 — Tensors, Datasets, Training Loop</span>"
    ]
  },
  {
    "objectID": "lec19.html#agenda-75-min",
    "href": "lec19.html#agenda-75-min",
    "title": "19  Session 19 — Tensors, Datasets, Training Loop",
    "section": "19.2 Agenda (75 min)",
    "text": "19.2 Agenda (75 min)\n\n(10 min) Slides: tensors & batching; Dataset/DataLoader; pinning memory; reproducibility with seeds\n(10 min) Slides: training loop anatomy; early stopping; AMP; checkpoints\n(35 min) In‑class lab: build WindowedDataset → DataLoaders → tiny GRU regressor → train w/ early stopping → save best checkpoint; evaluate\n(10 min) Wrap‑up + homework brief\n(10 min) Buffer",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Session 19 — Tensors, Datasets, Training Loop</span>"
    ]
  },
  {
    "objectID": "lec19.html#slides-talking-points-drop-into-your-deck",
    "href": "lec19.html#slides-talking-points-drop-into-your-deck",
    "title": "19  Session 19 — Tensors, Datasets, Training Loop",
    "section": "19.3 Slides / talking points (drop into your deck)",
    "text": "19.3 Slides / talking points (drop into your deck)\n\n19.3.1 Tensors & batching\n\nTensors are N‑D arrays on CPU or GPU (.to(device)); keep everything float32 unless using AMP.\nFor sequences: batch × time × features ⇒ (B, T, F). Predict a scalar per sequence end (r_1d at time t+1).\n\n\n\n19.3.2 Dataset/DataLoader patterns\n\nPrecompute an index of windows: for each ticker, sliding windows end at i with context T; target is r_1d[i].\nDataLoader tips:\n\npin_memory=True and non_blocking=True on .to(device) speed H2D copies (when using GPU).\nSeed workers for reproducibility; keep num_workers=2 (Colab stable).\n\n\n\n\n19.3.3 Seeds & determinism\n\nSet python, numpy, torch seeds; disable CuDNN benchmarking for reproducibility; prefer small batch sizes that fit CPU/GPU.\n\n\n\n19.3.4 Training loop w/ early stopping\n\nLoop: train_step (model in train()), val_step (model in eval() + no_grad()).\nTrack best val loss; stop after patience epochs without improvement.\nSave checkpoint: state_dict, optimizer state, epoch, metrics. Load with load_state_dict.\n\n\n\n19.3.5 AMP & checkpoints\n\nOn CUDA, wrap forward in torch.cuda.amp.autocast() and use GradScaler to scale loss.\nSave the best checkpoint to models/…pt; log a CSV under reports/.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Session 19 — Tensors, Datasets, Training Loop</span>"
    ]
  },
  {
    "objectID": "lec19.html#inclass-lab-35-min-colabfriendly",
    "href": "lec19.html#inclass-lab-35-min-colabfriendly",
    "title": "19  Session 19 — Tensors, Datasets, Training Loop",
    "section": "19.4 In‑class lab (35 min, Colab‑friendly)",
    "text": "19.4 In‑class lab (35 min, Colab‑friendly)\n\nRun each block as its own separate cell in Colab. Replace REPO_NAME as needed.\n\n\n19.4.1 0) Setup, mount, and check device\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"   # &lt;-- change to your repo\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, sys\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/processed\",\"models\",\"reports\",\"scripts\",\"tests\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\nimport torch, platform\nprint(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available(), \"| Python:\", sys.version.split()[0], \"| OS:\", platform.system())\n\n\n19.4.2 1) Load features and pick columns (with fallbacks)\nimport pandas as pd, numpy as np\nfrom pathlib import Path\n\n# Prefer static universe file if present (from Session 17)\nf_static = Path(\"data/processed/features_v1_static.parquet\")\nf_base   = Path(\"data/processed/features_v1.parquet\")\n\nif f_static.exists():\n    df = pd.read_parquet(f_static)\nelif f_base.exists():\n    df = pd.read_parquet(f_base)\nelse:\n    # Minimal fallback from returns\n    rpath = Path(\"data/processed/returns.parquet\")\n    if not rpath.exists():\n        # synthesize small dataset\n        rng = np.random.default_rng(0)\n        dates = pd.bdate_range(\"2022-01-03\", periods=320)\n        frames=[]\n        for t in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\"]:\n            eps = rng.normal(0, 0.012, size=len(dates)).astype(\"float32\")\n            adj = 100*np.exp(np.cumsum(eps))\n            g = pd.DataFrame({\n                \"date\": dates, \"ticker\": t,\n                \"adj_close\": adj.astype(\"float32\"),\n                \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n            })\n            g[\"r_1d\"] = g[\"log_return\"].shift(-1)\n            frames.append(g)\n        df = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)\n        df[\"ticker\"] = df[\"ticker\"].astype(\"category\")\n    else:\n        df = pd.read_parquet(rpath)\n        df = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n        # add minimal lags\n        for k in [1,2,3]:\n            df[f\"lag{k}\"] = df.groupby(\"ticker\")[\"log_return\"].shift(k)\n        df = df.dropna().reset_index(drop=True)\n\n# Ensure minimal features exist\ncand_feats = [\"log_return\",\"lag1\",\"lag2\",\"lag3\",\"zscore_20\",\"roll_std_20\"]\nFEATS = [c for c in cand_feats if c in df.columns]\nassert \"r_1d\" in df.columns, \"Label r_1d missing; rebuild returns/features pipeline.\"\nassert \"log_return\" in df.columns, \"log_return missing.\"\n\n# Keep a small subset of tickers for speed (5–10 tickers)\nsubset = df[\"ticker\"].astype(str).unique().tolist()[:8]\ndf = df[df[\"ticker\"].astype(str).isin(subset)].copy()\n\n# Harmonize types & sort\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\ndf[\"ticker\"] = df[\"ticker\"].astype(\"category\")\ndf = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n\nprint(\"Using features:\", FEATS, \"| tickers:\", df[\"ticker\"].nunique(), \"| rows:\", len(df))\ndf.head()\n\n\n19.4.3 2) Time‑based split (first rolling‑origin split with embargo)\ndef make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    i = train_min - 1; out=[]\n    while True:\n        if i &gt;= len(u): break\n        a,b = u[0], u[i]\n        vs = i + embargo + 1\n        ve = vs + val_size - 1\n        if ve &gt;= len(u): break\n        out.append((a,b,u[vs],u[ve]))\n        i += step\n    return out\n\nsplits = make_rolling_origin_splits(df[\"date\"], 252, 63, 63, 5)\nassert splits, \"Not enough history for a first split.\"\na,b,c,d = splits[0]\ntrain_df = df[(df[\"date\"]&gt;=a)&(df[\"date\"]&lt;=b)].copy()\nval_df   = df[(df[\"date\"]&gt;=c)&(df[\"date\"]&lt;=d)].copy()\nprint(\"Split 1 - Train:\", a.date(), \"→\", b.date(), \"| Val:\", c.date(), \"→\", d.date(), \n      \"| train rows:\", len(train_df), \"| val rows:\", len(val_df))\n\n\n19.4.4 3) Reproducibility helpers, FeatureScaler, and WindowedDataset\nimport random, math, json\n\ndef seed_everything(seed=1337):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\nseed_everything(1337)\n\nclass FeatureScaler:\n    \"\"\"Train-only mean/std scaler for numpy arrays.\"\"\"\n    def __init__(self): self.mean_=None; self.std_=None\n    def fit(self, X: np.ndarray):\n        self.mean_ = X.mean(axis=0, dtype=np.float64)\n        self.std_  = X.std(axis=0, dtype=np.float64) + 1e-8\n        return self\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        return (X - self.mean_) / self.std_\n    def state_dict(self): \n        return {\"mean\": self.mean_.tolist(), \"std\": self.std_.tolist()}\n    def load_state_dict(self, d): \n        self.mean_ = np.array(d[\"mean\"], dtype=np.float64)\n        self.std_  = np.array(d[\"std\"],  dtype=np.float64)\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass WindowedDataset(Dataset):\n    \"\"\"\n    Sliding windows over time per ticker (multi-ticker, fixed context_len).\n    Each item: X in shape (T, F), y scalar: r_1d at window end.\n    \"\"\"\n    def __init__(self, frame: pd.DataFrame, feature_cols, context_len=64, scaler: FeatureScaler|None=None):\n        assert \"ticker\" in frame and \"date\" in frame and \"r_1d\" in frame\n        self.feature_cols = feature_cols\n        self.T = int(context_len)\n        self.groups = {}   # ticker -&gt; dict('X': np.ndarray [N,F], 'y': np.ndarray [N])\n        self.index  = []   # list of (ticker, end_idx)\n        # Build groups (per ticker)\n        for tkr, g in frame.groupby(\"ticker\"):\n            g = g.sort_values(\"date\").reset_index(drop=True)\n            X = g[feature_cols].to_numpy(dtype=np.float32)\n            y = g[\"r_1d\"].to_numpy(dtype=np.float32)\n            # valid windows end where we have T steps and y is finite\n            for end in range(self.T-1, len(g)):\n                if not np.isfinite(y[end]): \n                    continue\n                self.index.append((tkr, end))\n            self.groups[tkr] = {\"X\": X, \"y\": y}\n        # Fit scaler on all TRAIN rows (only when building train dataset)\n        self.scaler = scaler or FeatureScaler().fit(\n            np.concatenate([self.groups[t][\"X\"] for t in self.groups], axis=0)\n        )\n    def __len__(self): return len(self.index)\n    def __getitem__(self, i):\n        tkr, end = self.index[i]\n        g = self.groups[tkr]\n        xs = g[\"X\"][end-self.T+1:end+1]        # (T, F) context\n        xs = self.scaler.transform(xs)         # scale using train stats\n        y  = g[\"y\"][end]                       # scalar target\n        return torch.from_numpy(xs), torch.tensor(y), str(tkr)\n\ndef make_loaders(train_df, val_df, feature_cols, context_len=64, batch_size=256, num_workers=2):\n    # Train dataset fits scaler; Val shares it\n    train_ds = WindowedDataset(train_df, feature_cols, context_len=context_len, scaler=None)\n    val_ds   = WindowedDataset(val_df,   feature_cols, context_len=context_len, scaler=train_ds.scaler)\n    # Persist scaler for reuse\n    Path(\"models\").mkdir(exist_ok=True)\n    Path(\"models/scaler_split1.json\").write_text(json.dumps(train_ds.scaler.state_dict()))\n    pin = torch.cuda.is_available()\n    g = torch.Generator()\n    g.manual_seed(42)\n    def _seed_worker(_):\n        worker_seed = torch.initial_seed() % (2**32)\n        np.random.seed(worker_seed); random.seed(worker_seed)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True,\n                              num_workers=num_workers, pin_memory=pin, persistent_workers=(num_workers&gt;0),\n                              worker_init_fn=_seed_worker, generator=g)\n    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False,\n                              num_workers=num_workers, pin_memory=pin, persistent_workers=(num_workers&gt;0),\n                              worker_init_fn=_seed_worker)\n    return train_ds, val_ds, train_loader, val_loader\n\ntrain_ds, val_ds, train_loader, val_loader = make_loaders(train_df, val_df, FEATS, context_len=64, batch_size=256)\nlen(train_ds), len(val_ds), next(iter(train_loader))[0].shape\n\n\n19.4.5 4) Define a tiny GRU regressor\nimport torch.nn as nn, torch\n\nclass GRURegressor(nn.Module):\n    def __init__(self, in_features: int, hidden: int = 64, num_layers: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.gru = nn.GRU(input_size=in_features, hidden_size=hidden, num_layers=num_layers,\n                          batch_first=True, dropout=dropout if num_layers&gt;1 else 0.0)\n        self.head = nn.Sequential(\n            nn.Linear(hidden, hidden),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, 1)\n        )\n    def forward(self, x):        # x: (B, T, F)\n        _, hN = self.gru(x)      # hN: (num_layers, B, H); take last layer\n        h = hN[-1]               # (B, H)\n        return self.head(h).squeeze(-1)  # (B,)\n\ndef make_model():\n    return GRURegressor(in_features=len(FEATS), hidden=64, num_layers=2, dropout=0.1)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = make_model().to(device)\nsum(p.numel() for p in model.parameters())/1e6, device\n\n\n19.4.6 5) Training loop with AMP, early stopping, checkpointing\nfrom torch.optim import AdamW\nfrom torch.cuda.amp import autocast, GradScaler\nimport math, time\n\ndef mae_t(y_true, y_pred): return torch.mean(torch.abs(y_true - y_pred))\ndef smape_t(y_true, y_pred, eps=1e-8):\n    return torch.mean(2.0*torch.abs(y_true - y_pred)/(torch.abs(y_true)+torch.abs(y_pred)+eps))\n\ndef train_one_epoch(model, loader, optimizer, scaler, device, use_amp=True):\n    model.train(); total=0.0; n=0\n    for xb, yb, _ in loader:\n        xb = xb.to(device, non_blocking=True).float()\n        yb = yb.to(device, non_blocking=True).float()\n        optimizer.zero_grad(set_to_none=True)\n        if use_amp and device.type==\"cuda\":\n            with autocast(dtype=torch.float16):\n                pred = model(xb)\n                loss = mae_t(yb, pred)  # train with MAE (robust)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer); scaler.update()\n        else:\n            pred = model(xb); loss = mae_t(yb, pred)\n            loss.backward(); optimizer.step()\n        bs = xb.size(0); total += loss.item()*bs; n += bs\n    return total/max(n,1)\n\n@torch.no_grad()\ndef evaluate(model, loader, device):\n    model.eval(); tot_mae=tot_smape=0.0; n=0\n    for xb, yb, _ in loader:\n        xb = xb.to(device, non_blocking=True).float()\n        yb = yb.to(device, non_blocking=True).float()\n        pred = model(xb)\n        bs = xb.size(0)\n        tot_mae   += mae_t(yb, pred).item()*bs\n        tot_smape += smape_t(yb, pred).item()*bs\n        n += bs\n    return {\"mae\": tot_mae/max(n,1), \"smape\": tot_smape/max(n,1)}\n\ndef fit(model, train_loader, val_loader, epochs=12, lr=1e-3, wd=1e-5, patience=3, use_amp=True):\n    opt = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n    scaler = GradScaler(enabled=use_amp and (device.type==\"cuda\"))\n    best = math.inf; best_metrics=None; best_epoch=-1\n    ckpt_path = Path(\"models/gru_split1.pt\")\n    history=[]\n    for epoch in range(1, epochs+1):\n        t0=time.time()\n        tr_loss = train_one_epoch(model, train_loader, opt, scaler, device, use_amp)\n        val = evaluate(model, val_loader, device)\n        dt=time.time()-t0\n        history.append({\"epoch\":epoch,\"train_mae\":tr_loss,\"val_mae\":val[\"mae\"],\"val_smape\":val[\"smape\"],\"seconds\":dt})\n        print(f\"Epoch {epoch:02d}  train_mae={tr_loss:.5f}  val_mae={val['mae']:.5f}  val_sMAPE={val['smape']:.5f}  ({dt:.1f}s)\")\n        # early stopping on val mae\n        if val[\"mae\"] &lt; best - 1e-6:\n            best = val[\"mae\"]; best_metrics=val; best_epoch=epoch\n            torch.save({\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": opt.state_dict(),\n                \"epoch\": epoch,\n                \"val\": val,\n                \"config\": {\"lr\":lr,\"wd\":wd,\"epochs\":epochs,\"context_len\":train_loader.dataset.T,\"feats\":FEATS}\n            }, ckpt_path)\n        elif epoch - best_epoch &gt;= patience:\n            print(f\"Early stopping at epoch {epoch} (best {best:.5f} @ {best_epoch})\")\n            break\n    return history, best, best_epoch, ckpt_path\n\nhistory, best, best_epoch, ckpt_path = fit(model, train_loader, val_loader,\n                                           epochs=10, lr=1e-3, wd=1e-5, patience=3, use_amp=True)\nprint(\"Best val_mae:\", best, \"at epoch\", best_epoch, \"| saved:\", ckpt_path.exists())\n\n\n19.4.7 6) Evaluate best checkpoint & write a small report\n# Reload best checkpoint and compute final validation metrics + save CSV\nckpt = torch.load(\"models/gru_split1.pt\", map_location=\"cpu\")\nmodel.load_state_dict(ckpt[\"model_state\"])\nmodel.to(device)\nfinal = evaluate(model, val_loader, device)\nimport pandas as pd\nrep = pd.DataFrame([{\n    \"split\": 1,\n    \"context_len\": train_loader.dataset.T,\n    \"feats\": \",\".join(FEATS),\n    \"val_mae\": final[\"mae\"],\n    \"val_smape\": final[\"smape\"],\n    \"best_epoch\": ckpt.get(\"epoch\", None),\n    \"params_M\": round(sum(p.numel() for p in model.parameters())/1e6, 3)\n}])\nrep.to_csv(\"reports/gru_split1_metrics.csv\", index=False)\nrep\n\nTime check: With ~5–8 tickers, T=64, and 10 epochs, this should finish in a couple of minutes on Colab CPU; faster on GPU.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Session 19 — Tensors, Datasets, Training Loop</span>"
    ]
  },
  {
    "objectID": "lec19.html#wrapup-10-min-emphasize-these-points",
    "href": "lec19.html#wrapup-10-min-emphasize-these-points",
    "title": "19  Session 19 — Tensors, Datasets, Training Loop",
    "section": "19.5 Wrap‑up (10 min) — emphasize these points",
    "text": "19.5 Wrap‑up (10 min) — emphasize these points\n\nWindowedDataset emits causal windows (≤ t) and targets r_1d[t] (i.e., t+1 return).\nUse train‑fit scaler and reuse it on validation to avoid leakage.\nKeep the training loop simple: MAE training objective, AMP on CUDA, early stopping on validation MAE, and save a checkpoint.\nProduce a CSV with validation metrics to track progress and compare future models.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Session 19 — Tensors, Datasets, Training Loop</span>"
    ]
  },
  {
    "objectID": "lec19.html#homework-due-before-session-20",
    "href": "lec19.html#homework-due-before-session-20",
    "title": "19  Session 19 — Tensors, Datasets, Training Loop",
    "section": "19.6 Homework (due before Session 20)",
    "text": "19.6 Homework (due before Session 20)\nGoal: Train a stronger sequence baseline (choose LSTM or TCN) on a subset (5–10 tickers). Log metrics and push your checkpoint + report.\n\n19.6.1 Part A — Script scripts/train_seq.py (LSTM or TCN)\n#!/usr/bin/env python\nfrom __future__ import annotations\nimport argparse, json, math\nfrom pathlib import Path\nimport numpy as np, pandas as pd, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import autocast, GradScaler\n\n# --- (reuse minimal dataset/scaler from class; compact copy here) ---\nclass FeatureScaler:\n    def __init__(self): self.mean_=None; self.std_=None\n    def fit(self, X): self.mean_=X.mean(0); self.std_=X.std(0)+1e-8; return self\n    def transform(self, X): return (X-self.mean_)/self.std_\n    def state_dict(self): return {\"mean\": self.mean_.tolist(), \"std\": self.std_.tolist()}\n    def load_state_dict(self, d): import numpy as np; self.mean_=np.array(d[\"mean\"]); self.std_=np.array(d[\"std\"])\nclass WindowedDataset(Dataset):\n    def __init__(self, df, feats, T=64, scaler=None):\n        self.feats=feats; self.T=T; self.idx=[]; self.g={}\n        for tkr,g in df.groupby(\"ticker\"):\n            g=g.sort_values(\"date\").reset_index(drop=True)\n            X=g[feats].to_numpy(\"float32\"); y=g[\"r_1d\"].to_numpy(\"float32\")\n            for end in range(T-1,len(g)):\n                if np.isfinite(y[end]): self.idx.append((tkr,end))\n            self.g[tkr]={\"X\":X,\"y\":y}\n        self.scaler = scaler or FeatureScaler().fit(np.concatenate([self.g[t][\"X\"] for t in self.g],0))\n    def __len__(self): return len(self.idx)\n    def __getitem__(self,i):\n        tkr,end=self.idx[i]; g=self.g[tkr]\n        X=g[\"X\"][end-self.T+1:end+1]; X=self.scaler.transform(X)\n        y=g[\"y\"][end]\n        return torch.from_numpy(X), torch.tensor(y)\n\ndef make_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u=np.array(sorted(pd.to_datetime(pd.Series(dates).unique()))); i=train_min-1; out=[]\n    while True:\n        if i&gt;=len(u): break\n        a,b=u[0],u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve&gt;=len(u): break\n        out.append((a,b,u[vs],u[ve])); i+=step\n    return out\n\n# --- Models ---\nclass LSTMReg(nn.Module):\n    def __init__(self, in_f, hidden=64, layers=2, dropout=0.1):\n        super().__init__()\n        self.lstm = nn.LSTM(in_f, hidden, num_layers=layers, batch_first=True, dropout=dropout if layers&gt;1 else 0.)\n        self.head = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Dropout(dropout), nn.Linear(hidden,1))\n    def forward(self, x):\n        out,_ = self.lstm(x)\n        h = out[:,-1,:]\n        return self.head(h).squeeze(-1)\n\nclass TCNBlock(nn.Module):\n    def __init__(self, in_c, out_c, k=3, d=1, dropout=0.1):\n        super().__init__()\n        pad = (k-1)*d\n        self.net = nn.Sequential(\n            nn.Conv1d(in_c, out_c, k, padding=pad, dilation=d),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Conv1d(out_c, out_c, k, padding=pad, dilation=d),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n        )\n        self.down = nn.Conv1d(in_c, out_c, 1) if in_c!=out_c else nn.Identity()\n    def forward(self, x):        # x: (B, F, T)\n        y = self.net(x)\n        # Causal crop to ensure output aligns with last time step\n        crop = y.shape[-1]-x.shape[-1]\n        if crop&gt;0: y = y[..., :-crop]\n        return y + self.down(x)\n\nclass TCNReg(nn.Module):\n    def __init__(self, in_f, ch=64, blocks=3, k=3, dropout=0.1):\n        super().__init__()\n        layers=[]; c=in_f\n        for b in range(blocks):\n            layers.append(TCNBlock(c, ch, k=k, d=2**b, dropout=dropout)); c=ch\n        self.tcn = nn.Sequential(*layers)\n        self.head = nn.Sequential(nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Linear(ch,1))\n    def forward(self, x):\n        # x: (B,T,F) -&gt; (B,F,T) for Conv1d\n        x = x.transpose(1,2)\n        y = self.tcn(x)              # (B, C, T)\n        return self.head(y).squeeze(-1)\n\ndef mae_t(y,yhat): return torch.mean(torch.abs(y - yhat))\ndef smape_t(y,yhat,eps=1e-8): return torch.mean(2*torch.abs(y-yhat)/(torch.abs(y)+torch.abs(yhat)+eps))\n\ndef main():\n    ap=argparse.ArgumentParser()\n    ap.add_argument(\"--features\", default=\"data/processed/features_v1.parquet\")\n    ap.add_argument(\"--context\", type=int, default=64)\n    ap.add_argument(\"--model\", choices=[\"lstm\",\"tcn\"], default=\"lstm\")\n    ap.add_argument(\"--epochs\", type=int, default=12)\n    ap.add_argument(\"--batch\", type=int, default=256)\n    ap.add_argument(\"--lr\", type=float, default=1e-3)\n    ap.add_argument(\"--patience\", type=int, default=3)\n    ap.add_argument(\"--tickers\", type=int, default=8)\n    args=ap.parse_args()\n\n    df = pd.read_parquet(\"data/processed/features_v1_static.parquet\") if Path(\"data/processed/features_v1_static.parquet\").exists() else pd.read_parquet(args.features)\n    df = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    cand = [\"log_return\",\"lag1\",\"lag2\",\"lag3\",\"zscore_20\",\"roll_std_20\"]\n    feats = [c for c in cand if c in df.columns]\n    assert \"r_1d\" in df.columns\n    # subset tickers\n    keep = df[\"ticker\"].astype(str).unique().tolist()[:args.tickers]\n    df = df[df[\"ticker\"].astype(str).isin(keep)].copy()\n\n    splits = make_splits(df[\"date\"])\n    a,b,c,d = splits[0]\n    tr = df[(df[\"date\"]&gt;=a)&(df[\"date\"]&lt;=b)]\n    va = df[(df[\"date\"]&gt;=c)&(df[\"date\"]&lt;=d)]\n\n    train_ds = WindowedDataset(tr, feats, T=args.context, scaler=None)\n    val_ds   = WindowedDataset(va, feats, T=args.context, scaler=train_ds.scaler)\n\n    pin = torch.cuda.is_available()\n    g = torch.Generator(); g.manual_seed(42)\n    def _seed_worker(_): import numpy as np, random, torch; ws=torch.initial_seed()%2**32; np.random.seed(ws); random.seed(ws)\n    train_ld = DataLoader(train_ds, batch_size=args.batch, shuffle=True, drop_last=True,\n                          num_workers=2, pin_memory=pin, worker_init_fn=_seed_worker, generator=g)\n    val_ld   = DataLoader(val_ds, batch_size=args.batch, shuffle=False, drop_last=False,\n                          num_workers=2, pin_memory=pin, worker_init_fn=_seed_worker)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    net = (LSTMReg(len(feats)) if args.model==\"lstm\" else TCNReg(len(feats))).to(device)\n    opt = torch.optim.AdamW(net.parameters(), lr=args.lr, weight_decay=1e-5)\n    scaler = GradScaler(enabled=(device.type==\"cuda\"))\n    best = 1e9; best_epoch=0\n    ckpt = Path(f\"models/{args.model}_split1.pt\")\n\n    for epoch in range(1, args.epochs+1):\n        net.train(); tmae=0; n=0\n        for xb,yb in train_ld:\n            xb=xb.to(device).float(); yb=yb.to(device).float()\n            opt.zero_grad(set_to_none=True)\n            with autocast(enabled=(device.type==\"cuda\"), dtype=torch.float16):\n                yhat = net(xb)\n                loss = mae_t(yb, yhat)\n            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n            bs=xb.size(0); tmae += loss.item()*bs; n+=bs\n        tr_mae=tmae/n\n        # val\n        net.eval(); vmae=vsm=0; n=0\n        with torch.no_grad():\n            for xb,yb in val_ld:\n                xb=xb.to(device).float(); yb=yb.to(device).float()\n                yhat = net(xb)\n                bs=xb.size(0); vmae += mae_t(yb,yhat).item()*bs; vsm += smape_t(yb,yhat).item()*bs; n+=bs\n        vmae/=n; vsm/=n\n        print(f\"Epoch {epoch:02d}  tr_mae={tr_mae:.5f}  val_mae={vmae:.5f}  val_sMAPE={vsm:.5f}\")\n        if vmae &lt; best-1e-6:\n            best=vmae; best_epoch=epoch\n            torch.save({\"model\": net.state_dict(), \"epoch\": epoch, \"feats\": feats, \"context\": args.context}, ckpt)\n        elif epoch - best_epoch &gt;= args.patience:\n            print(\"Early stopping.\")\n            break\n\n    Path(\"reports\").mkdir(exist_ok=True)\n    pd.DataFrame([{\"model\":args.model,\"context\":args.context,\"val_mae\":best,\"best_epoch\":best_epoch,\"feats\":\",\".join(feats)}]).to_csv(\n        f\"reports/{args.model}_split1_metrics.csv\", index=False)\n\nif __name__ == \"__main__\":\n    main()\nRun (from repo root):\n%%bash\nchmod +x scripts/train_seq.py\npython scripts/train_seq.py --model lstm --context 64 --tickers 8 --epochs 12\n\n\n19.6.2 Part B — Add a quick Makefile target and a tiny test\nAppend to Makefile:\n.PHONY: train-lstm\ntrain-lstm: ## Train LSTM baseline on split 1 (subset of tickers)\n\\tpython scripts/train_seq.py --model lstm --context 64 --tickers 8 --epochs 12\nBasic shape test for dataset windows:\n# tests/test_windowed_dataset.py\nimport pandas as pd, numpy as np, os\ndef test_window_shapes():\n    import scripts.train_seq as T\n    df = pd.read_parquet(\"data/processed/features_v1.parquet\").sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    feats = [c for c in [\"log_return\",\"lag1\",\"lag2\",\"lag3\"] if c in df.columns]\n    splits = T.make_splits(df[\"date\"])\n    a,b,c,d = splits[0]\n    ds = T.WindowedDataset(df[(df[\"date\"]&gt;=a)&(df[\"date\"]&lt;=b)], feats, T=32, scaler=None)\n    X,y = ds[0]\n    assert X.shape == (32, len(feats))\n    assert np.isfinite(y.item())\nRun:\n%%bash\npytest -q -k windowed_dataset\n\n\n19.6.3 Part C — Report\nAdd to your Quarto report (e.g., reports/eda.qmd):\n## PyTorch Baselines\n\n::: {#d083c67a .cell execution_count=1}\n````` {.python .cell-code}\nimport pandas as pd\nprint(pd.read_csv(\"reports/gru_split1_metrics.csv\"))\ntry:\n    print(pd.read_csv(\"reports/lstm_split1_metrics.csv\"))\nexcept Exception as e:\n    print(\"lstm metrics not found yet\")\n````` :::",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Session 19 — Tensors, Datasets, Training Loop</span>"
    ]
  },
  {
    "objectID": "lec19.html#instructor-checklist-before-class",
    "href": "lec19.html#instructor-checklist-before-class",
    "title": "19  Session 19 — Tensors, Datasets, Training Loop",
    "section": "19.7 Instructor checklist (before class)",
    "text": "19.7 Instructor checklist (before class)\n\nConfirm features_v1.parquet exists with r_1d. If not, ensure the fallback creates minimal lags and r_1d.\n\nDry‑run the GRU training in a fresh Colab (~2–5 min).\n\nPrepare a slide on why we use MAE for training (robust to outliers) and sMAPE for reporting.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Session 19 — Tensors, Datasets, Training Loop</span>"
    ]
  },
  {
    "objectID": "lec19.html#emphasize-while-teaching",
    "href": "lec19.html#emphasize-while-teaching",
    "title": "19  Session 19 — Tensors, Datasets, Training Loop",
    "section": "19.8 Emphasize while teaching",
    "text": "19.8 Emphasize while teaching\n\nCausality: windows contain only information up to time t; the label is t+1.\n\nNo leakage: scaler fit on train only; reuse for val/test.\n\nReproducibility: seeds, deterministic flags, and saving checkpoints.\n\nEfficiency: pin memory, small batch size, AMP on CUDA.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Session 19 — Tensors, Datasets, Training Loop</span>"
    ]
  },
  {
    "objectID": "lec19.html#grading-passrevise",
    "href": "lec19.html#grading-passrevise",
    "title": "19  Session 19 — Tensors, Datasets, Training Loop",
    "section": "19.9 Grading (pass/revise)",
    "text": "19.9 Grading (pass/revise)\n\nWindowedDataset works and yields (B, T, F) batches.\n\nTraining loop runs with early stopping and writes models/gru_split1.pt.\n\nreports/gru_split1_metrics.csv exists with val_mae and val_smape.\n\nHomework script scripts/train_seq.py runs and writes its metrics CSV; optional test passes.\n\nYou now have a clean PyTorch scaffold: deterministic dataset windows, a minimal training loop with early stopping, and saved checkpoints—ready for Session 20, where you’ll build a unified multi‑asset model (ticker embeddings, mixed batches) on the same pipeline. ```",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Session 19 — Tensors, Datasets, Training Loop</span>"
    ]
  },
  {
    "objectID": "lec20.html",
    "href": "lec20.html",
    "title": "20  Session 20 — Multi‑asset training (unified model",
    "section": "",
    "text": "20.1 Session 20 — Multi‑asset training (unified model) (75 min)\nBelow is a complete lecture package for Session 20 — Multi‑asset training (unified model) (75 minutes). It includes a timed agenda, slide talking points, a Colab‑friendly in‑class lab with copy‑paste code, and homework with copy‑paste code. In class you’ll add a ticker_id embedding (and optional sector embedding) to a sequence model so one model learns across all tickers.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Session 20 — Multi‑asset training (unified model</span>"
    ]
  },
  {
    "objectID": "lec20.html#session-20-multiasset-training-unified-model-75-min",
    "href": "lec20.html#session-20-multiasset-training-unified-model-75-min",
    "title": "20  Session 20 — Multi‑asset training (unified model",
    "section": "",
    "text": "20.1.1 Learning goals\nBy the end of class, students can:\n\nTrain one unified model across many tickers instead of one model per ticker.\nAdd a ticker_id embedding (and optional sector embedding) and concatenate it to sequence inputs.\nBatch mixed tickers safely (respecting the same time‑based splits and embargo from Session 15).\nLog overall validation metrics and per‑ticker metrics for fair comparison later.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Session 20 — Multi‑asset training (unified model</span>"
    ]
  },
  {
    "objectID": "lec20.html#agenda-75-min",
    "href": "lec20.html#agenda-75-min",
    "title": "20  Session 20 — Multi‑asset training (unified model",
    "section": "20.2 Agenda (75 min)",
    "text": "20.2 Agenda (75 min)\n\n(10 min) Slides: unified vs per‑asset models; why embeddings; pitfalls\n(10 min) Slides: batching mixed tickers; leakage guardrails; embedding size heuristics\n(35 min) In‑class lab: dataset that returns ticker_id → GRU with nn.Embedding → train/evaluate → write per‑ticker metrics\n(10 min) Wrap‑up + homework brief\n(10 min) Buffer / Q&A",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Session 20 — Multi‑asset training (unified model</span>"
    ]
  },
  {
    "objectID": "lec20.html#slide-talking-points-add-to-your-deck",
    "href": "lec20.html#slide-talking-points-add-to-your-deck",
    "title": "20  Session 20 — Multi‑asset training (unified model",
    "section": "20.3 Slide talking points (add to your deck)",
    "text": "20.3 Slide talking points (add to your deck)\n\n20.3.1 Why unified?\n\nData efficiency: share statistical strength across assets.\nPersonalization: learn asset‑specific biases via ID embeddings (and optional sector embeddings).\nSimplicity: one checkpoint, easier hyperparam search.\nTrade‑off: can overfit to IDs if embedding too large; must compare to per‑ticker baselines.\n\n\n\n20.3.2 Embeddings in a regression model\n\nTreat categorical IDs (ticker, sector) as learnable vectors.\nConcatenate the embedding to every time step features: \\(x'_t = [x_t \\;\\|\\; e_{\\text{ticker}} \\;\\|\\; e_{\\text{sector}}]\\).\nHeuristic sizes: \\(d_\\text{ticker} \\in [8, 16]\\); \\(d_\\text{sector} \\in [4, 8]\\). Start small.\n\n\n\n20.3.3 Batching mixed tickers without leakage\n\nSplits are by date (same as Session 15); do not fit scalers or thresholds on validation.\nThe ID mapping is just indices; it’s not a data leak.\nKeep train‑fit scaler, reuse on val/test.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Session 20 — Multi‑asset training (unified model</span>"
    ]
  },
  {
    "objectID": "lec20.html#inclass-lab-35-min-colabfriendly",
    "href": "lec20.html#inclass-lab-35-min-colabfriendly",
    "title": "20  Session 20 — Multi‑asset training (unified model",
    "section": "20.4 In‑class lab (35 min, Colab‑friendly)",
    "text": "20.4 In‑class lab (35 min, Colab‑friendly)\n\nRun each block as its own cell in Colab. Replace REPO_NAME if needed.\n\n\n20.4.1 0) Setup & device\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"   # &lt;- change to your repo name\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, sys, platform, random\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/processed\",\"models\",\"reports\",\"scripts\",\"tests\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\nimport numpy as np, pandas as pd, torch\nprint(\"Torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available(), \"| Python:\", sys.version.split()[0], \"| OS:\", platform.system())\n\ndef seed_everything(seed=2025):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\nseed_everything(2025)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\n\n20.4.2 1) Load features (with safe fallback) and define a split\nfrom pathlib import Path\n\nf_static = Path(\"data/processed/features_v1_static.parquet\")\nf_base   = Path(\"data/processed/features_v1.parquet\")\n\nif f_static.exists():\n    df = pd.read_parquet(f_static)\nelif f_base.exists():\n    df = pd.read_parquet(f_base)\nelse:\n    # Fallback from returns\n    rpath = Path(\"data/processed/returns.parquet\")\n    if not rpath.exists():\n        # synthesize a tiny dataset to keep class flowing\n        rng = np.random.default_rng(0)\n        dates = pd.bdate_range(\"2022-01-03\", periods=340)\n        frames=[]\n        for t in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\",\"META\"]:\n            eps = rng.normal(0,0.012,size=len(dates)).astype(\"float32\")\n            adj = 100*np.exp(np.cumsum(eps))\n            g = pd.DataFrame({\n                \"date\": dates, \"ticker\": t,\n                \"adj_close\": adj.astype(\"float32\"),\n                \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n            })\n            g[\"r_1d\"] = g[\"log_return\"].shift(-1)\n            frames.append(g)\n        df = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)\n        df[\"ticker\"] = df[\"ticker\"].astype(\"category\")\n        df.to_parquet(\"data/processed/returns.parquet\", index=False)\n    else:\n        df = pd.read_parquet(rpath)\n        df = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    # ensure minimal features\n    for k in [1,2,3]:\n        df[f\"lag{k}\"] = df.groupby(\"ticker\")[\"log_return\"].shift(k)\n    df[\"roll_std_20\"]  = df.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)\n    df[\"zscore_20\"]    = (df[\"log_return\"] - df.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).mean().reset_index(level=0, drop=True)) / (df[\"roll_std_20\"] + 1e-8)\n    df = df.dropna().reset_index(drop=True)\n\n# Harmonize and subset for speed\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\ndf[\"ticker\"] = df[\"ticker\"].astype(\"category\")\ndf = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nkeep = df[\"ticker\"].cat.categories.tolist()[:10]  # up to 10 tickers for class\ndf = df[df[\"ticker\"].isin(keep)].copy()\n\n# Choose features\nCAND = [\"log_return\",\"lag1\",\"lag2\",\"lag3\",\"zscore_20\",\"roll_std_20\"]\nFEATS = [c for c in CAND if c in df.columns]\nassert \"r_1d\" in df.columns and FEATS, \"Missing required columns.\"\n\ndef make_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    i=train_min-1; out=[]\n    while True:\n        if i&gt;=len(u): break\n        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve&gt;=len(u): break\n        out.append((a,b,u[vs],u[ve])); i+=step\n    return out\n\nsplits = make_splits(df[\"date\"], 252, 63, 63, 5)\nassert splits, \"Not enough history for split 1.\"\na,b,c,d = splits[0]\ntrain_df = df[(df[\"date\"]&gt;=a)&(df[\"date\"]&lt;=b)].copy()\nval_df   = df[(df[\"date\"]&gt;=c)&(df[\"date\"]&lt;=d)].copy()\nprint(\"Split1 Train:\", a.date(), \"→\", b.date(), \"| Val:\", c.date(), \"→\", d.date(),\n      \"| tickers train:\", train_df[\"ticker\"].nunique(), \"val:\", val_df[\"ticker\"].nunique())\n\n\n20.4.3 2) Dataset with ticker IDs (+ optional sector)\nimport json\nfrom torch.utils.data import Dataset, DataLoader\n\nclass FeatureScaler:\n    def __init__(self): self.mean_=None; self.std_=None\n    def fit(self, X): self.mean_=X.mean(0, dtype=np.float64); self.std_=X.std(0, dtype=np.float64)+1e-8; return self\n    def transform(self, X): return (X - self.mean_) / self.std_\n    def state_dict(self): return {\"mean\": self.mean_.tolist(), \"std\": self.std_.tolist()}\n    def load_state_dict(self, d): self.mean_=np.array(d[\"mean\"]); self.std_=np.array(d[\"std\"])\n\ndef build_sector_map(frame: pd.DataFrame):\n    # Optional: if a 'sector' column exists (from Session 12 scrape), use it; else all 'UNKNOWN'.\n    if \"sector\" in frame.columns:\n        sec = frame[[\"ticker\",\"sector\"]].drop_duplicates().copy()\n    else:\n        sec = frame[[\"ticker\"]].drop_duplicates().copy()\n        sec[\"sector\"] = \"UNKNOWN\"\n    sec[\"sector\"] = sec[\"sector\"].astype(\"category\")\n    return dict(zip(sec[\"ticker\"].astype(str), sec[\"sector\"].cat.codes.astype(int))), len(sec[\"sector\"].cat.categories)\n\nclass WindowedDatasetXID(Dataset):\n    \"\"\"\n    Sliding windows per ticker with ID features.\n    Returns: X_scaled (T,F), y (scalar), ticker_id (long), sector_id (long)\n    \"\"\"\n    def __init__(self, frame: pd.DataFrame, feature_cols, context_len=64,\n                 scaler: FeatureScaler|None=None,\n                 ticker2id: dict|None=None, sector2id: dict|None=None, n_sectors: int|None=None):\n        assert {\"ticker\",\"date\",\"r_1d\"}.issubset(frame.columns)\n        self.feature_cols = list(feature_cols); self.T = int(context_len)\n        self.groups, self.index = {}, []\n\n        # Build mappings on TRAIN; reuse on VAL\n        if ticker2id is None:\n            cats = frame[\"ticker\"].astype(\"category\").cat.categories.tolist()\n            self.ticker2id = {t:i for i,t in enumerate(cats)}\n        else:\n            self.ticker2id = dict(ticker2id)\n        sec_map, nsec = build_sector_map(frame)\n        if sector2id is None:\n            # Freeze sector ids according to sec_map order\n            uniq_secs = sorted(set(sec_map.values()))\n            self.sector2id = {s:i for i,s in enumerate(uniq_secs)}\n            self.n_sectors = len(self.sector2id)\n        else:\n            self.sector2id = dict(sector2id)\n            self.n_sectors = int(n_sectors)\n\n        # Build per-ticker arrays and global window index\n        for tkr, g in frame.groupby(\"ticker\"):\n            g = g.sort_values(\"date\").reset_index(drop=True)\n            X = g[self.feature_cols].to_numpy(\"float32\")\n            y = g[\"r_1d\"].to_numpy(\"float32\")\n            t_id = self.ticker2id[str(tkr)]\n            s_id = self.sector2id.get(build_sector_map(g)[0][str(tkr)], 0) if \"sector\" in g else 0\n            # Valid windows\n            for end in range(self.T-1, len(g)):\n                if np.isfinite(y[end]):\n                    self.index.append((str(tkr), end, t_id, s_id))\n            self.groups[str(tkr)] = {\"X\": X, \"y\": y}\n\n        # Fit or reuse scaler\n        self.scaler = scaler or FeatureScaler().fit(\n            np.concatenate([self.groups[t][\"X\"] for t in self.groups], axis=0)\n        )\n\n    def __len__(self): return len(self.index)\n\n    def __getitem__(self, i):\n        tkr, end, t_id, s_id = self.index[i]\n        g = self.groups[tkr]\n        xs = g[\"X\"][end-self.T+1:end+1]\n        xs = self.scaler.transform(xs)\n        y  = g[\"y\"][end]\n        return (torch.from_numpy(xs), torch.tensor(y, dtype=torch.float32),\n                torch.tensor(t_id, dtype=torch.long), torch.tensor(s_id, dtype=torch.long))\n\n# Build TRAIN/VAL datasets & loaders (reusing train-fitted scaler and ID maps)\nT = 64; BATCH=256; WORKERS=2; PIN=torch.cuda.is_available()\n\ntrain_ds = WindowedDatasetXID(train_df, FEATS, context_len=T)\nval_ds   = WindowedDatasetXID(val_df,   FEATS, context_len=T,\n                              scaler=train_ds.scaler,\n                              ticker2id=train_ds.ticker2id,\n                              sector2id=train_ds.sector2id,\n                              n_sectors=train_ds.n_sectors)\n\nfrom torch.utils.data import DataLoader\ndef _seed_worker(_):\n    ws = torch.initial_seed() % (2**32)\n    np.random.seed(ws); random.seed(ws)\n\ng = torch.Generator(); g.manual_seed(42)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, drop_last=True,\n                          num_workers=WORKERS, pin_memory=PIN, persistent_workers=(WORKERS&gt;0),\n                          worker_init_fn=_seed_worker, generator=g)\nval_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, drop_last=False,\n                          num_workers=WORKERS, pin_memory=PIN, persistent_workers=(WORKERS&gt;0),\n                          worker_init_fn=_seed_worker)\n\nlen(train_ds), len(val_ds), next(iter(train_loader))[0].shape, len(train_ds.ticker2id)\n\n\n20.4.4 3) Unified GRU with ticker_id embedding (optional sector)\nimport torch.nn as nn, torch\nfrom torch.cuda.amp import autocast, GradScaler\n\nclass UnifiedGRUWithID(nn.Module):\n    def __init__(self, in_features: int, n_tickers: int, d_ticker: int = 12,\n                 n_sectors: int | None = None, d_sector: int = 0,\n                 hidden: int = 64, num_layers: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.tok = nn.Embedding(n_tickers, d_ticker)\n        self.sec = nn.Embedding(n_sectors, d_sector) if (n_sectors and d_sector&gt;0) else None\n        augmented_in = in_features + d_ticker + (d_sector if self.sec else 0)\n        self.gru = nn.GRU(input_size=augmented_in, hidden_size=hidden, num_layers=num_layers,\n                          batch_first=True, dropout=dropout if num_layers&gt;1 else 0.0)\n        self.head = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Dropout(dropout), nn.Linear(hidden, 1))\n\n    def forward(self, x, ticker_ids, sector_ids=None):\n        # x: (B,T,F)\n        e = self.tok(ticker_ids)                          # (B, d_ticker)\n        if self.sec is not None and sector_ids is not None:\n            e = torch.cat([e, self.sec(sector_ids)], dim=-1)  # (B, d_ticker+d_sector)\n        e = e.unsqueeze(1).expand(-1, x.size(1), -1)      # repeat across time\n        x_aug = torch.cat([x, e], dim=-1)                 # (B,T,F+E)\n        _, hN = self.gru(x_aug)\n        h = hN[-1]                                        # (B, H)\n        return self.head(h).squeeze(-1)\n\ndef make_model():\n    return UnifiedGRUWithID(in_features=len(FEATS),\n                            n_tickers=len(train_ds.ticker2id),\n                            d_ticker=12,\n                            n_sectors=val_ds.n_sectors, d_sector=0,  # set &gt;0 if you have sector\n                            hidden=64, num_layers=2, dropout=0.1)\n\nmodel = make_model().to(device)\nsum(p.numel() for p in model.parameters())/1e6, device\n\n\n20.4.5 4) Training loop (AMP + early stopping) and evaluation (per‑ticker metrics)\nfrom torch.optim import AdamW\nimport time, math\n\ndef mae_t(y, yhat): return torch.mean(torch.abs(y - yhat))\ndef smape_t(y, yhat, eps=1e-8): return torch.mean(2*torch.abs(y - yhat)/(torch.abs(y)+torch.abs(yhat)+eps))\n\ndef train_one_epoch(model, loader, optimizer, scaler, device, use_amp=True):\n    model.train(); tot=0.0; n=0\n    for xb, yb, tid, sid in loader:\n        xb = xb.to(device, non_blocking=True).float()\n        yb = yb.to(device, non_blocking=True).float()\n        tid = tid.to(device, non_blocking=True)\n        sid = sid.to(device, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        if use_amp and device.type==\"cuda\":\n            with autocast(dtype=torch.float16):\n                pred = model(xb, tid, sid)\n                loss = mae_t(yb, pred)\n            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n        else:\n            pred = model(xb, tid, sid); loss = mae_t(yb, pred); loss.backward(); optimizer.step()\n        bs = xb.size(0); tot += loss.item()*bs; n += bs\n    return tot/max(n,1)\n\n@torch.no_grad()\ndef evaluate(model, loader, device, return_preds=False):\n    model.eval(); t_mae=t_smape=0.0; n=0\n    all_rows=[]\n    for xb, yb, tid, sid in loader:\n        xb = xb.to(device, non_blocking=True).float()\n        yb = yb.to(device, non_blocking=True).float()\n        tid = tid.to(device, non_blocking=True)\n        sid = sid.to(device, non_blocking=True)\n        pred = model(xb, tid, sid)\n        bs = xb.size(0); t_mae += mae_t(yb, pred).item()*bs; t_smape += smape_t(yb, pred).item()*bs; n+=bs\n        if return_preds:\n            all_rows.append((yb.detach().cpu().numpy(), pred.detach().cpu().numpy(), tid.detach().cpu().numpy()))\n    out = {\"mae\": t_mae/max(n,1), \"smape\": t_smape/max(n,1)}\n    if return_preds:\n        ys = np.concatenate([r[0] for r in all_rows]); yh = np.concatenate([r[1] for r in all_rows]); tids = np.concatenate([r[2] for r in all_rows])\n        out[\"y_true\"] = ys; out[\"y_pred\"] = yh; out[\"ticker_id\"] = tids\n    return out\n\ndef fit_unified(model, train_loader, val_loader, epochs=12, lr=1e-3, wd=1e-5, patience=3, use_amp=True):\n    opt = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n    scaler = GradScaler(enabled=(use_amp and device.type==\"cuda\"))\n    best=math.inf; best_ep=-1; ckpt = Path(\"models/unified_gru_split1.pt\")\n    hist=[]\n    for ep in range(1, epochs+1):\n        t0=time.time()\n        tr = train_one_epoch(model, train_loader, opt, scaler, device, use_amp)\n        val = evaluate(model, val_loader, device)\n        dt=time.time()-t0\n        hist.append({\"epoch\":ep,\"train_mae\":tr,\"val_mae\":val[\"mae\"],\"val_smape\":val[\"smape\"],\"sec\":dt})\n        print(f\"Epoch {ep:02d}  train_mae={tr:.5f}  val_mae={val['mae']:.5f}  val_sMAPE={val['smape']:.5f}  ({dt:.1f}s)\")\n        if val[\"mae\"] &lt; best - 1e-6:\n            best = val[\"mae\"]; best_ep=ep\n            torch.save({\"model\": model.state_dict(),\n                        \"epoch\": ep,\n                        \"config\": {\"feats\": FEATS, \"T\": T, \"d_ticker\": 12}}, ckpt)\n        elif ep - best_ep &gt;= patience:\n            print(f\"Early stopping at epoch {ep} (best {best:.5f} @ {best_ep})\"); break\n    return hist, best, best_ep, ckpt\n\nmodel = make_model().to(device)\nhist, best, best_ep, ckpt = fit_unified(model, train_loader, val_loader, epochs=10, lr=1e-3, wd=1e-5, patience=3, use_amp=True)\nprint(\"Best val_mae:\", best, \"| epoch:\", best_ep, \"| saved:\", ckpt.exists())\n\n20.4.5.1 Per‑ticker metrics & CSVs\n# Reload best and compute per-ticker metrics\nckpt = torch.load(\"models/unified_gru_split1.pt\", map_location=device)\nmodel.load_state_dict(ckpt[\"model\"]); model.to(device)\n\n# Evaluate with predictions and map back to ticker symbols\nres = evaluate(model, val_loader, device, return_preds=True)\nid2ticker = {v:k for k,v in train_ds.ticker2id.items()}\npt_rows=[]\nfor tid in np.unique(res[\"ticker_id\"]):\n    m = res[\"ticker_id\"]==tid\n    y = res[\"y_true\"][m]; yhat = res[\"y_pred\"][m]\n    if len(y)==0: continue\n    pt_rows.append({\"ticker\": id2ticker[int(tid)], \"n\": int(len(y)),\n                    \"mae\": float(np.mean(np.abs(y - yhat))),\n                    \"smape\": float(np.mean(2*np.abs(y - yhat)/(np.abs(y)+np.abs(yhat)+1e-8)))})\nper_ticker = pd.DataFrame(pt_rows).sort_values(\"mae\")\nper_ticker.to_csv(\"reports/unified_gru_split1_per_ticker.csv\", index=False)\n\n# Overall micro metrics (pooled)\noverall = pd.DataFrame([{\n    \"split\": 1, \"model\": \"unified_gru_id\", \"context\": T, \"feats\": \",\".join(FEATS),\n    \"val_mae\": float(np.mean(np.abs(res[\"y_true\"] - res[\"y_pred\"]))),\n    \"val_smape\": float(np.mean(2*np.abs(res[\"y_true\"] - res[\"y_pred\"])/(np.abs(res[\"y_true\"])+np.abs(res[\"y_pred\"])+1e-8))),\n    \"best_epoch\": ckpt.get(\"epoch\", None),\n    \"params_M\": round(sum(p.numel() for p in model.parameters())/1e6, 3)\n}])\noverall.to_csv(\"reports/unified_gru_split1_metrics.csv\", index=False)\n(per_ticker.head(), overall)\n\nTime check: With ~8–10 tickers, T=64, and 10 epochs, this should finish in a couple of minutes on Colab CPU; faster on GPU.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Session 20 — Multi‑asset training (unified model</span>"
    ]
  },
  {
    "objectID": "lec20.html#wrapup-10-min-key-points",
    "href": "lec20.html#wrapup-10-min-key-points",
    "title": "20  Session 20 — Multi‑asset training (unified model",
    "section": "20.5 Wrap‑up (10 min) — key points",
    "text": "20.5 Wrap‑up (10 min) — key points\n\nUnified models share information across assets and adapt via ID embeddings.\nEmbedding dims should be small (8–16) to avoid overfitting; they act like bias + style vectors.\nKeep the same splits and train‑fit scaler to avoid leakage.\nAlways log per‑ticker metrics to check that no specific asset collapses.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Session 20 — Multi‑asset training (unified model</span>"
    ]
  },
  {
    "objectID": "lec20.html#homework-due-before-session-21",
    "href": "lec20.html#homework-due-before-session-21",
    "title": "20  Session 20 — Multi‑asset training (unified model",
    "section": "20.6 Homework (due before Session 21)",
    "text": "20.6 Homework (due before Session 21)\nGoal: Compare unified (ID‑augmented) vs per‑ticker models on split 1. Produce a single table reports/unified_vs_per_ticker_split1.csv and a short paragraph in your Quarto report.\n\n20.6.1 Part A — Train per‑ticker GRU baselines and compare\nCreate scripts/compare_unified_vs_per_ticker.py:\n#!/usr/bin/env python\nfrom __future__ import annotations\nimport numpy as np, pandas as pd, torch, torch.nn as nn\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.cuda.amp import autocast, GradScaler\n\n# ---- Utilities reused from class ----\ndef make_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique()))); i=train_min-1; out=[]\n    while True:\n        if i&gt;=len(u): break\n        a,b=u[0],u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve&gt;=len(u): break\n        out.append((a,b,u[vs],u[ve])); i+=step\n    return out\n\nclass FeatureScaler:\n    def __init__(self): self.mean_=None; self.std_=None\n    def fit(self, X): self.mean_=X.mean(0); self.std_=X.std(0)+1e-8; return self\n    def transform(self, X): return (X-self.mean_)/self.std_\n\nclass WindowedDataset(Dataset):\n    def __init__(self, df, feats, T=64, scaler=None):\n        self.feats=feats; self.T=T; self.idx=[]; self.X=None; self.y=None\n        g=df.sort_values(\"date\").reset_index(drop=True)\n        self.X = g[feats].to_numpy(\"float32\"); self.y = g[\"r_1d\"].to_numpy(\"float32\")\n        for end in range(T-1, len(g)):\n            if np.isfinite(self.y[end]): self.idx.append(end)\n        self.scaler = scaler or FeatureScaler().fit(self.X)\n    def __len__(self): return len(self.idx)\n    def __getitem__(self,i):\n        end=self.idx[i]; X=self.scaler.transform(self.X[end-self.T+1:end+1])\n        return torch.from_numpy(X), torch.tensor(self.y[end], dtype=torch.float32)\n\nclass GRUReg(nn.Module):\n    def __init__(self, in_f, h=64, L=2, p=0.1):\n        super().__init__()\n        self.gru = nn.GRU(in_f, h, num_layers=L, batch_first=True, dropout=p if L&gt;1 else 0.)\n        self.head= nn.Sequential(nn.Linear(h,h), nn.ReLU(), nn.Dropout(p), nn.Linear(h,1))\n    def forward(self, x):\n        _,hN = self.gru(x); return self.head(hN[-1]).squeeze(-1)\n\ndef mae(y,yhat): return float(np.mean(np.abs(np.asarray(y)-np.asarray(yhat))))\ndef smape(y,yhat,eps=1e-8):\n    y=np.asarray(y); yhat=np.asarray(yhat); return float(np.mean(2*np.abs(y-yhat)/(np.abs(y)+np.abs(yhat)+eps)))\n\ndef train_eval_one_ticker(df_t, df_v, feats, T=64, epochs=10, lr=1e-3, batch=128, device=\"cpu\"):\n    tr_ds = WindowedDataset(df_t, feats, T=T); va_ds = WindowedDataset(df_v, feats, T=T, scaler=tr_ds.scaler)\n    tr_ld = DataLoader(tr_ds, batch_size=batch, shuffle=True, drop_last=True)\n    va_ld = DataLoader(va_ds, batch_size=batch, shuffle=False)\n    net = GRUReg(len(feats)).to(device); opt = AdamW(net.parameters(), lr=lr, weight_decay=1e-5)\n    scaler = GradScaler(enabled=(device==\"cuda\"))\n    best=1e9\n    for ep in range(1, epochs+1):\n        net.train()\n        for xb,yb in tr_ld:\n            xb=xb.to(device).float(); yb=yb.to(device).float()\n            opt.zero_grad(set_to_none=True)\n            with autocast(enabled=(device==\"cuda\"), dtype=torch.float16):\n                yhat = net(xb); loss = torch.mean(torch.abs(yb - yhat))\n            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n        # quick val\n        net.eval(); preds=[]; ys=[]\n        with torch.no_grad():\n            for xb,yb in va_ld:\n                xb=xb.to(device).float(); yb=yb.to(device).float()\n                preds.append(net(xb).cpu().numpy()); ys.append(yb.cpu().numpy())\n        y = np.concatenate(ys); yhat = np.concatenate(preds)\n        best = min(best, mae(y,yhat))\n    return best, smape(y,yhat)\n\ndef main():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    f_static = Path(\"data/processed/features_v1_static.parquet\")\n    df = pd.read_parquet(f_static if f_static.exists() else \"data/processed/features_v1.parquet\").sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    CAND = [\"log_return\",\"lag1\",\"lag2\",\"lag3\",\"zscore_20\",\"roll_std_20\"]\n    feats = [c for c in CAND if c in df.columns]; assert \"r_1d\" in df.columns\n    splits = make_splits(df[\"date\"]); a,b,c,d = splits[0]\n    tr_all = df[(df[\"date\"]&gt;=a)&(df[\"date\"]&lt;=b)]; va_all = df[(df[\"date\"]&gt;=c)&(df[\"date\"]&lt;=d)]\n    tickers = tr_all[\"ticker\"].astype(str).unique().tolist()\n    rows=[]\n    for t in tickers:\n        tr = tr_all[tr_all[\"ticker\"]==t]; va = va_all[va_all[\"ticker\"]==t]\n        if len(tr)&lt;100 or len(va)&lt;20: continue\n        vmae, vsmape = train_eval_one_ticker(tr, va, feats, T=64, epochs=10, device=device)\n        rows.append({\"ticker\":t, \"model\":\"per_ticker_gru\", \"val_mae\":vmae, \"val_smape\":vsmape})\n    per_ticker = pd.DataFrame(rows)\n    # Load unified results written in class\n    uni_pt = pd.read_csv(\"reports/unified_gru_split1_per_ticker.csv\").rename(columns={\"mae\":\"val_mae\",\"smape\":\"val_smape\"})\n    uni_pt[\"model\"] = \"unified_gru_id\"\n    # Join and compare\n    comp = per_ticker.merge(uni_pt[[\"ticker\",\"model\",\"val_mae\",\"val_smape\"]], on=\"ticker\", how=\"outer\", suffixes=(\"_per\",\"_uni\"))\n    comp.to_csv(\"reports/unified_vs_per_ticker_split1.csv\", index=False)\n    print(\"Wrote reports/unified_vs_per_ticker_split1.csv\")\n\nif __name__ == \"__main__\":\n    main()\nRun:\n%%bash\nchmod +x scripts/compare_unified_vs_per_ticker.py\npython scripts/compare_unified_vs_per_ticker.py\n\n\n20.6.2 Part B — Add a Makefile target and a minimal test\nAppend to Makefile:\n.PHONY: train-unified compare-unified\ntrain-unified: ## Train unified GRU with ticker embeddings on split 1\n\\tpython - &lt;&lt;'PY'\nfrom pathlib import Path\nimport runpy\nrunpy.run_module('scripts.__init__', run_name='__main__') if Path('scripts/__init__.py').exists() else None\nPY\n\ncompare-unified: ## Compare unified vs per-ticker baselines\n\\tpython scripts/compare_unified_vs_per_ticker.py\n(If you prefer, replace train-unified with a thin wrapper to re-run the in‑class cells as a script.)\nTest: ensure per‑ticker comparison file exists and has required columns.\n# tests/test_unified_outputs.py\nimport os, pandas as pd\n\ndef test_unified_vs_per_ticker_table():\n    assert os.path.exists(\"reports/unified_vs_per_ticker_split1.csv\")\n    df = pd.read_csv(\"reports/unified_vs_per_ticker_split1.csv\")\n    need = {\"ticker\",\"val_mae_per\",\"val_mae_uni\"}\n    assert need.issubset(df.columns)\nRun:\n%%bash\npytest -q -k unified_outputs\n\n\n20.6.3 Part C — Add a short paragraph + table to your Quarto report\nIn reports/eda.qmd (or a new reports/unified.qmd), add:\n## Unified vs Per‑Ticker\n\nWe trained a **unified GRU with ticker embeddings** and compared it to **per‑ticker GRUs** on Split 1.\n\n::: {#f2249310 .cell execution_count=1}\n````` {.python .cell-code}\nimport pandas as pd\npd.read_csv(\"reports/unified_vs_per_ticker_split1.csv\").head(10)\nNote. Unified models can outperform when data per asset is limited, but may underperform on idiosyncratic assets. Keep the embedding small to limit overfitting. ````` :::",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Session 20 — Multi‑asset training (unified model</span>"
    ]
  },
  {
    "objectID": "lec20.html#instructor-notes-gotchas",
    "href": "lec20.html#instructor-notes-gotchas",
    "title": "20  Session 20 — Multi‑asset training (unified model",
    "section": "20.7 Instructor notes / gotchas",
    "text": "20.7 Instructor notes / gotchas\n\nEmbedding dimension: start small (8–16). Larger dims can overfit and memorize IDs.\n\nScaler: Fit on TRAIN only; reuse on VAL. It prevents target leakage through normalization.\n\nPer‑ticker metrics: Always report them to avoid hiding a few failing assets inside a good aggregate.\n\nSector embedding (optional): If you have sector from Session 12, set d_sector&gt;0 in the model and pass sector_ids from the dataset.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Session 20 — Multi‑asset training (unified model</span>"
    ]
  },
  {
    "objectID": "lec20.html#grading-passrevise",
    "href": "lec20.html#grading-passrevise",
    "title": "20  Session 20 — Multi‑asset training (unified model",
    "section": "20.8 Grading (pass/revise)",
    "text": "20.8 Grading (pass/revise)\n\nUnified model with ticker embeddings trains and writes:\n\nmodels/unified_gru_split1.pt\n\nreports/unified_gru_split1_metrics.csv (overall)\n\nreports/unified_gru_split1_per_ticker.csv (per‑ticker)\n\n\nComparison table reports/unified_vs_per_ticker_split1.csv exists.\n\nQuarto report updated with the comparison and a short discussion.\n\n\n\n20.8.1 Optional extensions (if students finish early)\n\nAblate embedding size: d=0 (no ID) vs 8 vs 16.\n\nAdd dropout on embeddings or L2 weight decay on embedding parameters only.\n\nTry concatenating embeddings at the head (after GRU) instead of at input, and compare.\n\nTry sector embeddings (d_sector=4) if you have sector metadata.\n\nYou now have a single multi‑asset model with tickers encoded as embeddings—the foundation for Session 21, where you’ll implement attention and a tiny GPT on toy data before adapting it to time series. ```",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Session 20 — Multi‑asset training (unified model</span>"
    ]
  },
  {
    "objectID": "lec21.html",
    "href": "lec21.html",
    "title": "21  Session 21 — Attention & Tiny GPT on Toy Data",
    "section": "",
    "text": "21.1 Session 21 — Attention & Tiny GPT on Toy Data (75 min)\nBelow is a complete lecture package for Session 21 — Attention & Tiny GPT on Toy Data (75 minutes). It includes a timed agenda, slide talking points, a Colab‑friendly in‑class lab with copy‑paste code, and homework with copy‑paste code. In class you’ll implement a tiny GPT (token + positional embeddings → stacked Transformer blocks → LM head) and verify learning on a toy character‑level next‑token task.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Session 21 — Attention & Tiny GPT on Toy Data</span>"
    ]
  },
  {
    "objectID": "lec21.html#session-21-attention-tiny-gpt-on-toy-data-75-min",
    "href": "lec21.html#session-21-attention-tiny-gpt-on-toy-data-75-min",
    "title": "21  Session 21 — Attention & Tiny GPT on Toy Data",
    "section": "",
    "text": "21.1.1 Learning goals\nBy the end of class, students can:\n\nExplain the core pieces of GPT: token embeddings, positional embeddings, multi‑head causal self‑attention, MLP, residual + LayerNorm.\nImplement a causal mask (no look‑ahead).\nTrain a tiny GPT (≈100–300k parameters) on a toy corpus and monitor loss/perplexity.\nGenerate text samples and diagnose calibration/overfit qualitatively.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Session 21 — Attention & Tiny GPT on Toy Data</span>"
    ]
  },
  {
    "objectID": "lec21.html#agenda-75-min",
    "href": "lec21.html#agenda-75-min",
    "title": "21  Session 21 — Attention & Tiny GPT on Toy Data",
    "section": "21.2 Agenda (75 min)",
    "text": "21.2 Agenda (75 min)\n\n(12 min) Slides: attention recap; Transformer block anatomy; causal masking\n(8 min) Slides: next‑token objective, cross‑entropy, perplexity; weight tying; tiny configs\n(35 min) In‑class lab: build TinyGPT → train on toy corpus → plot train/val loss → sample text\n(10 min) Wrap‑up + homework brief\n(10 min) Buffer / Q&A",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Session 21 — Attention & Tiny GPT on Toy Data</span>"
    ]
  },
  {
    "objectID": "lec21.html#slides-talking-points-drop-into-your-deck",
    "href": "lec21.html#slides-talking-points-drop-into-your-deck",
    "title": "21  Session 21 — Attention & Tiny GPT on Toy Data",
    "section": "21.3 Slides / talking points (drop into your deck)",
    "text": "21.3 Slides / talking points (drop into your deck)\n\n21.3.1 Scaled dot‑product attention (single head)\n\nQueries Q, keys K, values V from input X via linear projections.\nScores = \\(QK^\\top / \\sqrt{d_k}\\); apply causal mask to set future positions to \\(-\\infty\\); softmax over last dim; output \\(A = \\text{softmax}(\\text{masked scores})V\\).\n\n\n\n21.3.2 Multi‑head + block\n\nParallel heads capture different patterns; concat and project.\nPre‑norm block (LayerNorm → Attention → residual; LayerNorm → MLP → residual) is stable and common.\n\n\n\n21.3.3 Next‑token objective\n\nGiven context of length T, predict token \\(x_{t+1}\\) from \\(x_{\\le t}\\) using cross‑entropy.\nReport loss and perplexity \\(=\\exp(\\text{loss})\\).\n\n\n\n21.3.4 Tiny GPT config (fits in Colab/CPU)\n\nd_model=64, n_head=2, n_layer=2, ffn=128, ctx=64, batch 64–128, ~1–3 minutes on CPU (faster on GPU).",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Session 21 — Attention & Tiny GPT on Toy Data</span>"
    ]
  },
  {
    "objectID": "lec21.html#inclass-lab-35-min-colabfriendly",
    "href": "lec21.html#inclass-lab-35-min-colabfriendly",
    "title": "21  Session 21 — Attention & Tiny GPT on Toy Data",
    "section": "21.4 In‑class lab (35 min, Colab‑friendly)",
    "text": "21.4 In‑class lab (35 min, Colab‑friendly)\n\nRun each block as its own cell. Update REPO_NAME to your repo.\n\n\n21.4.1 0) Setup & folders\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME = \"unified-stocks-teamX\"  # &lt;- change if needed\nBASE_DIR  = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR  = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, sys, platform, random, math, time\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"models\",\"reports\",\"scripts\",\"tests\",\"data/raw\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd(), \"| Python\", sys.version.split()[0], \"| OS\", platform.system())\n\n\n21.4.2 1) Tiny toy corpus (char‑level), encoding, and splits\nimport numpy as np, pandas as pd, torch\n\n# Try to load an existing text; else use a small public‑domain‑friendly snippet\ntxt_path = pathlib.Path(\"data/raw/tiny_text.txt\")\nif txt_path.exists():\n    text = txt_path.read_text(encoding=\"utf-8\")\nelse:\n    text = (\n        \"To be, or not to be, that is the question:\\n\"\n        \"Whether 'tis nobler in the mind to suffer\\n\"\n        \"The slings and arrows of outrageous fortune,\\n\"\n        \"Or to take arms against a sea of troubles\\n\"\n        \"And by opposing end them.\\n\"\n    ) * 50  # repeat to make a few thousand chars\n\n# Build vocabulary\nchars = sorted(list(set(text)))\nstoi  = {ch:i for i,ch in enumerate(chars)}\nitos  = {i:ch for ch,i in stoi.items()}\nvocab_size = len(chars)\nprint(\"Vocab size:\", vocab_size, \"| Text length:\", len(text))\n\ndef encode(s: str): return torch.tensor([stoi[c] for c in s], dtype=torch.long)\ndef decode(t: torch.Tensor): return \"\".join(itos[int(i)] for i in t)\n\n# Train/val split (90/10 by characters)\ndata = encode(text)\nsplit = int(0.9 * len(data))\ntrain_data = data[:split]\nval_data   = data[split:]\nlen(train_data), len(val_data)\n\n\n21.4.3 2) Batch generator (random contiguous chunks)\ntorch.manual_seed(1337); np.random.seed(1337); random.seed(1337)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nblock_size = 64   # context length\nbatch_size = 128  # reduce to 64 if CPU is slow\n\ndef get_batch(split=\"train\"):\n    src = train_data if split==\"train\" else val_data\n    ix = torch.randint(low=0, high=len(src) - block_size - 1, size=(batch_size,))\n    x  = torch.stack([src[i:i+block_size] for i in ix])\n    y  = torch.stack([src[i+1:i+block_size+1] for i in ix])\n    return x.to(device), y.to(device)\n\nxb, yb = get_batch(\"train\")\nxb.shape, yb.shape\n\n\n21.4.4 3) Tiny GPT model (causal mask, multi‑head attention, pre‑norm)\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, d_model: int, n_head: int, dropout: float = 0.0):\n        super().__init__()\n        assert d_model % n_head == 0\n        self.n_head = n_head\n        self.d_head = d_model // n_head\n        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n        self.proj = nn.Linear(d_model, d_model, bias=False)\n        self.attn_drop = nn.Dropout(dropout)\n        self.resid_drop = nn.Dropout(dropout)\n        # Causal mask buffer (max at runtime = block_size we train with)\n        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)).unsqueeze(0).unsqueeze(0))\n    def forward(self, x):\n        B, T, C = x.size()\n        qkv = self.qkv(x)                            # (B,T,3C)\n        q, k, v = qkv.split(C, dim=2)                # (B,T,C) each\n        # reshape to heads\n        q = q.view(B, T, self.n_head, self.d_head).transpose(1,2)  # (B,H,T,d)\n        k = k.view(B, T, self.n_head, self.d_head).transpose(1,2)\n        v = v.view(B, T, self.n_head, self.d_head).transpose(1,2)\n        # attention scores\n        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_head)   # (B,H,T,T)\n        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float(\"-inf\"))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_drop(att)\n        y = att @ v                                                # (B,H,T,d)\n        y = y.transpose(1,2).contiguous().view(B, T, C)            # (B,T,C)\n        y = self.resid_drop(self.proj(y))\n        return y\n\nclass MLP(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout),\n        )\n    def forward(self, x): return self.net(x)\n\nclass Block(nn.Module):\n    def __init__(self, d_model: int, n_head: int, d_ff: int, dropout: float = 0.0):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(d_model)\n        self.attn = CausalSelfAttention(d_model, n_head, dropout)\n        self.ln2 = nn.LayerNorm(d_model)\n        self.mlp = MLP(d_model, d_ff, dropout)\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\nclass TinyGPT(nn.Module):\n    def __init__(self, vocab_size: int, d_model=64, n_head=2, n_layer=2, d_ff=128, block_size=64, dropout=0.0):\n        super().__init__()\n        self.block_size = block_size\n        self.tok_emb = nn.Embedding(vocab_size, d_model)\n        self.pos_emb = nn.Embedding(block_size, d_model)\n        self.blocks  = nn.ModuleList([Block(d_model, n_head, d_ff, dropout) for _ in range(n_layer)])\n        self.ln_f    = nn.LayerNorm(d_model)\n        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n        # Weight tying (common trick)\n        self.lm_head.weight = self.tok_emb.weight\n        # Init\n        self.apply(self._init_weights)\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias is not None: nn.init.zeros_(m.bias)\n        elif isinstance(m, nn.Embedding):\n            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n    def forward(self, idx, targets=None):\n        B, T = idx.size()\n        assert T &lt;= self.block_size, \"Sequence length &gt; block size\"\n        pos = torch.arange(0, T, device=idx.device)\n        x = self.tok_emb(idx) + self.pos_emb(pos)[None, :, :]\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)                      # (B,T,vocab)\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(B*T, -1), targets.view(B*T))\n        return logits, loss\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens=100, temperature=1.0, top_k=None):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -self.block_size:]      # crop to block size\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :] / temperature   # last time step\n            if top_k is not None:\n                v, _ = torch.topk(logits, top_k)\n                logits[logits &lt; v[:, [-1]]] = -float(\"inf\")\n            probs = F.softmax(logits, dim=-1)\n            next_id = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat([idx, next_id], dim=1)\n        return idx\n\n# Instantiate model\nd_model, n_head, n_layer, d_ff = 64, 2, 2, 128\nmodel = TinyGPT(vocab_size, d_model, n_head, n_layer, d_ff, block_size, dropout=0.0).to(device)\nsum(p.numel() for p in model.parameters())/1e6\n\n\n21.4.5 4) Training loop (cross‑entropy), eval, and sampling\nfrom torch.optim import AdamW\nfrom math import exp\n\nmax_steps     = 1200          # keep small for class; ~1–3 min on CPU\neval_interval = 100\nlr            = 3e-3\ntorch.manual_seed(1337)\noptimizer = AdamW(model.parameters(), lr=lr, betas=(0.9, 0.99), weight_decay=0.01)\n\n@torch.no_grad()\ndef estimate_loss(iters=200):\n    model.eval()\n    out = {}\n    for split in [\"train\",\"val\"]:\n        losses = torch.zeros(iters, device=device)\n        for k in range(iters):\n            xb, yb = get_batch(split)\n            _, loss = model(xb, yb)\n            losses[k] = loss\n        out[split] = losses.mean().item()\n    model.train()\n    return out\n\nloss_history = []\nt0 = time.time()\nfor step in range(1, max_steps+1):\n    xb, yb = get_batch(\"train\")\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    optimizer.step()\n\n    if step % eval_interval == 0 or step == 1 or step == max_steps:\n        est = estimate_loss(50)\n        loss_history.append({\"step\": step, \"train_loss\": est[\"train\"], \"val_loss\": est[\"val\"],\n                             \"train_ppl\": exp(est[\"train\"]), \"val_ppl\": exp(est[\"val\"])})\n        print(f\"step {step:4d} | train loss {est['train']:.3f} (ppl {exp(est['train']):.1f}) \"\n              f\"| val loss {est['val']:.3f} (ppl {exp(est['val']):.1f})\")\n\nprint(f\"Done in {time.time()-t0:.1f}s\")\npd.DataFrame(loss_history).to_csv(\"reports/tinygpt_train_curve.csv\", index=False)\n\n21.4.5.1 Quick plots and sampling\nimport matplotlib.pyplot as plt, pandas as pd\ndf = pd.DataFrame(loss_history)\nplt.figure(figsize=(6,3.5))\nplt.plot(df[\"step\"], df[\"train_loss\"], marker=\"o\")\nplt.plot(df[\"step\"], df[\"val_loss\"], marker=\"s\")\nplt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.title(\"TinyGPT train/val loss\"); plt.tight_layout()\nplt.show()\n\n# Sample text\nstart_prompt = \"To be\"\nstart_ids = encode(start_prompt)[None, :].to(device)\nsample_ids = model.generate(start_ids, max_new_tokens=200, temperature=0.8, top_k=40)\nprint(decode(sample_ids[0].tolist()))\n\nSanity checks: • Loss should fall and val perplexity should be reasonable (&gt;1, &lt; vocab_size). • Generated text should mimic the corpus style (not necessarily coherent).",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Session 21 — Attention & Tiny GPT on Toy Data</span>"
    ]
  },
  {
    "objectID": "lec21.html#wrapup-10-min-key-points-to-emphasize",
    "href": "lec21.html#wrapup-10-min-key-points-to-emphasize",
    "title": "21  Session 21 — Attention & Tiny GPT on Toy Data",
    "section": "21.5 Wrap‑up (10 min) — key points to emphasize",
    "text": "21.5 Wrap‑up (10 min) — key points to emphasize\n\nCausal mask prevents look‑ahead; this mirrors leakage controls from Sessions 17–18.\nWeight tying reduces params and often helps.\nPositional embeddings are required for order; we used simple learned absolute positions today.\nThis tiny model is a teaching scaffold—Session 22 adapts the embedding to real‑valued features for time‑series forecasting.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Session 21 — Attention & Tiny GPT on Toy Data</span>"
    ]
  },
  {
    "objectID": "lec21.html#homework-due-before-session-22",
    "href": "lec21.html#homework-due-before-session-22",
    "title": "21  Session 21 — Attention & Tiny GPT on Toy Data",
    "section": "21.6 Homework (due before Session 22)",
    "text": "21.6 Homework (due before Session 22)\nGoal: Explore how depth (layers), width (heads/hidden), and context length affect tiny GPT’s learning on the toy corpus. Produce a one‑page table and a short reflection (≈300–500 words).\n\n21.6.1 A. Script: scripts/tinygpt_train.py (configurable tiny GPT)\n#!/usr/bin/env python\nfrom __future__ import annotations\nimport argparse, time, math, random, pathlib\nimport numpy as np, torch, torch.nn as nn\nfrom torch.nn import functional as F\nimport pandas as pd\n\n# ---- Model components from class (compact) ----\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, d_model, n_head, ctx, p=0.0):\n        super().__init__()\n        assert d_model % n_head == 0\n        self.n_head = n_head; self.d_head = d_model // n_head\n        self.qkv = nn.Linear(d_model, 3*d_model, bias=False)\n        self.proj = nn.Linear(d_model, d_model, bias=False)\n        self.attn_drop = nn.Dropout(p); self.resid_drop = nn.Dropout(p)\n        self.register_buffer(\"mask\", torch.tril(torch.ones(ctx, ctx)).unsqueeze(0).unsqueeze(0))\n    def forward(self, x):\n        B,T,C=x.size()\n        qkv=self.qkv(x); q,k,v=qkv.split(C,dim=2)\n        q=q.view(B,T,self.n_head,self.d_head).transpose(1,2)\n        k=k.view(B,T,self.n_head,self.d_head).transpose(1,2)\n        v=v.view(B,T,self.n_head,self.d_head).transpose(1,2)\n        att=(q @ k.transpose(-2,-1)) / math.sqrt(self.d_head)\n        att=att.masked_fill(self.mask[:,:,:T,:T]==0, float(\"-inf\"))\n        att=att.softmax(dim=-1); att=self.attn_drop(att)\n        y=att @ v; y=y.transpose(1,2).contiguous().view(B,T,C)\n        return self.resid_drop(self.proj(y))\n\nclass Block(nn.Module):\n    def __init__(self, d_model, n_head, d_ff, ctx, p=0.0):\n        super().__init__()\n        self.ln1=nn.LayerNorm(d_model); self.attn=CausalSelfAttention(d_model,n_head,ctx,p)\n        self.ln2=nn.LayerNorm(d_model); self.mlp=nn.Sequential(nn.Linear(d_model,d_ff), nn.GELU(), nn.Dropout(p), nn.Linear(d_ff,d_model), nn.Dropout(p))\n    def forward(self,x):\n        x=x+self.attn(self.ln1(x)); x=x+self.mlp(self.ln2(x)); return x\n\nclass TinyGPT(nn.Module):\n    def __init__(self, vocab, d_model=64, n_head=2, n_layer=2, d_ff=128, ctx=64, p=0.0):\n        super().__init__()\n        self.ctx=ctx\n        self.tok=nn.Embedding(vocab,d_model); self.pos=nn.Embedding(ctx,d_model)\n        self.blocks=nn.ModuleList([Block(d_model,n_head,d_ff,ctx,p) for _ in range(n_layer)])\n        self.ln=nn.LayerNorm(d_model); self.head=nn.Linear(d_model,vocab,bias=False)\n        self.head.weight = self.tok.weight\n        self.apply(self._init)\n    def _init(self,m):\n        if isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight); \n        if isinstance(m, nn.Embedding): nn.init.normal_(m.weight, 0.0, 0.02)\n    def forward(self, idx, targets=None):\n        B,T=idx.size(); pos=torch.arange(T, device=idx.device)\n        x=self.tok(idx)+self.pos(pos)[None,:,:]\n        for b in self.blocks: x=b(x)\n        x=self.ln(x); logits=self.head(x)\n        loss=None\n        if targets is not None: loss=F.cross_entropy(logits.view(B*T,-1), targets.view(B*T))\n        return logits, loss\n\ndef main():\n    ap=argparse.ArgumentParser()\n    ap.add_argument(\"--text\", default=\"data/raw/tiny_text.txt\")\n    ap.add_argument(\"--d_model\", type=int, default=64)\n    ap.add_argument(\"--n_head\", type=int, default=2)\n    ap.add_argument(\"--n_layer\", type=int, default=2)\n    ap.add_argument(\"--d_ff\", type=int, default=128)\n    ap.add_argument(\"--ctx\", type=int, default=64)\n    ap.add_argument(\"--batch\", type=int, default=128)\n    ap.add_argument(\"--steps\", type=int, default=1200)\n    ap.add_argument(\"--lr\", type=float, default=3e-3)\n    ap.add_argument(\"--seed\", type=int, default=1337)\n    ap.add_argument(\"--out\", default=\"reports/tinygpt_run.csv\")\n    args=ap.parse_args()\n\n    # Load or synthesize text\n    p=pathlib.Path(args.text)\n    if p.exists(): text=p.read_text(encoding=\"utf-8\")\n    else:\n        text=(\"To be, or not to be: that is the question.\\n\")*50\n        pathlib.Path(p.parent).mkdir(parents=True, exist_ok=True); p.write_text(text)\n\n    # Vocab/encode\n    chars=sorted(list(set(text))); stoi={c:i for i,c in enumerate(chars)}; itos={i:c for c,i in stoi.items()}\n    def enc(s): return torch.tensor([stoi[c] for c in s], dtype=torch.long)\n    data=enc(text); split=int(0.9*len(data)); train, val = data[:split], data[split:]\n\n    # Batching\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    random.seed(args.seed); np.random.seed(args.seed); torch.manual_seed(args.seed)\n    def batch(src):\n        ix=torch.randint(0, len(src)-args.ctx-1, (args.batch,))\n        x=torch.stack([src[i:i+args.ctx] for i in ix])\n        y=torch.stack([src[i+1:i+args.ctx+1] for i in ix])\n        return x.to(device), y.to(device)\n\n    # Model/opt\n    net=TinyGPT(vocab=len(chars), d_model=args.d_model, n_head=args.n_head, n_layer=args.n_layer,\n                d_ff=args.d_ff, ctx=args.ctx, p=0.0).to(device)\n    opt=torch.optim.AdamW(net.parameters(), lr=args.lr, betas=(0.9,0.99), weight_decay=0.01)\n\n    @torch.no_grad()\n    def eval_loss(iters=100):\n        net.eval(); import math\n        def run(src):\n            losses=[]\n            for _ in range(iters):\n                xb, yb = batch(src)\n                _, loss = net(xb, yb); losses.append(loss.item())\n            m=np.mean(losses); return m, math.exp(m)\n        tr, trp = run(train); va, vap = run(val); net.train()\n        return tr, trp, va, vap\n\n    hist=[]\n    for step in range(1, args.steps+1):\n        xb,yb = batch(train)\n        _,loss = net(xb,yb)\n        opt.zero_grad(set_to_none=True); loss.backward()\n        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n        opt.step()\n        if step%100==0 or step==1 or step==args.steps:\n            tr,trp,va,vap = eval_loss(50)\n            hist.append({\"step\":step,\"train_loss\":tr,\"train_ppl\":trp,\"val_loss\":va,\"val_ppl\":vap,\n                         \"d_model\":args.d_model,\"n_head\":args.n_head,\"n_layer\":args.n_layer,\n                         \"ctx\":args.ctx})\n            print(f\"step {step:4d}  train {tr:.3f} (ppl {trp:.1f}) | val {va:.3f} (ppl {vap:.1f})\")\n\n    pd.DataFrame(hist).to_csv(args.out, index=False)\n    print(\"Wrote\", args.out)\n\nif __name__ == \"__main__\":\n    main()\nRun examples:\n%%bash\nchmod +x scripts/tinygpt_train.py\npython scripts/tinygpt_train.py --n_head 1 --n_layer 1 --ctx 32  --steps 600  --out reports/tinygpt_run_a.csv\npython scripts/tinygpt_train.py --n_head 2 --n_layer 2 --ctx 64  --steps 1200 --out reports/tinygpt_run_b.csv\n\n\n21.6.2 B. Summarize runs into a table for your report\nimport pandas as pd\nruns = []\nfor f in [\"reports/tinygpt_run_a.csv\",\"reports/tinygpt_run_b.csv\"]:\n    try:\n        df = pd.read_csv(f)\n        last = df.sort_values(\"step\").iloc[-1].to_dict()\n        last[\"run\"] = f\n        runs.append(last)\n    except Exception as e:\n        print(\"Skip\", f, e)\npd.DataFrame(runs).to_csv(\"reports/tinygpt_ablation_summary.csv\", index=False)\npd.read_csv(\"reports/tinygpt_ablation_summary.csv\")\n\n\n21.6.3 C. Reflection prompts (≈300–500 words)\n\nHow did context length (32 → 64) affect val perplexity?\nDid additional heads or layers help consistently? Where did overfitting show up?\nWhat happened to training speed and stability as you increased model size?\nOne paragraph on how you expect to adapt embeddings for time‑series in Session 22 (hint: replace token embedding with a linear projection of features; keep positional encodings and causal mask).\n\n\nSubmit reports/tinygpt_ablation_summary.csv and a short Quarto section with your table + reflection.\n\n\n\n21.6.4 D. (Optional) Tiny test for the causal mask\n# tests/test_causal_mask.py\nimport torch\nfrom scripts.tinygpt_train import TinyGPT\ndef test_mask_is_causal():\n    m = TinyGPT(vocab=10, ctx=16, d_model=32, n_head=2, n_layer=1)\n    # Find an attention module and check its mask upper triangle is zero\n    att = [mod.attn for mod in m.blocks][:1][0]\n    M = att.mask[0,0]  # (T,T)\n    assert torch.all(M.triu(diagonal=1)==0)\nRun:\n%%bash\npytest -q -k causal_mask",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Session 21 — Attention & Tiny GPT on Toy Data</span>"
    ]
  },
  {
    "objectID": "lec21.html#instructor-checklist-before-class",
    "href": "lec21.html#instructor-checklist-before-class",
    "title": "21  Session 21 — Attention & Tiny GPT on Toy Data",
    "section": "21.7 Instructor checklist (before class)",
    "text": "21.7 Instructor checklist (before class)\n\nDry‑run the in‑class script on CPU; confirm loss decreases within ~1–3 minutes.\nPrepare a slide showing attention mask visually (upper‑triangle masked).\n(Optional) Show a single head’s attention weights on a short prompt for intuition.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Session 21 — Attention & Tiny GPT on Toy Data</span>"
    ]
  },
  {
    "objectID": "lec21.html#emphasize-while-teaching",
    "href": "lec21.html#emphasize-while-teaching",
    "title": "21  Session 21 — Attention & Tiny GPT on Toy Data",
    "section": "21.8 Emphasize while teaching",
    "text": "21.8 Emphasize while teaching\n\nCausal masking is the Transformer’s built‑in “no leakage” rule—maps directly to Sessions 17–18.\nSmaller is fine: We use tiny configs to teach mechanics and keep run times short.\nNext session swaps token embeddings for real‑valued feature projections and a regression head.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Session 21 — Attention & Tiny GPT on Toy Data</span>"
    ]
  },
  {
    "objectID": "lec21.html#grading-passrevise",
    "href": "lec21.html#grading-passrevise",
    "title": "21  Session 21 — Attention & Tiny GPT on Toy Data",
    "section": "21.9 Grading (pass/revise)",
    "text": "21.9 Grading (pass/revise)\n\nTinyGPT trains; reports/tinygpt_train_curve.csv exists; curve shows decreasing val loss.\nStudent ran at least two configurations via scripts/tinygpt_train.py and produced reports/tinygpt_ablation_summary.csv.\nReflection answers the prompts with evidence from the runs.\n\nYou now have a working Tiny GPT on toy data. In Session 22, you’ll adapt this architecture to time‑series returns by replacing the token embedding with a linear projection of features and keeping the causal machinery.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Session 21 — Attention & Tiny GPT on Toy Data</span>"
    ]
  },
  {
    "objectID": "lec22.html",
    "href": "lec22.html",
    "title": "22  Session 22 — Adapting GPT to Time Series",
    "section": "",
    "text": "22.1 Session 22 — Adapting GPT to Time Series (75 min)\nBelow is a complete lecture package for Session 22 — Adapting GPT to Time Series (75 minutes). It includes a timed agenda, slide talking points, a Colab‑friendly in‑class lab with copy‑paste code, and homework with copy‑paste code. In class you’ll replace token embeddings with a linear projection of real‑valued features, keep positional embeddings and a causal mask, and train a tiny Transformer (GPT‑style) to predict next‑day return.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Session 22 — Adapting GPT to Time Series</span>"
    ]
  },
  {
    "objectID": "lec22.html#session-22-adapting-gpt-to-time-series-75-min",
    "href": "lec22.html#session-22-adapting-gpt-to-time-series-75-min",
    "title": "22  Session 22 — Adapting GPT to Time Series",
    "section": "",
    "text": "22.1.1 Learning goals\nBy the end of class, students can:\n\nConvert a window of real‑valued features into a sequence input for a GPT‑style model.\nUse positional embeddings and causal self‑attention for sequence‑to‑one regression (predict r_1d[t] from x[≤t]).\nTrain with MAE/Huber loss, early stopping, and AMP (optional on CUDA).\nSave reproducible artifacts (checkpoint + metrics CSV) and produce a quick loss plot.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Session 22 — Adapting GPT to Time Series</span>"
    ]
  },
  {
    "objectID": "lec22.html#agenda-75-min",
    "href": "lec22.html#agenda-75-min",
    "title": "22  Session 22 — Adapting GPT to Time Series",
    "section": "22.2 Agenda (75 min)",
    "text": "22.2 Agenda (75 min)\n\n(10 min) Slides: from tokens to real‑valued features; causal masking & alignment; loss choices\n(10 min) Slides: tiny TS‑GPT architecture (feature projector → pos emb → blocks → head)\n(35 min) In‑class lab: dataset → TS‑GPT → train/validate → save metrics & checkpoint → quick plots\n(10 min) Wrap‑up + homework brief\n(10 min) Buffer / Q&A",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Session 22 — Adapting GPT to Time Series</span>"
    ]
  },
  {
    "objectID": "lec22.html#slide-talking-points-drop-into-your-deck",
    "href": "lec22.html#slide-talking-points-drop-into-your-deck",
    "title": "22  Session 22 — Adapting GPT to Time Series",
    "section": "22.3 Slide talking points (drop into your deck)",
    "text": "22.3 Slide talking points (drop into your deck)\n\n22.3.1 1) From characters to features\n\nChar‑GPT maps discrete tokens → embeddings.\nTS‑GPT maps real‑valued features x_t∈ℝ^F → ℝ^d_model via a linear projection at each time step:\n\\[\nh_t^{(0)} = W_\\text{proj} x_t + b + \\text{pos\\_emb}(t).\n\\]\n\n\n\n22.3.2 2) Labels & alignment (no leakage)\n\nInput window: \\(X_{t-T+1:t}\\) (length \\(T\\)).\nTarget: \\(y_t = r\\_1d[t]\\) (next‑day log return), computed outside the model and shifted by −1.\nCausal mask ensures the block at step \\(t\\) cannot see \\(&gt;t\\).\n\n\n\n22.3.3 3) Loss & scaling\n\nLoss: MAE or Huber (robust to outliers in returns); report MAE and sMAPE.\nScaling: Fit a feature scaler on TRAIN only, reuse for VAL/TEST.\n\n\n\n22.3.4 4) Tiny TS‑GPT config (Colab‑friendly)\n\nd_model=64, n_head=2, n_layer=2, d_ff=128, ctx=64, dropout 0.0–0.1, batch 128–256.\nSave: models/tsgpt_split1.pt and reports/tsgpt_split1_metrics.csv.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Session 22 — Adapting GPT to Time Series</span>"
    ]
  },
  {
    "objectID": "lec22.html#inclass-lab-35-min-colabfriendly",
    "href": "lec22.html#inclass-lab-35-min-colabfriendly",
    "title": "22  Session 22 — Adapting GPT to Time Series",
    "section": "22.4 In‑class lab (35 min, Colab‑friendly)",
    "text": "22.4 In‑class lab (35 min, Colab‑friendly)\n\nRun each block as its own cell. Update REPO_NAME if needed.\n\n\n22.4.1 0) Setup & device\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME = \"unified-stocks-teamX\"  # &lt;- change if needed\nBASE_DIR  = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR  = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, sys, platform, random, math, time\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/processed\",\"models\",\"reports\",\"scripts\",\"tests\",\"docs/figs\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd(), \"| Python\", sys.version.split()[0], \"| OS\", platform.system())\n\nimport numpy as np, pandas as pd, torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available(), \"| Device:\", device)\n\ndef seed_everything(seed=2222):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\nseed_everything()\n\n\n22.4.2 1) Load features; build split; TRAIN‑fit scaler (with fallbacks)\nfrom pathlib import Path\n\n# Prefer static universe from Session 17\nf_static = Path(\"data/processed/features_v1_static.parquet\")\nf_base   = Path(\"data/processed/features_v1.parquet\")\n\nif f_static.exists():\n    df = pd.read_parquet(f_static)\nelif f_base.exists():\n    df = pd.read_parquet(f_base)\nelse:\n    # Fallback from returns; add minimal features\n    rpath = Path(\"data/processed/returns.parquet\")\n    if rpath.exists():\n        df = pd.read_parquet(rpath).sort_values([\"ticker\",\"date\"])\n    else:\n        # synthesize small dataset\n        rng = np.random.default_rng(0)\n        dates = pd.bdate_range(\"2022-01-03\", periods=320)\n        frames=[]\n        for t in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\",\"META\"]:\n            eps = rng.normal(0, 0.012, size=len(dates)).astype(\"float32\")\n            adj = 100*np.exp(np.cumsum(eps))\n            g = pd.DataFrame({\n                \"date\": dates, \"ticker\": t,\n                \"adj_close\": adj.astype(\"float32\"),\n                \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n            })\n            g[\"r_1d\"] = g[\"log_return\"].shift(-1)\n            frames.append(g)\n        df = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)\n    # add basic features\n    df = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    for k in [1,2,3]:\n        df[f\"lag{k}\"] = df.groupby(\"ticker\")[\"log_return\"].shift(k)\n    df[\"roll_std_20\"]  = df.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)\n    df[\"zscore_20\"]    = (df[\"log_return\"] - df.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).mean().reset_index(level=0, drop=True)) / (df[\"roll_std_20\"] + 1e-8)\n    df = df.dropna().reset_index(drop=True)\n\n# Harmonize and subset for speed\ndf[\"date\"] = pd.to_datetime(df[\"date\"]); df[\"ticker\"] = df[\"ticker\"].astype(\"category\")\ndf = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nkeep = df[\"ticker\"].cat.categories.tolist()[:10]  # up to 10 tickers for class\ndf = df[df[\"ticker\"].isin(keep)].copy()\n\n# Choose features (keep small & causal)\nCAND = [\"log_return\",\"lag1\",\"lag2\",\"lag3\",\"zscore_20\",\"roll_std_20\",\"weekday\",\"month\"]\nFEATS = [c for c in CAND if c in df.columns]\nassert \"r_1d\" in df.columns and len(FEATS)&gt;0, \"Missing label or features.\"\n\ndef make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    i=train_min-1; out=[]\n    while True:\n        if i&gt;=len(u): break\n        a,b=u[0],u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve&gt;=len(u): break\n        out.append((a,b,u[vs],u[ve])); i+=step\n    return out\n\nsplits = make_rolling_origin_splits(df[\"date\"], 252, 63, 63, 5)\nassert splits, \"Not enough history.\"\na,b,c,d = splits[0]\ntrain_df = df[(df[\"date\"]&gt;=a)&(df[\"date\"]&lt;=b)].copy()\nval_df   = df[(df[\"date\"]&gt;=c)&(df[\"date\"]&lt;=d)].copy()\nprint(\"Split1: train\", a.date(), \"→\", b.date(), \"| val\", c.date(), \"→\", d.date(),\n      \"| train rows:\", len(train_df), \"| val rows:\", len(val_df))\n\n\n22.4.3 2) Dataset with feature projection targetting r_1d\nfrom torch.utils.data import Dataset, DataLoader\nimport json\n\nclass FeatureScaler:\n    def __init__(self): self.mean_=None; self.std_=None\n    def fit(self, X): self.mean_=X.mean(0, dtype=np.float64); self.std_=X.std(0, dtype=np.float64)+1e-8; return self\n    def transform(self, X): return (X - self.mean_) / self.std_\n    def state_dict(self): return {\"mean\": self.mean_.tolist(), \"std\": self.std_.tolist()}\n    def load_state_dict(self, d): self.mean_=np.array(d[\"mean\"]); self.std_=np.array(d[\"std\"])\n\nclass WindowedTS(Dataset):\n    \"\"\"\n    Sliding windows per ticker. Each item:\n      X_scaled: (T, F), y: scalar r_1d at window end, ticker_id: long\n    \"\"\"\n    def __init__(self, frame: pd.DataFrame, feats, T=64, scaler: FeatureScaler|None=None,\n                 ticker2id: dict|None=None):\n        self.T = int(T); self.feats=list(feats); self.idx=[]; self.groups={}\n        # Build ticker ids (frozen on TRAIN)\n        if ticker2id is None:\n            cats = frame[\"ticker\"].astype(\"category\").cat.categories.tolist()\n            self.ticker2id = {t:i for i,t in enumerate(cats)}\n        else:\n            self.ticker2id = dict(ticker2id)\n        # Per‑ticker arrays and index of valid windows\n        for tkr, g in frame.groupby(\"ticker\"):\n            g = g.sort_values(\"date\").reset_index(drop=True)\n            X = g[self.feats].to_numpy(\"float32\")\n            y = g[\"r_1d\"].to_numpy(\"float32\")\n            tid = self.ticker2id[str(tkr)]\n            for end in range(self.T-1, len(g)):\n                if np.isfinite(y[end]): self.idx.append((str(tkr), end, tid))\n            self.groups[str(tkr)] = {\"X\": X, \"y\": y}\n        # Fit scaler on TRAIN if not provided\n        self.scaler = scaler or FeatureScaler().fit(np.concatenate([self.groups[t][\"X\"] for t in self.groups], 0))\n\n    def __len__(self): return len(self.idx)\n    def __getitem__(self, i):\n        tkr, end, tid = self.idx[i]\n        g = self.groups[tkr]\n        X = g[\"X\"][end-self.T+1:end+1]\n        X = self.scaler.transform(X)\n        y = g[\"y\"][end]\n        return torch.from_numpy(X), torch.tensor(y, dtype=torch.float32), torch.tensor(tid, dtype=torch.long)\n\n# Build TRAIN/VAL datasets & loaders\nCTX = 64; BATCH=256; WORKERS=2; PIN=torch.cuda.is_available()\n\ntrain_ds = WindowedTS(train_df, FEATS, T=CTX)\nval_ds   = WindowedTS(val_df,   FEATS, T=CTX, scaler=train_ds.scaler, ticker2id=train_ds.ticker2id)\n\n# Persist scaler (reproducibility)\nPath(\"models\").mkdir(exist_ok=True)\nPath(\"models/tsgpt_scaler_split1.json\").write_text(json.dumps(train_ds.scaler.state_dict()))\n\ndef _seed_worker(_):\n    ws = torch.initial_seed() % (2**32)\n    np.random.seed(ws); random.seed(ws)\n\ng = torch.Generator(); g.manual_seed(42)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, drop_last=True,\n                          num_workers=WORKERS, pin_memory=PIN, persistent_workers=(WORKERS&gt;0),\n                          worker_init_fn=_seed_worker, generator=g)\nval_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, drop_last=False,\n                          num_workers=WORKERS, pin_memory=PIN, persistent_workers=(WORKERS&gt;0),\n                          worker_init_fn=_seed_worker)\n\nlen(train_ds), len(val_ds), next(iter(train_loader))[0].shape\n\n\n22.4.4 3) Time‑Series GPT (feature projector + pos emb + Transformer blocks + regression head)\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nBLOCK_SIZE = CTX  # maximum context supported\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, d_model: int, n_head: int, dropout: float = 0.0):\n        super().__init__()\n        assert d_model % n_head == 0\n        self.n_head = n_head; self.d_head = d_model // n_head\n        self.qkv = nn.Linear(d_model, 3*d_model, bias=False)\n        self.proj = nn.Linear(d_model, d_model, bias=False)\n        self.attn_drop = nn.Dropout(dropout); self.resid_drop = nn.Dropout(dropout)\n        self.register_buffer(\"mask\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)).unsqueeze(0).unsqueeze(0))\n    def forward(self, x):\n        B,T,C = x.size()\n        qkv = self.qkv(x); q,k,v = qkv.split(C, dim=2)\n        q = q.view(B,T,self.n_head,self.d_head).transpose(1,2)\n        k = k.view(B,T,self.n_head,self.d_head).transpose(1,2)\n        v = v.view(B,T,self.n_head,self.d_head).transpose(1,2)\n        att = (q @ k.transpose(-2,-1)) / math.sqrt(self.d_head)\n        att = att.masked_fill(self.mask[:,:,:T,:T]==0, float(\"-inf\"))\n        att = F.softmax(att, dim=-1); att = self.attn_drop(att)\n        y = att @ v\n        y = y.transpose(1,2).contiguous().view(B,T,C)\n        return self.resid_drop(self.proj(y))\n\nclass Block(nn.Module):\n    def __init__(self, d_model: int, n_head: int, d_ff: int, dropout: float = 0.0):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(d_model); self.attn = CausalSelfAttention(d_model, n_head, dropout)\n        self.ln2 = nn.LayerNorm(d_model); self.mlp  = nn.Sequential(\n            nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_ff, d_model), nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\nclass TimeSeriesGPT(nn.Module):\n    \"\"\"\n    Real-valued sequence → regression (predict r_1d at last step).\n    \"\"\"\n    def __init__(self, in_features: int, d_model=64, n_head=2, n_layer=2, d_ff=128, ctx=64, dropout=0.0,\n                 n_tickers: int|None=None, d_ticker: int=0):\n        super().__init__()\n        self.ctx = ctx\n        self.proj = nn.Linear(in_features, d_model)        # FEATURE PROJECTION\n        self.pos  = nn.Embedding(ctx, d_model)             # POSITIONAL EMBEDDINGS\n        self.id_emb = nn.Embedding(n_tickers, d_ticker) if (n_tickers and d_ticker&gt;0) else None\n        augmented = d_model + (d_ticker if self.id_emb else 0)\n        self.blocks = nn.ModuleList([Block(augmented, n_head, d_ff, dropout) for _ in range(n_layer)])\n        self.ln = nn.LayerNorm(augmented)\n        self.head = nn.Linear(augmented, 1)                # REGRESSION HEAD\n\n        self.apply(self._init)\n    def _init(self, m):\n        if isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight); \n        if isinstance(m, nn.Embedding): nn.init.normal_(m.weight, 0.0, 0.02)\n    def forward(self, x, ticker_ids=None):\n        # x: (B,T,F)\n        B,T,F = x.size()\n        pos = torch.arange(T, device=x.device)\n        h = self.proj(x) + self.pos(pos)[None,:,:]         # (B,T,d_model)\n        if self.id_emb is not None and ticker_ids is not None:\n            e = self.id_emb(ticker_ids).unsqueeze(1).expand(-1, T, -1)   # (B,T,d_ticker)\n            h = torch.cat([h, e], dim=-1)                                 # (B,T,d_model+d_ticker)\n        for blk in self.blocks: h = blk(h)\n        h = self.ln(h)\n        yhat = self.head(h[:,-1,:]).squeeze(-1)            # use last time step’s hidden state\n        return yhat\n\n# Instantiate tiny model (no ticker embedding for baseline)\nD_MODEL, N_HEAD, N_LAYER, D_FF, DROPOUT = 64, 2, 2, 128, 0.0\nmodel = TimeSeriesGPT(in_features=len(FEATS), d_model=D_MODEL, n_head=N_HEAD,\n                      n_layer=N_LAYER, d_ff=D_FF, ctx=CTX, dropout=DROPOUT,\n                      n_tickers=None, d_ticker=0).to(device)\nsum(p.numel() for p in model.parameters())/1e6, device\n\n\n22.4.5 4) Train (MAE or Huber), early stop, checkpoint, and metrics\nfrom torch.optim import AdamW\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef mae_t(y, yhat): return torch.mean(torch.abs(y - yhat))\ndef smape_t(y, yhat, eps=1e-8): return torch.mean(2.0*torch.abs(y - yhat)/(torch.abs(y)+torch.abs(yhat)+eps))\n\nUSE_HUBER = True\nhuber_delta = 0.01\nhuber = torch.nn.HuberLoss(delta=huber_delta, reduction=\"mean\")\n\ndef train_one_epoch(model, loader, opt, scaler, device, use_amp=True):\n    model.train(); tot=0.0; n=0\n    for xb, yb, _ in loader:\n        xb = xb.to(device, non_blocking=True).float()\n        yb = yb.to(device, non_blocking=True).float()\n        opt.zero_grad(set_to_none=True)\n        if use_amp and device.type==\"cuda\":\n            with autocast(dtype=torch.float16):\n                yhat = model(xb)\n                loss = huber(yhat, yb) if USE_HUBER else mae_t(yb, yhat)\n            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n        else:\n            yhat = model(xb); loss = huber(yhat, yb) if USE_HUBER else mae_t(yb, yhat)\n            loss.backward(); opt.step()\n        bs = xb.size(0); tot += loss.item()*bs; n += bs\n    return tot/max(n,1)\n\n@torch.no_grad()\ndef evaluate(model, loader, device):\n    model.eval(); m_mae=m_smape=0.0; n=0\n    for xb, yb, _ in loader:\n        xb=xb.to(device, non_blocking=True).float()\n        yb=yb.to(device, non_blocking=True).float()\n        yhat = model(xb)\n        bs=xb.size(0)\n        m_mae += mae_t(yb, yhat).item()*bs\n        m_smape += smape_t(yb, yhat).item()*bs\n        n+=bs\n    return {\"mae\": m_mae/max(n,1), \"smape\": m_smape/max(n,1)}\n\ndef fit(model, train_loader, val_loader, epochs=12, lr=2e-3, wd=1e-5, patience=3, use_amp=True):\n    opt = AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=(0.9,0.99))\n    scaler = GradScaler(enabled=(use_amp and device.type==\"cuda\"))\n    best=float(\"inf\"); best_ep=-1; ckpt=Path(\"models/tsgpt_split1.pt\")\n    history=[]\n    for ep in range(1, epochs+1):\n        tr = train_one_epoch(model, train_loader, opt, scaler, device, use_amp)\n        val = evaluate(model, val_loader, device)\n        history.append({\"epoch\":ep,\"train_mae\":tr,\"val_mae\":val[\"mae\"],\"val_smape\":val[\"smape\"]})\n        print(f\"Epoch {ep:02d}  train_mae={tr:.5f}  val_mae={val['mae']:.5f}  val_sMAPE={val['smape']:.5f}\")\n        if val[\"mae\"] &lt; best - 1e-6:\n            best = val[\"mae\"]; best_ep=ep\n            torch.save({\"model\": model.state_dict(),\n                        \"epoch\": ep,\n                        \"config\": {\"ctx\":CTX,\"d_model\":D_MODEL,\"n_head\":N_HEAD,\"n_layer\":N_LAYER,\n                                   \"d_ff\":D_FF,\"dropout\":DROPOUT,\"feats\":FEATS,\"huber\":USE_HUBER}}, ckpt)\n        elif ep - best_ep &gt;= patience:\n            print(f\"Early stopping at epoch {ep} (best {best:.5f} @ {best_ep})\")\n            break\n    return history, best, best_ep, ckpt\n\nhistory, best, best_ep, ckpt = fit(model, train_loader, val_loader, epochs=10, lr=2e-3, wd=1e-5, patience=3, use_amp=True)\nprint(\"Best val_mae:\", best, \"| epoch:\", best_ep, \"| saved:\", ckpt.exists())\n\n# Save metrics CSV & quick plot data\npd.DataFrame(history).to_csv(\"reports/tsgpt_train_curve.csv\", index=False)\nfinal = evaluate(model, val_loader, device)\npd.DataFrame([{\"split\":1,\"model\":\"tsgpt\",\"ctx\":CTX,\"val_mae\":final[\"mae\"],\"val_smape\":final[\"smape\"],\n               \"params_M\": round(sum(p.numel() for p in model.parameters())/1e6, 3),\"best_epoch\":best_ep}]).to_csv(\n    \"reports/tsgpt_split1_metrics.csv\", index=False)\nfinal\n\n\n22.4.6 5) Quick loss plot (optional in‑class visualization)\nimport matplotlib.pyplot as plt, pandas as pd\ncur = pd.read_csv(\"reports/tsgpt_train_curve.csv\")\nplt.figure(figsize=(6,3.5))\nplt.plot(cur[\"epoch\"], cur[\"train_mae\"], marker=\"o\", label=\"train MAE\")\nplt.plot(cur[\"epoch\"], cur[\"val_mae\"], marker=\"s\", label=\"val MAE\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"MAE\"); plt.title(\"TS-GPT training\"); plt.legend(); plt.tight_layout()\nplt.savefig(\"docs/figs/tsgpt_train_curve.png\", dpi=160)\n\"Saved docs/figs/tsgpt_train_curve.png\"",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Session 22 — Adapting GPT to Time Series</span>"
    ]
  },
  {
    "objectID": "lec22.html#wrapup-10-min-emphasize-these-points",
    "href": "lec22.html#wrapup-10-min-emphasize-these-points",
    "title": "22  Session 22 — Adapting GPT to Time Series",
    "section": "22.5 Wrap‑up (10 min) — emphasize these points",
    "text": "22.5 Wrap‑up (10 min) — emphasize these points\n\nFeature projection ≈ token embeddings for real‑valued inputs; positional embeddings + causal mask stay the same.\nUse TRAIN‑fit scaler to avoid leakage; keep the rolling‑origin split and embargo from previous sessions.\nFor noisy returns, Huber/MAE often stabilize training better than MSE.\nSave checkpoints & metrics; we’ll ablate config choices (context/head/dropout) in the homework.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Session 22 — Adapting GPT to Time Series</span>"
    ]
  },
  {
    "objectID": "lec22.html#homework-due-before-session-23",
    "href": "lec22.html#homework-due-before-session-23",
    "title": "22  Session 22 — Adapting GPT to Time Series",
    "section": "22.6 Homework (due before Session 23)",
    "text": "22.6 Homework (due before Session 23)\nGoal: Run a small ablation on context length (32 vs 64), dropout (0.0 vs 0.1), and heads (2 vs 4), then summarize results in one table and add a short discussion to your Quarto report.\n\n22.6.1 A. Training script: scripts/tsgpt_train.py (configurable TS‑GPT)\n#!/usr/bin/env python\nfrom __future__ import annotations\nimport argparse, json, math, random\nfrom pathlib import Path\nimport numpy as np, pandas as pd, torch, torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# --------- Dataset & scaler (compact) ----------\nclass FeatureScaler:\n    def __init__(self): self.mean_=None; self.std_=None\n    def fit(self, X): self.mean_=X.mean(0, dtype=np.float64); self.std_=X.std(0, dtype=np.float64)+1e-8; return self\n    def transform(self, X): return (X - self.mean_) / self.std_\n\nclass WindowedTS(Dataset):\n    def __init__(self, df, feats, T=64, scaler=None, ticker2id=None):\n        self.T=T; self.feats=list(feats); self.idx=[]; self.g={}\n        if ticker2id is None:\n            cats=df[\"ticker\"].astype(\"category\").cat.categories.tolist()\n            self.ticker2id={t:i for i,t in enumerate(cats)}\n        else: self.ticker2id=dict(ticker2id)\n        for tkr,g in df.groupby(\"ticker\"):\n            g=g.sort_values(\"date\").reset_index(drop=True)\n            X=g[self.feats].to_numpy(\"float32\"); y=g[\"r_1d\"].to_numpy(\"float32\")\n            for end in range(T-1,len(g)):\n                if np.isfinite(y[end]): self.idx.append((str(tkr), end, self.ticker2id[str(tkr)]))\n            self.g[str(tkr)]={\"X\":X,\"y\":y}\n        self.scaler = scaler or FeatureScaler().fit(np.concatenate([self.g[t][\"X\"] for t in self.g],0))\n    def __len__(self): return len(self.idx)\n    def __getitem__(self,i):\n        tkr,end,tid=self.idx[i]; g=self.g[tkr]\n        X=self.scaler.transform(g[\"X\"][end-self.T+1:end+1]); y=g[\"y\"][end]\n        return torch.from_numpy(X), torch.tensor(y, dtype=torch.float32), torch.tensor(tid, dtype=torch.long)\n\ndef make_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u=np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    i=train_min-1; out=[]\n    while True:\n        if i&gt;=len(u): break\n        a,b=u[0],u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve&gt;=len(u): break\n        out.append((a,b,u[vs],u[ve])); i+=step\n    return out\n\n# --------- Model ----------\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, d_model, n_head, ctx, p=0.0):\n        super().__init__(); assert d_model % n_head == 0\n        self.nh=n_head; dh=d_model//n_head; self.dh=dh; self.ctx=ctx\n        self.qkv=nn.Linear(d_model,3*d_model,bias=False); self.proj=nn.Linear(d_model,d_model,bias=False)\n        self.ad=nn.Dropout(p); self.rd=nn.Dropout(p)\n        self.register_buffer(\"mask\", torch.tril(torch.ones(ctx,ctx)).unsqueeze(0).unsqueeze(0))\n    def forward(self,x):\n        B,T,C=x.shape; qkv=self.qkv(x); q,k,v=qkv.split(C,dim=2)\n        q=q.view(B,T,self.nh,self.dh).transpose(1,2); k=k.view(B,T,self.nh,self.dh).transpose(1,2); v=v.view(B,T,self.nh,self.dh).transpose(1,2)\n        att=(q @ k.transpose(-2,-1))/math.sqrt(self.dh); att=att.masked_fill(self.mask[:,:,:T,:T]==0, float(\"-inf\"))\n        att=att.softmax(dim=-1); att=self.ad(att); y=att @ v; y=y.transpose(1,2).contiguous().view(B,T,C)\n        return self.rd(self.proj(y))\n\nclass Block(nn.Module):\n    def __init__(self, d_model, n_head, ctx, d_ff, p=0.0):\n        super().__init__()\n        self.ln1=nn.LayerNorm(d_model); self.att=CausalSelfAttention(d_model,n_head,ctx,p)\n        self.ln2=nn.LayerNorm(d_model); self.mlp=nn.Sequential(nn.Linear(d_model,d_ff), nn.GELU(), nn.Dropout(p), nn.Linear(d_ff,d_model), nn.Dropout(p))\n    def forward(self,x): x=x+self.att(self.ln1(x)); x=x+self.mlp(self.ln2(x)); return x\n\nclass TimeSeriesGPT(nn.Module):\n    def __init__(self, in_f, ctx=64, d_model=64, n_head=2, n_layer=2, d_ff=128, p=0.0, n_tickers=None, d_ticker=0):\n        super().__init__(); self.ctx=ctx\n        self.proj=nn.Linear(in_f,d_model); self.pos=nn.Embedding(ctx,d_model)\n        self.id_emb=nn.Embedding(n_tickers,d_ticker) if (n_tickers and d_ticker&gt;0) else None\n        aug=d_model+(d_ticker if self.id_emb else 0)\n        self.blocks=nn.ModuleList([Block(aug,n_head,ctx,d_ff,p) for _ in range(n_layer)])\n        self.ln=nn.LayerNorm(aug); self.head=nn.Linear(aug,1)\n        self.apply(self._init)\n    def _init(self,m):\n        if isinstance(m,nn.Linear): nn.init.xavier_uniform_(m.weight)\n        if isinstance(m,nn.Embedding): nn.init.normal_(m.weight,0.0,0.02)\n    def forward(self,x,tid=None):\n        B,T,F=x.shape; pos=torch.arange(T, device=x.device)\n        h=self.proj(x)+self.pos(pos)[None,:,:]\n        if self.id_emb is not None and tid is not None:\n            e=self.id_emb(tid).unsqueeze(1).expand(-1,T,-1); h=torch.cat([h,e],dim=-1)\n        for blk in self.blocks: h=blk(h)\n        h=self.ln(h); return self.head(h[:,-1,:]).squeeze(-1)\n\ndef main():\n    ap=argparse.ArgumentParser()\n    ap.add_argument(\"--features\", default=\"data/processed/features_v1.parquet\")\n    ap.add_argument(\"--context\", type=int, default=64)\n    ap.add_argument(\"--d_model\", type=int, default=64)\n    ap.add_argument(\"--n_head\", type=int, default=2)\n    ap.add_argument(\"--n_layer\", type=int, default=2)\n    ap.add_argument(\"--d_ff\", type=int, default=128)\n    ap.add_argument(\"--dropout\", type=float, default=0.0)\n    ap.add_argument(\"--epochs\", type=int, default=10)\n    ap.add_argument(\"--batch\", type=int, default=256)\n    ap.add_argument(\"--lr\", type=float, default=2e-3)\n    ap.add_argument(\"--patience\", type=int, default=3)\n    ap.add_argument(\"--tickers\", type=int, default=10)\n    ap.add_argument(\"--out\", default=\"reports/tsgpt_metrics.csv\")\n    args=ap.parse_args()\n\n    # Load data\n    f_static = Path(\"data/processed/features_v1_static.parquet\")\n    df = pd.read_parquet(f_static) if f_static.exists() else pd.read_parquet(args.features)\n    df=df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True); df[\"ticker\"]=df[\"ticker\"].astype(\"category\")\n    cand=[\"log_return\",\"lag1\",\"lag2\",\"lag3\",\"zscore_20\",\"roll_std_20\",\"weekday\",\"month\"]\n    feats=[c for c in cand if c in df.columns]; assert \"r_1d\" in df.columns and feats\n    keep=df[\"ticker\"].cat.categories.tolist()[:args.tickers]; df=df[df[\"ticker\"].isin(keep)].copy()\n\n    # Split\n    def splits(dates, train_min=252, val=63, step=63, embargo=5):\n        u=np.array(sorted(pd.to_datetime(pd.Series(dates).unique()))); i=train_min-1; out=[]\n        while True:\n            if i&gt;=len(u): break\n            a,b=u[0],u[i]; vs=i+embargo+1; ve=vs+val-1\n            if ve&gt;=len(u): break\n            out.append((a,b,u[vs],u[ve])); i+=step\n        return out\n    a,b,c,d = splits(df[\"date\"])[0]\n    tr=df[(df[\"date\"]&gt;=a)&(df[\"date\"]&lt;=b)]; va=df[(df[\"date\"]&gt;=c)&(df[\"date\"]&lt;=d)]\n\n    # Datasets\n    tr_ds=WindowedTS(tr, feats, T=args.context); va_ds=WindowedTS(va, feats, T=args.context, scaler=tr_ds.scaler, ticker2id=tr_ds.ticker2id)\n    pin=torch.cuda.is_available()\n    def _seed(_): ws=torch.initial_seed()%2**32; np.random.seed(ws); random.seed(ws)\n    g=torch.Generator(); g.manual_seed(42)\n    tr_ld=DataLoader(tr_ds, batch_size=args.batch, shuffle=True, drop_last=True, num_workers=2, pin_memory=pin, worker_init_fn=_seed, generator=g)\n    va_ld=DataLoader(va_ds, batch_size=args.batch, shuffle=False, drop_last=False, num_workers=2, pin_memory=pin, worker_init_fn=_seed)\n\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    net=TimeSeriesGPT(len(feats), ctx=args.context, d_model=args.d_model, n_head=args.n_head, n_layer=args.n_layer, d_ff=args.d_ff, p=args.dropout).to(device)\n    opt=torch.optim.AdamW(net.parameters(), lr=args.lr, weight_decay=1e-5, betas=(0.9,0.99))\n    huber=torch.nn.HuberLoss(delta=0.01)\n\n    def mae_t(y,yhat): return torch.mean(torch.abs(y - yhat))\n    def smape_t(y,yhat,eps=1e-8): return torch.mean(2*torch.abs(y-yhat)/(torch.abs(y)+torch.abs(yhat)+eps))\n\n    best=1e9; best_ep=0\n    for ep in range(1, args.epochs+1):\n        net.train()\n        for xb,yb,_ in tr_ld:\n            xb=xb.to(device).float(); yb=yb.to(device).float()\n            opt.zero_grad(set_to_none=True)\n            yhat=net(xb); loss=huber(yhat,yb); loss.backward()\n            torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n            opt.step()\n        # val\n        net.eval(); m_mae=m_smape=0.0; n=0\n        with torch.no_grad():\n            for xb,yb,_ in va_ld:\n                xb=xb.to(device).float(); yb=yb.to(device).float()\n                yhat=net(xb); bs=xb.size(0)\n                m_mae += mae_t(yb,yhat).item()*bs; m_smape += smape_t(yb,yhat).item()*bs; n+=bs\n        val_mae=m_mae/n; val_smape=m_smape/n\n        print(f\"Epoch {ep:02d}  val_mae={val_mae:.5f}  val_sMAPE={val_smape:.5f}\")\n        if val_mae &lt; best-1e-6: best=val_mae; best_ep=ep\n        elif ep-best_ep &gt;= args.patience: break\n\n    Path(\"reports\").mkdir(exist_ok=True)\n    pd.DataFrame([{\"model\":\"tsgpt\",\"ctx\":args.context,\"d_model\":args.d_model,\"n_head\":args.n_head,\n                   \"n_layer\":args.n_layer,\"dropout\":args.dropout,\"val_mae\":best,\"best_epoch\":best_ep}]).to_csv(args.out, index=False)\n    print(\"Wrote\", args.out)\n\nif __name__ == \"__main__\":\n    main()\nRun examples:\n%%bash\nchmod +x scripts/tsgpt_train.py\npython scripts/tsgpt_train.py --context 32 --dropout 0.0 --n_head 2 --n_layer 2 --out reports/tsgpt_run_a.csv\npython scripts/tsgpt_train.py --context 64 --dropout 0.1 --n_head 4 --n_layer 2 --out reports/tsgpt_run_b.csv\n\n\n22.6.2 B. Summarize ablations into one table\nimport pandas as pd, glob\npaths = glob.glob(\"reports/tsgpt_run_*.csv\")\nrows = []\nfor p in paths:\n    try:\n        r = pd.read_csv(p).iloc[0].to_dict(); r[\"run\"] = p; rows.append(r)\n    except Exception as e:\n        print(\"Skip\", p, e)\nab = pd.DataFrame(rows).sort_values(\"val_mae\")\nab.to_csv(\"reports/tsgpt_ablation_summary.csv\", index=False)\nab.head(10)\n\n\n22.6.3 C. Add to Quarto report (one section)\nIn reports/eda.qmd (or reports/ts_gpt.qmd), add:\n## Tiny Transformer for Time Series\n\n**Split 1** results for TS‑GPT (sequence‑to‑one regression).\n\n::: {#80a7e242 .cell execution_count=1}\n````` {.python .cell-code}\nimport pandas as pd\nprint(pd.read_csv(\"reports/tsgpt_split1_metrics.csv\"))\nprint(pd.read_csv(\"reports/tsgpt_ablation_summary.csv\").sort_values(\"val_mae\").head(8))\n\n\n\nTS‑GPT training curve\n\n\n\n### D. Minimal tests (protect causal mask & window shapes)\n```python\n# tests/test_tsgpt_mask_and_shapes.py\nimport torch, pandas as pd\nfrom pathlib import Path\n\ndef test_mask_is_causal():\n    from scripts.tsgpt_train import TimeSeriesGPT\n    m = TimeSeriesGPT(in_f=4, ctx=16, d_model=32, n_head=2, n_layer=1, d_ff=64, p=0.0)\n    att = [b.att for b in m.blocks][0]\n    M = att.mask[0,0]\n    assert torch.all(M.triu(diagonal=1)==0)\n\ndef test_window_shape():\n    df = pd.read_parquet(\"data/processed/features_v1.parquet\").sort_values([\"ticker\",\"date\"])\n    feats = [c for c in [\"log_return\",\"lag1\",\"lag2\",\"lag3\"] if c in df.columns]\n    assert feats\n    from scripts.tsgpt_train import WindowedTS, make_splits\n    a,b,c,d = make_splits(df[\"date\"])[0]\n    ds = WindowedTS(df[(df[\"date\"]&gt;=a)&(df[\"date\"]&lt;=b)], feats, T=32)\n    X, y, tid = ds[0]\n    assert X.shape == (32, len(feats))\n    assert torch.isfinite(torch.tensor(y)).item() == 1\nRun:\n%%bash\npytest -q -k tsgpt_mask_and_shapes\n:::\n\n\n22.6.4 E. (Optional) Makefile targets\nAppend to Makefile:\n.PHONY: train-tsgpt ablate-tsgpt\ntrain-tsgpt: ## Train TS-GPT on split 1 (tiny config)\n\\tpython scripts/tsgpt_train.py --context 64 --n_head 2 --n_layer 2 --dropout 0.0 --out reports/tsgpt_run_base.csv\n\nablate-tsgpt: ## Run two small ablations\n\\tpython scripts/tsgpt_train.py --context 32 --n_head 2 --dropout 0.0 --out reports/tsgpt_run_a.csv\n\\tpython scripts/tsgpt_train.py --context 64 --n_head 4 --dropout 0.1 --out reports/tsgpt_run_b.csv",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Session 22 — Adapting GPT to Time Series</span>"
    ]
  },
  {
    "objectID": "lec22.html#instructor-checklist-before-class",
    "href": "lec22.html#instructor-checklist-before-class",
    "title": "22  Session 22 — Adapting GPT to Time Series",
    "section": "22.7 Instructor checklist (before class)",
    "text": "22.7 Instructor checklist (before class)\n\nEnsure at least log_return, r_1d, and a few lags are present, or rely on fallback generation.\nDry‑run the tiny config on CPU or GPU; confirm val MAE decreases in &lt;5 minutes.\nPrepare one slide mapping “char embedding” → “feature projection”.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Session 22 — Adapting GPT to Time Series</span>"
    ]
  },
  {
    "objectID": "lec22.html#emphasize-while-teaching",
    "href": "lec22.html#emphasize-while-teaching",
    "title": "22  Session 22 — Adapting GPT to Time Series",
    "section": "22.8 Emphasize while teaching",
    "text": "22.8 Emphasize while teaching\n\nCausality and alignment are unchanged from char‑GPT; only the input embedding changes.\nScaling on TRAIN only; never refit on VAL/TEST.\nReport at least MAE and sMAPE; add calibration by regime later if desired.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Session 22 — Adapting GPT to Time Series</span>"
    ]
  },
  {
    "objectID": "lec22.html#grading-passrevise",
    "href": "lec22.html#grading-passrevise",
    "title": "22  Session 22 — Adapting GPT to Time Series",
    "section": "22.9 Grading (pass/revise)",
    "text": "22.9 Grading (pass/revise)\n\nmodels/tsgpt_split1.pt and reports/tsgpt_split1_metrics.csv exist.\nStudents ran at least two ablations and wrote reports/tsgpt_ablation_summary.csv.\nTests pass (test_tsgpt_mask_and_shapes).\nQuarto report updated with numbers and the training curve.\n\nYou now have a tiny GPT adapted to time series, ready to be compared to your GRU/LSTM baselines and incorporated into the unified multi‑asset pipeline.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Session 22 — Adapting GPT to Time Series</span>"
    ]
  },
  {
    "objectID": "lec23.html",
    "href": "lec23.html",
    "title": "23  Session 23 — Packaging & CLI (Typer)",
    "section": "",
    "text": "23.1 Session 23 — Packaging & CLI (Typer) (75 min)\nBelow is a complete lecture package for Session 23 — Packaging & CLI (Typer) (75 minutes). It includes a timed agenda, slide talking points, a Colab‑friendly in‑class lab with copy‑paste code, and homework with copy‑paste code. You’ll package your repo into a proper Python project using a src/ layout, add a minimal Typer CLI, and wire it to score models over a date range using your existing features Parquet and saved checkpoints.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Session 23 — Packaging & CLI (Typer)</span>"
    ]
  },
  {
    "objectID": "lec23.html#session-23-packaging-cli-typer-75-min",
    "href": "lec23.html#session-23-packaging-cli-typer-75-min",
    "title": "23  Session 23 — Packaging & CLI (Typer)",
    "section": "",
    "text": "23.1.1 Learning goals\nBy the end, students can:\n\nConvert a research repo into an installable package with a src/ layout and pyproject.toml.\nCreate a Typer CLI (python -m projectname.cli …) that reads config, loads data, and scores a model over a date range.\nCentralize config in YAML, keep paths tidy, and write results to reports/.\nRun a fresh‑clone smoke test: make env && pip install -e . && python -m projectname.cli score ….",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Session 23 — Packaging & CLI (Typer)</span>"
    ]
  },
  {
    "objectID": "lec23.html#agenda-75-min",
    "href": "lec23.html#agenda-75-min",
    "title": "23  Session 23 — Packaging & CLI (Typer)",
    "section": "23.2 Agenda (75 min)",
    "text": "23.2 Agenda (75 min)\n\n(10 min) Slides: why package? src/ layout; pyproject.toml; editable installs (pip install -e .).\n(10 min) Slides: CLI ergonomics with Typer; single binary vs python -m; structured logging & exit codes.\n(40 min) In‑class lab: add package skeleton → pyproject.toml → config YAML → CLI with score and split-info → run end‑to‑end.\n(15 min) Wrap‑up, Makefile targets, homework brief.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Session 23 — Packaging & CLI (Typer)</span>"
    ]
  },
  {
    "objectID": "lec23.html#slides-talking-points-paste-into-deck",
    "href": "lec23.html#slides-talking-points-paste-into-deck",
    "title": "23  Session 23 — Packaging & CLI (Typer)",
    "section": "23.3 Slides — talking points (paste into deck)",
    "text": "23.3 Slides — talking points (paste into deck)\nWhy package your project?\n\nImport your code anywhere (import projectname…) → reliable relative imports, easier testing/CI, cleaner notebooks.\n“src/ layout” prevents accidental imports from the working dir; forces installed code to be used.\n\npyproject.toml essentials\n\n[build-system] selects backend (setuptools, hatchling, …).\n[project] declares name, version, dependencies.\nUse editable install during research: pip install -e .[dev].\n\nTyper for CLIs\n\nDeclarative options & automatic help (--help).\nCompose commands: score, split-info, show-config.\nReturn non‑zero exit codes on errors; don’t hide failures.\n\nConfig in YAML\n\nOne config/config.yaml to centralize paths, features, split params.\nParse with yaml.safe_load; add a tiny typed wrapper.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Session 23 — Packaging & CLI (Typer)</span>"
    ]
  },
  {
    "objectID": "lec23.html#inclass-lab-40-min",
    "href": "lec23.html#inclass-lab-40-min",
    "title": "23  Session 23 — Packaging & CLI (Typer)",
    "section": "23.4 In‑class Lab (40 min)",
    "text": "23.4 In‑class Lab (40 min)\n\nRun each block as its own cell (or file creation step). Update REPO_NAME as needed.\n\n\n23.4.1 0) Mount Drive & cd to repo\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME = \"unified-stocks-teamX\"  # &lt;- change\nBASE_DIR  = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR  = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, sys, platform\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nprint(\"Working dir:\", os.getcwd(), \"| Python:\", sys.version.split()[0], \"| OS:\", platform.system())\n\n# Create standard dirs if missing\nfor p in [\"src/projectname\", \"src/projectname/data\", \"src/projectname/features\",\n          \"src/projectname/models\", \"src/projectname/utils\", \"config\",\n          \"reports\", \"models\", \"tests\", \"docs/figs\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\n\n\n23.4.2 1) Create pyproject.toml (packaging metadata)\n\nTip: We keep requirements.txt for now, but the package will also declare dependencies.\n\n# pyproject.toml  (place at repo root)\n[build-system]\nrequires = [\"setuptools&gt;=68\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"projectname\"                # rename if you like; keep it simple & lowercase\nversion = \"0.1.0\"\ndescription = \"Unified stock forecasting pipeline (teaching)\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.10\"\ndependencies = [\n  \"pandas&gt;=2.1\",\n  \"numpy&gt;=1.25\",\n  \"pyarrow&gt;=14\",\n  \"typer[all]&gt;=0.9\",\n  \"rich&gt;=13.3\",\n  \"PyYAML&gt;=6.0\",\n  \"scikit-learn&gt;=1.3\",\n  \"torch&gt;=2.1\",\n]\n[project.optional-dependencies]\ndev = [\"pytest&gt;=7\", \"pytest-cov&gt;=4\"]\n\n[tool.setuptools]\npackage-dir = {\"\" = \"src\"}\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n\n[project.scripts]\n# Optional console entrypoint (`projectname`); we also support `python -m projectname.cli`\nprojectname = \"projectname.cli:main\"\n\n\n23.4.3 2) Add a default config YAML\n# config/config.yaml\ndata:\n  processed_dir: \"data/processed\"\n  features_file: \"data/processed/features_v1.parquet\"\n  returns_file:  \"data/processed/returns.parquet\"\n  models_dir:    \"models\"\n  reports_dir:   \"reports\"\n\neval:\n  train_min: 252\n  val_size: 63\n  embargo: 5\n  context: 64\n\nfeatures:\n  use: [\"log_return\", \"lag1\", \"lag2\", \"lag3\", \"zscore_20\", \"roll_std_20\"]  # only those present will be used\n\n\n23.4.4 3) Package boilerplate\nsrc/projectname/__init__.py\n__all__ = [\"config\", \"utils\", \"models\"]\n__version__ = \"0.1.0\"\nsrc/projectname/config.py\nfrom __future__ import annotations\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport yaml\n\nDEFAULT_CFG = Path(\"config/config.yaml\")\n\n@dataclass\nclass DataPaths:\n    processed_dir: Path\n    features_file: Path\n    returns_file: Path\n    models_dir: Path\n    reports_dir: Path\n\n@dataclass\nclass EvalCfg:\n    train_min: int\n    val_size: int\n    embargo: int\n    context: int\n\n@dataclass\nclass Config:\n    data: DataPaths\n    eval: EvalCfg\n    features_use: list[str]\n\ndef load_config(path: str | Path = DEFAULT_CFG) -&gt; Config:\n    p = Path(path)\n    if not p.exists():\n        raise FileNotFoundError(f\"Config not found: {p.resolve()}\")\n    cfg = yaml.safe_load(p.read_text())\n    data = cfg.get(\"data\", {})\n    ev   = cfg.get(\"eval\", {})\n    feats= cfg.get(\"features\", {}).get(\"use\", [])\n    return Config(\n        data=DataPaths(\n            processed_dir=Path(data[\"processed_dir\"]),\n            features_file=Path(data[\"features_file\"]),\n            returns_file=Path(data[\"returns_file\"]),\n            models_dir=Path(data[\"models_dir\"]),\n            reports_dir=Path(data[\"reports_dir\"]),\n        ),\n        eval=EvalCfg(\n            train_min=int(ev[\"train_min\"]),\n            val_size=int(ev[\"val_size\"]),\n            embargo=int(ev[\"embargo\"]),\n            context=int(ev[\"context\"]),\n        ),\n        features_use=list(feats),\n    )\nsrc/projectname/utils/splits.py\nfrom __future__ import annotations\nimport numpy as np\nimport pandas as pd\n\ndef make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    i = train_min - 1\n    out = []\n    while True:\n        if i &gt;= len(u):\n            break\n        a, b = u[0], u[i]\n        vs = i + embargo + 1\n        ve = vs + val_size - 1\n        if ve &gt;= len(u):\n            break\n        out.append((a, b, u[vs], u[ve]))\n        i += step\n    return out\nsrc/projectname/utils/metrics.py\nfrom __future__ import annotations\nimport numpy as np\nimport pandas as pd\n\ndef mae(y, yhat) -&gt; float:\n    y = np.asarray(y); yhat = np.asarray(yhat)\n    return float(np.mean(np.abs(y - yhat)))\n\ndef smape(y, yhat, eps: float = 1e-8) -&gt; float:\n    y = np.asarray(y); yhat = np.asarray(yhat)\n    return float(np.mean(2.0*np.abs(y - yhat)/(np.abs(y)+np.abs(yhat)+eps)))\n\ndef mase_scale_train(train_df: pd.DataFrame) -&gt; dict[str, float]:\n    # per-ticker naive MAE scale on TRAIN only\n    scales={}\n    for tkr, g in train_df.groupby(\"ticker\"):\n        g = g.dropna(subset=[\"r_1d\",\"log_return\"])\n        if len(g)==0: continue\n        scales[str(tkr)] = mae(g[\"r_1d\"], g[\"log_return\"])\n    return scales\n\ndef mase(y_true, y_pred, tickers, scale: dict[str, float]) -&gt; float:\n    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n    tickers = np.asarray(tickers).astype(str)\n    den = np.array([scale.get(t, np.nan) for t in tickers], dtype=float)\n    return float(np.nanmean(np.abs(y_true - y_pred)/(den + 1e-12)))\nsrc/projectname/models/baselines.py\nfrom __future__ import annotations\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\n\ndef predict_naive(val_df: pd.DataFrame) -&gt; pd.DataFrame:\n    out = val_df[[\"date\",\"ticker\",\"r_1d\",\"log_return\"]].copy()\n    out = out.rename(columns={\"r_1d\":\"y_true\",\"log_return\":\"yhat\"})\n    out[\"method\"] = \"naive\"\n    return out\n\ndef predict_lin_lags(train_df: pd.DataFrame, val_df: pd.DataFrame, feats: list[str]) -&gt; pd.DataFrame:\n    preds=[]\n    xcols = [c for c in feats if c in val_df.columns]\n    for tkr, tr in train_df.groupby(\"ticker\"):\n        va = val_df[val_df[\"ticker\"]==tkr]\n        if len(tr)==0 or len(va)==0: continue\n        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LinearRegression())])\n        pipe.fit(tr[xcols].values, tr[\"r_1d\"].values)\n        yhat = pipe.predict(va[xcols].values)\n        g = va[[\"date\",\"ticker\",\"r_1d\"]].copy()\n        g[\"yhat\"] = yhat\n        g[\"method\"] = \"lin_lags\"\n        preds.append(g)\n    out = pd.concat(preds, ignore_index=True) if preds else pd.DataFrame(columns=[\"date\",\"ticker\",\"y_true\",\"yhat\",\"method\"])\n    return out.rename(columns={\"r_1d\":\"y_true\"})\nsrc/projectname/models/torch_infer.py (tiny inference wrappers; safe if ckpt missing)\nfrom __future__ import annotations\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport torch, torch.nn as nn\n\n# Reuse tiny GRU and TS-GPT definitions (matching Sessions 19 & 22)\nclass GRURegressor(nn.Module):\n    def __init__(self, in_features: int, hidden=64, layers=2, dropout=0.1):\n        super().__init__()\n        self.gru = nn.GRU(in_features, hidden, num_layers=layers, batch_first=True, dropout=dropout if layers&gt;1 else 0.)\n        self.head = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Dropout(dropout), nn.Linear(hidden,1))\n    def forward(self, x):\n        _, hN = self.gru(x)\n        return self.head(hN[-1]).squeeze(-1)\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, d_model, n_head, ctx, p=0.0):\n        super().__init__(); assert d_model % n_head == 0\n        self.nh = n_head; self.dh = d_model//n_head\n        self.qkv = nn.Linear(d_model, 3*d_model, bias=False)\n        self.proj= nn.Linear(d_model, d_model, bias=False)\n        self.ad = nn.Dropout(p); self.rd = nn.Dropout(p)\n        self.register_buffer(\"mask\", torch.tril(torch.ones(ctx,ctx)).unsqueeze(0).unsqueeze(0))\n    def forward(self, x):\n        B,T,C=x.shape; qkv=self.qkv(x); q,k,v=qkv.split(C,dim=2)\n        q=q.view(B,T,self.nh,self.dh).transpose(1,2); k=k.view(B,T,self.nh,self.dh).transpose(1,2); v=v.view(B,T,self.nh,self.dh).transpose(1,2)\n        att=(q @ k.transpose(-2,-1))/np.sqrt(self.dh); att=att.masked_fill(self.mask[:,:,:T,:T]==0, float(\"-inf\"))\n        att=att.softmax(dim=-1); att=self.ad(att); y=att @ v; y=y.transpose(1,2).contiguous().view(B,T,C)\n        return self.rd(self.proj(y))\n\nclass Block(nn.Module):\n    def __init__(self, d_model, n_head, ctx, d_ff, p=0.0):\n        super().__init__()\n        self.ln1=nn.LayerNorm(d_model); self.att=CausalSelfAttention(d_model,n_head,ctx,p)\n        self.ln2=nn.LayerNorm(d_model); self.mlp=nn.Sequential(nn.Linear(d_model,d_ff), nn.GELU(), nn.Dropout(p), nn.Linear(d_ff,d_model), nn.Dropout(p))\n    def forward(self,x): x=x+self.att(self.ln1(x)); x=x+self.mlp(self.ln2(x)); return x\n\nclass TimeSeriesGPT(nn.Module):\n    def __init__(self, in_features, ctx=64, d_model=64, n_head=2, n_layer=2, d_ff=128, p=0.0):\n        super().__init__(); self.ctx=ctx\n        self.proj=nn.Linear(in_features,d_model); self.pos=nn.Embedding(ctx,d_model)\n        self.blocks=nn.ModuleList([Block(d_model,n_head,ctx,d_ff,p) for _ in range(n_layer)])\n        self.ln=nn.LayerNorm(d_model); self.head=nn.Linear(d_model,1)\n    def forward(self,x):\n        B,T,F=x.shape; pos=torch.arange(T, device=x.device)\n        h=self.proj(x)+self.pos(pos)[None,:,:]\n        for blk in self.blocks: h=blk(h)\n        h=self.ln(h)\n        return self.head(h[:,-1,:]).squeeze(-1)\n\ndef _windowize(df: pd.DataFrame, feats: list[str], T: int):\n    Xs, ys, tk = [], [], []\n    for tkr, g in df.groupby(\"ticker\"):\n        g = g.sort_values(\"date\").reset_index(drop=True)\n        X = g[feats].to_numpy(\"float32\"); y = g[\"r_1d\"].to_numpy(\"float32\")\n        for end in range(T-1, len(g)):\n            Xs.append(X[end-T+1:end+1]); ys.append(y[end]); tk.append(str(tkr))\n    Xs = np.stack(Xs,0); ys = np.array(ys); tk = np.array(tk)\n    return Xs, ys, tk\n\ndef predict_with_gru(val_df: pd.DataFrame, feats: list[str], models_dir: Path, T: int=64):\n    ckpt = models_dir / \"gru_split1.pt\"\n    if not ckpt.exists():\n        raise FileNotFoundError(\"Missing models/gru_split1.pt (train in Session 19)\")\n\n    meta = torch.load(ckpt, map_location=\"cpu\")\n    in_f = len(feats)\n    net = GRURegressor(in_features=in_f)\n    net.load_state_dict(meta[\"model_state\"]); net.eval()\n    Xs, ys, tk = _windowize(val_df, feats, T)\n    with torch.no_grad():\n        yhat = net(torch.from_numpy(Xs).float()).cpu().numpy()\n    out = val_df.iloc[T-1:].copy().reset_index(drop=True).loc[:, [\"date\",\"ticker\"]]\n    out = out.assign(y_true=ys, yhat=yhat, method=\"gru\")\n    return out\n\ndef predict_with_tsgpt(val_df: pd.DataFrame, feats: list[str], models_dir: Path, T: int=64):\n    ckpt = models_dir / \"tsgpt_split1.pt\"\n    if not ckpt.exists():\n        raise FileNotFoundError(\"Missing models/tsgpt_split1.pt (train in Session 22)\")\n\n    meta = torch.load(ckpt, map_location=\"cpu\")\n    in_f = len(feats)\n    net = TimeSeriesGPT(in_features=in_f, ctx=T)\n    net.load_state_dict(meta[\"model\"]); net.eval()\n    Xs, ys, tk = _windowize(val_df, feats, T)\n    with torch.no_grad():\n        yhat = net(torch.from_numpy(Xs).float()).cpu().numpy()\n    out = val_df.iloc[T-1:].copy().reset_index(drop=True).loc[:, [\"date\",\"ticker\"]]\n    out = out.assign(y_true=ys, yhat=yhat, method=\"tsgpt\")\n    return out\n\n\n23.4.5 4) The CLI (Typer): src/projectname/cli.py\nfrom __future__ import annotations\nimport sys\nfrom datetime import date\nfrom pathlib import Path\nfrom typing import Optional, Literal\n\nimport pandas as pd\nimport typer\nfrom rich import print as rprint\nfrom rich.table import Table\n\nfrom projectname.config import load_config\nfrom projectname.utils.splits import make_rolling_origin_splits\nfrom projectname.utils.metrics import mae, smape, mase_scale_train, mase\nfrom projectname.models.baselines import predict_naive, predict_lin_lags\n\n# Optional imports for torch models (graceful fallback)\ntry:\n    from projectname.models.torch_infer import predict_with_gru, predict_with_tsgpt\nexcept Exception as e:\n    predict_with_gru = predict_with_tsgpt = None  # type: ignore\n\napp = typer.Typer(add_completion=False, help=\"Project CLI for scoring/evaluation.\")\n\ndef _load_features(features_file: Path) -&gt; pd.DataFrame:\n    df = pd.read_parquet(features_file)\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    df[\"ticker\"] = df[\"ticker\"].astype(\"category\")\n    return df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n\n@app.command()\ndef split_info(\n    config: Path = typer.Option(\"config/config.yaml\", help=\"Path to YAML config.\"),\n):\n    \"Print available rolling-origin splits (first 3).\"\n    cfg = load_config(config)\n    df = _load_features(cfg.data.features_file)\n    splits = make_rolling_origin_splits(df[\"date\"], cfg.eval.train_min, cfg.eval.val_size, cfg.eval.val_size, cfg.eval.embargo)\n    table = Table(title=\"Rolling-origin splits (first 3)\")\n    table.add_column(\"id\"); table.add_column(\"train_start\"); table.add_column(\"train_end\"); table.add_column(\"val_start\"); table.add_column(\"val_end\")\n    for i,(a,b,c,d) in enumerate(splits[:3], start=1):\n        table.add_row(str(i), str(a.date()), str(b.date()), str(c.date()), str(d.date()))\n    rprint(table)\n\n@app.command()\ndef show_config(\n    config: Path = typer.Option(\"config/config.yaml\", help=\"Path to YAML config.\")\n):\n    \"Echo resolved config.\"\n    rprint(load_config(config))\n\n@app.command()\ndef score(\n    model: Literal[\"naive\",\"lin_lags\",\"gru\",\"tsgpt\"] = typer.Option(\"lin_lags\", help=\"Model to score.\"),\n    start: Optional[str] = typer.Option(None, help=\"Val period start (YYYY-MM-DD). If omitted, use split 1.\"),\n    end: Optional[str]   = typer.Option(None, help=\"Val period end (YYYY-MM-DD).\"),\n    split: int = typer.Option(1, help=\"Use this split if --start/--end not provided.\"),\n    out: Path = typer.Option(\"reports/cli_score.csv\", help=\"Output CSV path.\"),\n    config: Path = typer.Option(\"config/config.yaml\", help=\"Path to YAML config.\"),\n):\n    \"\"\"\n    Score a model over a date range; write tidy predictions with metrics.\n    \"\"\"\n    cfg = load_config(config)\n    df = _load_features(cfg.data.features_file)\n\n    # Pick features present in the file\n    feat_cols = [c for c in cfg.features_use if c in df.columns]\n    if model in (\"lin_lags\",\"gru\",\"tsgpt\") and not feat_cols:\n        typer.echo(\"No requested features found in features parquet.\", err=True)\n        raise typer.Exit(code=2)\n\n    # Build split\n    if start and end:\n        vstart = pd.to_datetime(start); vend = pd.to_datetime(end)\n        # Train = everything strictly before val start, respecting train_min\n        dates = sorted(df[\"date\"].unique())\n        idx = next(i for i,d in enumerate(dates) if d&gt;=vstart)\n        if idx &lt; cfg.eval.train_min: \n            typer.echo(\"Not enough history before val start for train_min.\", err=True); raise typer.Exit(code=2)\n        tstart = dates[0]; tend = dates[idx-1]\n    else:\n        splits = make_rolling_origin_splits(df[\"date\"], cfg.eval.train_min, cfg.eval.val_size, cfg.eval.val_size, cfg.eval.embargo)\n        try:\n            tstart, tend, vstart, vend = splits[split-1]\n        except IndexError:\n            typer.echo(f\"Split {split} not available.\", err=True); raise typer.Exit(code=2)\n\n    train_df = df[(df[\"date\"]&gt;=tstart)&(df[\"date\"]&lt;=tend)].copy()\n    val_df   = df[(df[\"date\"]&gt;=vstart)&(df[\"date\"]&lt;=vend)].copy()\n\n    if len(val_df)==0 or len(train_df)==0:\n        typer.echo(\"Empty train/val after slicing. Check dates.\", err=True); raise typer.Exit(code=2)\n\n    # Produce predictions\n    try:\n        if model == \"naive\":\n            preds = predict_naive(val_df)\n        elif model == \"lin_lags\":\n            preds = predict_lin_lags(train_df, val_df, feat_cols)\n        elif model == \"gru\":\n            if predict_with_gru is None:\n                typer.echo(\"Torch not available in this environment.\", err=True); raise typer.Exit(code=2)\n            preds = predict_with_gru(val_df, feat_cols, cfg.data.models_dir, T=cfg.eval.context)\n        elif model == \"tsgpt\":\n            if predict_with_tsgpt is None:\n                typer.echo(\"Torch not available in this environment.\", err=True); raise typer.Exit(code=2)\n            preds = predict_with_tsgpt(val_df, feat_cols, cfg.data.models_dir, T=cfg.eval.context)\n        else:\n            raise ValueError(model)\n    except FileNotFoundError as e:\n        typer.echo(f\"{e}. Falling back to lin_lags.\", err=True)\n        preds = predict_lin_lags(train_df, val_df, feat_cols)\n\n    # Metrics (micro; and MASE using train scale)\n    scale = mase_scale_train(train_df)\n    y = preds[\"y_true\"]; yhat = preds[\"yhat\"]; tick = preds[\"ticker\"].astype(str)\n    m_mae = mae(y,yhat); m_smape = smape(y,yhat); m_mase = mase(y,yhat,tick, scale)\n\n    # Save predictions and print summary\n    out.parent.mkdir(parents=True, exist_ok=True)\n    preds.to_csv(out, index=False)\n\n    rprint(f\"[bold green]Wrote[/bold green] {out}\")\n    table = Table(title=f\"{model} — {str(vstart.date())} → {str(vend.date())}\")\n    table.add_column(\"metric\"); table.add_column(\"value\")\n    table.add_row(\"MAE\", f\"{m_mae:.6f}\")\n    table.add_row(\"sMAPE\", f\"{m_smape:.6f}\")\n    table.add_row(\"MASE\", f\"{m_mase:.6f}\")\n    rprint(table)\n\ndef main():\n    app()\n\nif __name__ == \"__main__\":\n    main()\n\n\n23.4.6 5) Install the package (editable) & quick smoke test\n%%bash\npip install -e \".[dev]\" -q\npython -m projectname.cli --help | head -n 20\npython -m projectname.cli split-info\npython -m projectname.cli score --model lin_lags --out reports/cli_linlags.csv\nYou should see a metrics table and a saved CSV under reports/.\n\n\n23.4.7 6) Makefile additions\nAppend to your existing Makefile:\n.PHONY: install score-lin\ninstall: ## Editable install of the package\n\\tpip install -e \".[dev]\"\n\nscore-lin: ## Score lin_lags on split 1, save predictions\n\\tpython -m projectname.cli score --model lin_lags --out reports/cli_linlags.csv\n\n\n23.4.8 7) Minimal test for the CLI\ntests/test_cli_score.py\nfrom typer.testing import CliRunner\nfrom projectname.cli import app\nimport pandas as pd\nfrom pathlib import Path\n\nrunner = CliRunner()\n\ndef test_score_lin_lags(tmp_path: Path):\n    out = tmp_path/\"preds.csv\"\n    result = runner.invoke(app, [\"score\", \"--model\", \"lin_lags\", \"--out\", str(out)])\n    assert result.exit_code == 0\n    assert out.exists()\n    df = pd.read_csv(out)\n    assert {\"date\",\"ticker\",\"y_true\",\"yhat\",\"method\"}.issubset(df.columns)\nRun:\n%%bash\npytest -q -k cli_score",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Session 23 — Packaging & CLI (Typer)</span>"
    ]
  },
  {
    "objectID": "lec23.html#wrapup-15-min",
    "href": "lec23.html#wrapup-15-min",
    "title": "23  Session 23 — Packaging & CLI (Typer)",
    "section": "23.5 Wrap‑up (15 min)",
    "text": "23.5 Wrap‑up (15 min)\n\nPackage once, reuse everywhere: notebooks, scripts, CI.\nCLI is the user surface: deterministic, scriptable, easy to test.\nCentralized config prevents duplicated paths and parameters.\nKeep the CLI fast (no training). Scoring should finish in seconds to a couple minutes.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Session 23 — Packaging & CLI (Typer)</span>"
    ]
  },
  {
    "objectID": "lec23.html#homework-due-before-session-24",
    "href": "lec23.html#homework-due-before-session-24",
    "title": "23  Session 23 — Packaging & CLI (Typer)",
    "section": "23.6 Homework (due before Session 24)",
    "text": "23.6 Homework (due before Session 24)\nGoal: Prove your repo is reproducible from a fresh clone and your CLI works end‑to‑end.\n\n23.6.1 A. Fresh‑clone smoke test script (Colab cell)\nPaste this into a fresh Colab (or a new runtime):\n# --- variables ---\nREPO_SSH_OR_HTTPS=\"&lt;YOUR REPO URL&gt;\"   # e.g. https://github.com/you/unified-stocks-teamX.git\nREPO_DIR=\"unified-stocks-teamX\"\n\n# --- run ---\nrm -rf \"$REPO_DIR\"\ngit clone \"$REPO_SSH_OR_HTTPS\" \"$REPO_DIR\"\ncd \"$REPO_DIR\"\n\n# env + editable install + report + score\nmake env              # from Session 2 (installs requirements.txt)\npip install -e \".[dev]\"\nquarto --version &gt;/dev/null 2&gt;&1 || echo \"Quarto optional; skipping render.\"\npython -m projectname.cli split-info\npython -m projectname.cli score --model lin_lags --out reports/cli_linlags.csv\n\n# show outputs\npython - &lt;&lt;'PY'\nimport pandas as pd\nprint(pd.read_csv(\"reports/cli_linlags.csv\").head())\nPY\nDeliverables:\n\nA short note in README: “Fresh‑clone test (date), environment, and exact commands used.”\nThe file reports/cli_linlags.csv committed (small).\nOptional: run python -m projectname.cli score --model tsgpt --out reports/cli_tsgpt.csv if models/tsgpt_split1.pt exists, and include that CSV too.\n\n\n\n23.6.2 B. Tiny CI check (optional but recommended)\nAdd this job to .github/workflows/ci.yml after tests:\n- name: CLI smoke test\n  run: |\n    pip install -e \".[dev]\"\n    python -m projectname.cli split-info\n    python -m projectname.cli score --model lin_lags --out reports/cli_ci.csv",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Session 23 — Packaging & CLI (Typer)</span>"
    ]
  },
  {
    "objectID": "lec23.html#instructor-notes-gotchas",
    "href": "lec23.html#instructor-notes-gotchas",
    "title": "23  Session 23 — Packaging & CLI (Typer)",
    "section": "23.7 Instructor notes / gotchas",
    "text": "23.7 Instructor notes / gotchas\n\nIn Colab, after modifying pyproject.toml or code under src/, re‑run pip install -e . or restart the kernel to pick up changes.\nIf Torch isn’t available, the CLI still works with naive/lin_lags.\nIf students renamed the package (not projectname), update imports and pyproject.toml accordingly.\nKeep the CLI’s error messages actionable (e.g., “missing models/tsgpt_split1.pt; falling back to lin_lags”).",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Session 23 — Packaging & CLI (Typer)</span>"
    ]
  },
  {
    "objectID": "lec23.html#grading-passrevise",
    "href": "lec23.html#grading-passrevise",
    "title": "23  Session 23 — Packaging & CLI (Typer)",
    "section": "23.8 Grading (pass/revise)",
    "text": "23.8 Grading (pass/revise)\n\npip install -e . succeeds; import projectname works.\npython -m projectname.cli split-info prints splits.\npython -m projectname.cli score --model lin_lags writes reports/cli_linlags.csv and prints metrics.\nREADME includes a fresh‑clone reproduction section.\n(Optional) A simple CLI test passes in CI.\n\nThis session leaves you with a clean, installable project and a Typer CLI that others (or future‑you) can run in one command, setting you up for Session 24’s reproducibility audit (and optional FastAPI demo).",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Session 23 — Packaging & CLI (Typer)</span>"
    ]
  },
  {
    "objectID": "lec24.html",
    "href": "lec24.html",
    "title": "24  Session 24 — Reproducibility audit & optional FastAPI",
    "section": "",
    "text": "24.1 Session 24 — Reproducibility audit & optional FastAPI (75 min)\nBelow is a complete lecture package for Session 24 — Reproducibility audit & optional FastAPI (75 minutes). It includes a timed agenda, slide talking points, a Colab‑friendly in‑class lab with copy‑paste code, a lightweight reproducibility audit script, and homework with code (tagging v1.0-rc + a simple changelog generator + optional FastAPI “hello score” service).",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Session 24 — Reproducibility audit & optional FastAPI</span>"
    ]
  },
  {
    "objectID": "lec24.html#session-24-reproducibility-audit-optional-fastapi-75-min",
    "href": "lec24.html#session-24-reproducibility-audit-optional-fastapi-75-min",
    "title": "24  Session 24 — Reproducibility audit & optional FastAPI",
    "section": "",
    "text": "24.1.1 Learning goals\nBy the end, students can:\n\nVerify end‑to‑end reproducibility from a clean environment using a checklist and an automated script.\nCreate and persist a run manifest (commit hash, env, data checksums, seeds).\n(Optional) Stand up a FastAPI “hello score” endpoint that wraps the package scoring logic.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Session 24 — Reproducibility audit & optional FastAPI</span>"
    ]
  },
  {
    "objectID": "lec24.html#agenda-75-min",
    "href": "lec24.html#agenda-75-min",
    "title": "24  Session 24 — Reproducibility audit & optional FastAPI",
    "section": "24.2 Agenda (75 min)",
    "text": "24.2 Agenda (75 min)\n\n(12 min) Slides: what “reproducible” means; seeds, lockfiles, data snapshots, deterministic flags.\n(8 min) Slides: manifests & checksums; common pitfalls; CI scope (keep fast).\n(35 min) In‑class lab: run the audit, generate a manifest, compare two runs, file issues.\n(10 min) (Optional) FastAPI “hello score” demo.\n(10 min) Wrap‑up & homework brief.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Session 24 — Reproducibility audit & optional FastAPI</span>"
    ]
  },
  {
    "objectID": "lec24.html#slide-notes-paste-into-your-deck",
    "href": "lec24.html#slide-notes-paste-into-your-deck",
    "title": "24  Session 24 — Reproducibility audit & optional FastAPI",
    "section": "24.3 Slide notes (paste into your deck)",
    "text": "24.3 Slide notes (paste into your deck)\n\n24.3.1 What “reproducible” means for this course\n\nFresh‑clone → make env && pip install -e . && python -m projectname.cli score ... succeeds.\nOutputs (metrics/CSV) are identical or within a tiny tolerance on the same hardware/runtime.\nSeeds set consistently; deterministic flags used where available (Torch).\nData is fixed: versioned snapshot (in repo or via LFS) or cached API responses with checksums.\n\n\n\n24.3.2 Lockfiles & manifests\n\nKeep requirements.txt pinned (exact versions) and also store a freeze: pip freeze &gt; requirements-lock.txt.\nRecord a manifest per run: Git commit, dirty flag, Python & lib versions, seed values, config, data file hashes, CLI args. Save as reports/manifest.json.\n\n\n\n24.3.3 Deterministic training & scoring\n\nTorch: torch.backends.cudnn.deterministic=True, benchmark=False, torch.use_deterministic_algorithms(True) (when possible).\nDataLoader: seeded generator + worker_init_fn to seed workers.\nSort/group data deterministically (sort_values([\"ticker\",\"date\"])).\n\n\n\n24.3.4 Common pitfalls\n\nHidden internet calls (APIs) during scoring.\nUnpinned dependencies.\nMissing seeds in numpy, random, torch.\nDirty working tree (uncommitted changes).",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Session 24 — Reproducibility audit & optional FastAPI</span>"
    ]
  },
  {
    "objectID": "lec24.html#inclass-lab-35-min",
    "href": "lec24.html#inclass-lab-35-min",
    "title": "24  Session 24 — Reproducibility audit & optional FastAPI",
    "section": "24.4 In‑class lab (35 min)",
    "text": "24.4 In‑class lab (35 min)\n\nRun each block as its own cell in Colab. Replace REPO_NAME if needed. Partners pair‑audit and open issues for anything not reproducible.\n\n\n24.4.1 0) Mount & prepare dirs\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME = \"unified-stocks-teamX\"  # &lt;- change\nBASE_DIR  = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR  = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, sys, platform, subprocess, json, hashlib, time\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"reports\",\"models\",\"scripts\",\"docs\",\"docs/figs\",\"data/processed\",\"data/raw\",\".github/workflows\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd(), \"| Python:\", sys.version.split()[0], \"| OS:\", platform.system())\n\n\n24.4.2 1) Add a Repro Audit Checklist to the repo\nCreate docs/repro_audit_checklist.md:\n# Reproducibility Audit Checklist (Session 24)\n\n## Environment\n- [ ] Repo fresh-clones and installs: `make env && pip install -e .`\n- [ ] `requirements.txt` is pinned (== versions); `requirements-lock.txt` exists\n- [ ] Python version and OS noted in README\n\n## Data\n- [ ] `data/processed/*` has checksums recorded\n- [ ] No hidden network calls during scoring (`offline`/cached)\n- [ ] Train/val split code is deterministic and documented\n\n## Seeds & determinism\n- [ ] Seeds set for `random`, `numpy`, `torch`; DataLoader worker init seeded\n- [ ] Torch deterministic flags set; no non-deterministic ops used\n- [ ] Sorting by `[\"ticker\",\"date\"]` before windowing\n\n## CLI & artifacts\n- [ ] `python -m projectname.cli split-info` works\n- [ ] `python -m projectname.cli score --model lin_lags` writes CSV\n- [ ] Two back-to-back runs produce identical metrics (within tolerance)\n- [ ] A `reports/manifest.json` records commit hash, env, checksums, CLI args\n\n## CI (fast)\n- [ ] A small CI job runs `python -m projectname.cli score --model lin_lags`\n- [ ] CI artifacts uploaded or printed\n\n## Known deviations (explain/ticket links)\n- [ ] ...\n\n\n24.4.3 2) Add a simple data checksum helper (one cell to create the file)\nfrom pathlib import Path\nimport hashlib, json, glob\n\ndef file_sha256(path: str|Path) -&gt; str:\n    h = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(1&lt;&lt;20), b\"\"):\n            h.update(chunk)\n    return h.hexdigest()\n\nchecks = {}\nfor pat in [\"data/processed/*.parquet\",\"data/raw/*.parquet\",\"data/*.db\"]:\n    for p in glob.glob(pat):\n        checks[p] = {\"sha256\": file_sha256(p), \"bytes\": Path(p).stat().st_size}\n\nPath(\"reports/data_checksums.json\").write_text(json.dumps(checks, indent=2))\nprint(\"Wrote reports/data_checksums.json with\", len(checks), \"entries\")\n\n\n24.4.4 3) Automated reproducibility audit script\nCreate scripts/repro_audit.py:\n#!/usr/bin/env python\nfrom __future__ import annotations\nimport os, sys, json, time, platform, subprocess, hashlib, glob\nfrom pathlib import Path\n\nTOL = 1e-12  # numerical tolerance for equality of metrics\n\ndef sha256_text(s: str) -&gt; str:\n    import hashlib\n    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n\ndef run(cmd: list[str]) -&gt; tuple[int, str, str]:\n    p = subprocess.run(cmd, capture_output=True, text=True)\n    return p.returncode, p.stdout.strip(), p.stderr.strip()\n\ndef git_info() -&gt; dict:\n    def _get(args):\n        rc,out,err = run([\"git\"]+args)\n        return out if rc==0 else f\"NA({err})\"\n    return {\n        \"commit\": _get([\"rev-parse\",\"HEAD\"]),\n        \"dirty\": _get([\"status\",\"--porcelain\"]) != \"\",\n        \"branch\": _get([\"rev-parse\",\"--abbrev-ref\",\"HEAD\"]),\n        \"remote\": _get([\"remote\",\"-v\"]),\n    }\n\ndef pip_freeze() -&gt; str:\n    rc,out,err = run([sys.executable,\"-m\",\"pip\",\"freeze\",\"--exclude-editable\"])\n    return out\n\ndef read_csv_first_row(path: Path) -&gt; dict:\n    import pandas as pd\n    df = pd.read_csv(path)\n    if len(df)==0:\n        return {}\n    return df.iloc[:1].to_dict(orient=\"records\")[0]\n\ndef main():\n    start = time.time()\n    manifest = {\n        \"ts\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"python\": sys.version.split()[0],\n        \"platform\": platform.platform(),\n        \"git\": git_info(),\n    }\n\n    # Add freeze & hash\n    freeze = pip_freeze()\n    Path(\"reports\").mkdir(exist_ok=True)\n    Path(\"requirements-lock.txt\").write_text(freeze)\n    manifest[\"pip_freeze_sha256\"] = sha256_text(freeze)\n\n    # Data checksums (optional precomputed)\n    checks_path = Path(\"reports/data_checksums.json\")\n    if checks_path.exists():\n        manifest[\"data_checksums\"] = json.loads(checks_path.read_text())\n    else:\n        manifest[\"data_checksums\"] = {}\n\n    # 1) Split info should work\n    rc,out,err = run([sys.executable,\"-m\",\"projectname.cli\",\"split-info\"])\n    manifest[\"split_info_ok\"] = (rc==0)\n    if rc!=0:\n        print(\"split-info failed:\", err, file=sys.stderr)\n\n    # 2) Score twice (lin_lags) and compare metrics\n    out1 = Path(\"reports/cli_linlags_run1.csv\"); out2 = Path(\"reports/cli_linlags_run2.csv\")\n    cmd = [sys.executable,\"-m\",\"projectname.cli\",\"score\",\"--model\",\"lin_lags\",\"--out\",str(out1)]\n    rc1,_,err1 = run(cmd)\n    rc2,_,err2 = run([*cmd[:-1], str(out2)])\n    manifest[\"score_run1_ok\"] = (rc1==0); manifest[\"score_run2_ok\"] = (rc2==0)\n    if rc1!=0 or rc2!=0:\n        print(\"score failed:\", err1 or err2, file=sys.stderr)\n\n    # Compare metrics (read printed CSVs)\n    import pandas as pd\n    ok_equal = True\n    try:\n        d1 = pd.read_csv(out1)\n        d2 = pd.read_csv(out2)\n        # Aggregate micro MAE across both runs\n        import numpy as np\n        mae1 = float(np.mean(np.abs(d1[\"y_true\"].values - d1[\"yhat\"].values)))\n        mae2 = float(np.mean(np.abs(d2[\"y_true\"].values - d2[\"yhat\"].values)))\n        manifest[\"metrics\"] = {\"run1_mae\": mae1, \"run2_mae\": mae2, \"abs_diff\": abs(mae1-mae2)}\n        ok_equal = abs(mae1 - mae2) &lt;= TOL\n    except Exception as e:\n        ok_equal = False\n        manifest[\"metrics_error\"] = repr(e)\n\n    manifest[\"ok_equal_within_tol\"] = ok_equal\n    manifest[\"elapsed_sec\"] = round(time.time()-start, 2)\n\n    mpath = Path(\"reports/manifest.json\")\n    mpath.write_text(json.dumps(manifest, indent=2))\n    print(\"Wrote\", mpath)\n\n    # Non-zero exit on failure\n    if not (manifest[\"split_info_ok\"] and manifest[\"score_run1_ok\"] and manifest[\"score_run2_ok\"] and ok_equal):\n        print(\"Repro audit FAILED (see reports/manifest.json).\", file=sys.stderr)\n        sys.exit(2)\n    else:\n        print(\"Repro audit PASSED.\")\n\nif __name__ == \"__main__\":\n    main()\nMake it executable and run:\n%%bash\nchmod +x scripts/repro_audit.py\npython scripts/repro_audit.py || true\nYou should see reports/manifest.json, requirements-lock.txt, and two reports/cli_linlags_run*.csv. If the audit fails, open a GitHub issue with the error and your manifest attached.\n\n\n24.4.5 4) Add Makefile + CI snippets (fast)\nAppend to Makefile:\n.PHONY: lock repro-audit\nlock: ## Write requirements-lock.txt from pip freeze\n\\tpip freeze --exclude-editable &gt; requirements-lock.txt\n\nrepro-audit: ## Run fast reproducibility audit (lin_lags only)\n\\tpython scripts/repro_audit.py\nAdd a tiny CI job (append to .github/workflows/ci.yml):\n- name: Repro audit (fast)\n  run: |\n    pip install -e \".[dev]\"\n    python scripts/repro_audit.py",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Session 24 — Reproducibility audit & optional FastAPI</span>"
    ]
  },
  {
    "objectID": "lec24.html#optional-fastapi-hello-score-10-min-in-class-copypaste-runnable",
    "href": "lec24.html#optional-fastapi-hello-score-10-min-in-class-copypaste-runnable",
    "title": "24  Session 24 — Reproducibility audit & optional FastAPI",
    "section": "24.5 (Optional) FastAPI “hello score” (10 min in class; copy‑paste runnable)",
    "text": "24.5 (Optional) FastAPI “hello score” (10 min in class; copy‑paste runnable)\n\n24.5.1 A. Minimal service\nCreate serve/app.py:\n# serve/app.py\nfrom __future__ import annotations\nfrom fastapi import FastAPI, HTTPException, Query\nfrom pydantic import BaseModel\nfrom pathlib import Path\nimport pandas as pd\nfrom projectname.config import load_config\nfrom projectname.utils.splits import make_rolling_origin_splits\nfrom projectname.utils.metrics import mae, smape\nfrom projectname.models.baselines import predict_naive, predict_lin_lags\n\napp = FastAPI(title=\"Unified Stocks Scoring (Educational)\")\n\nclass ScoreRequest(BaseModel):\n    model: str = \"lin_lags\"          # \"naive\" | \"lin_lags\"\n    split: int = 1                   # use rolling-origin split id\n    start: str | None = None         # optional override\n    end: str | None = None\n\ndef _load_df(cfg):\n    df = pd.read_parquet(cfg.data.features_file)\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    df[\"ticker\"] = df[\"ticker\"].astype(\"category\")\n    return df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n\n@app.get(\"/health\")\ndef health():\n    import subprocess\n    def _g(args): \n        try: return subprocess.check_output([\"git\"]+args, text=True).strip()\n        except Exception: return \"NA\"\n    return {\"status\":\"ok\",\"commit\":_g([\"rev-parse\",\"HEAD\"])}\n\n@app.post(\"/score\")\ndef score(req: ScoreRequest):\n    cfg = load_config(\"config/config.yaml\")\n    df = _load_df(cfg)\n    feats = [c for c in cfg.features_use if c in df.columns]\n\n    if req.start and req.end:\n        vstart, vend = pd.to_datetime(req.start), pd.to_datetime(req.end)\n        dates = sorted(df[\"date\"].unique())\n        # train = everything before vstart respecting train_min (no embargo here for simplicity)\n        idx = next((i for i,d in enumerate(dates) if d&gt;=vstart), None)\n        if idx is None or idx &lt; cfg.eval.train_min:\n            raise HTTPException(400, detail=\"Not enough history before start.\")\n        tstart, tend = dates[0], dates[idx-1]\n    else:\n        splits = make_rolling_origin_splits(df[\"date\"], cfg.eval.train_min, cfg.eval.val_size, cfg.eval.val_size, cfg.eval.embargo)\n        try: tstart, tend, vstart, vend = splits[req.split-1]\n        except IndexError: raise HTTPException(400, detail=\"Split not available.\")\n\n    train_df = df[(df[\"date\"]&gt;=tstart)&(df[\"date\"]&lt;=tend)].copy()\n    val_df   = df[(df[\"date\"]&gt;=vstart)&(df[\"date\"]&lt;=vend)].copy()\n    if len(val_df)==0: raise HTTPException(400, detail=\"Empty validation slice\")\n\n    if req.model == \"naive\":\n        preds = predict_naive(val_df)\n    elif req.model == \"lin_lags\":\n        if not feats: raise HTTPException(400, detail=\"No feature columns available.\")\n        preds = predict_lin_lags(train_df, val_df, feats)\n    else:\n        raise HTTPException(400, detail=\"Model not supported in demo service\")\n\n    y, yhat = preds[\"y_true\"], preds[\"yhat\"]\n    return {\"model\": req.model,\n            \"split\": req.split,\n            \"start\": str(vstart.date()), \"end\": str(vend.date()),\n            \"n\": int(len(preds)),\n            \"mae\": mae(y,yhat), \"smape\": smape(y,yhat)}\nInstall & run locally:\n%%bash\npip install fastapi \"uvicorn[standard]\"\npython -m uvicorn serve.app:app --reload --port 8000\nTest (from another shell):\n# health\ncurl -s localhost:8000/health | jq\n# score\ncurl -s -X POST localhost:8000/score -H \"Content-Type: application/json\" \\\n     -d '{\"model\":\"lin_lags\",\"split\":1}' | jq\n\nNote: In Colab you can run uvicorn, but exposing the port publicly requires a tunnel; treat this as local optional.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Session 24 — Reproducibility audit & optional FastAPI</span>"
    ]
  },
  {
    "objectID": "lec24.html#wrapup-10-min",
    "href": "lec24.html#wrapup-10-min",
    "title": "24  Session 24 — Reproducibility audit & optional FastAPI",
    "section": "24.6 Wrap‑up (10 min)",
    "text": "24.6 Wrap‑up (10 min)\n\nReproducibility is a discipline, not an afterthought: lock the env, freeze data, write manifests, and keep the audit fast.\nCI should run the fast path (lin_lags) to guard your repo.\nOptional FastAPI shows how to wrap your scoring pipeline for interactive demos.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Session 24 — Reproducibility audit & optional FastAPI</span>"
    ]
  },
  {
    "objectID": "lec24.html#homework-due-before-session-25",
    "href": "lec24.html#homework-due-before-session-25",
    "title": "24  Session 24 — Reproducibility audit & optional FastAPI",
    "section": "24.7 Homework (due before Session 25)",
    "text": "24.7 Homework (due before Session 25)\nGoal: Produce a release candidate with a reproducibility statement, tag v1.0-rc, and resolve/triage audit issues.\n\n24.7.1 A. Update README (repro statement)\nAdd a “Reproducibility” section to README.md:\n## Reproducibility\n\n- Fresh clone:\nmake env pip install -e “.[dev]” python -m projectname.cli split-info python -m projectname.cli score –model lin_lags –out reports/cli_linlags.csv\n- Audit:\npython scripts/repro_audit.py\n- We commit `requirements.txt` (pinned) and `requirements-lock.txt` (pip freeze hash: see `reports/manifest.json`).\n- Data checksums in `reports/data_checksums.json`.\n\n\n24.7.2 B. Generate a simple changelog automatically\nCreate scripts/make_changelog.py:\n#!/usr/bin/env python\nimport subprocess, datetime\nfrom pathlib import Path\n\ndef run(args): \n    return subprocess.check_output(args, text=True).strip()\n\ndef main():\n    since = \"v0.0.0\"\n    # If previous tag exists, use it\n    try:\n        last = run([\"git\",\"describe\",\"--tags\",\"--abbrev=0\"])\n        since = last\n    except Exception:\n        pass\n    log = run([\"git\",\"log\",f\"{since}..HEAD\",\"--pretty=format:- %h %s (%an)\"])\n    today = datetime.date.today().isoformat()\n    header = f\"## {today} v1.0-rc\\n\\n\"\n    body = header + (log if log else \"- Initial release candidate\\n\")\n    p = Path(\"CHANGELOG.md\")\n    if p.exists():\n        old = p.read_text()\n        p.write_text(body + \"\\n\" + old)\n    else:\n        p.write_text(\"# Changelog\\n\\n\" + body)\n    print(\"Updated CHANGELOG.md\")\n\nif __name__ == \"__main__\":\n    main()\nRun:\n%%bash\nchmod +x scripts/make_changelog.py\npython scripts/make_changelog.py\ngit add CHANGELOG.md\ngit commit -m \"chore: update changelog for v1.0-rc\"\n\n\n24.7.3 C. Create & push the tag\n%%bash\ngit tag -a v1.0-rc -m \"Release candidate for reproducible pipeline\"\ngit push origin v1.0-rc\n\n\n24.7.4 D. Close or triage audit issues\n\nFor each checklist failure, open or update a GitHub issue with:\n\nError snippet, your reports/manifest.json, steps to reproduce.\nLabel: repro, rc.\nEither closed with a fix (commit hash referenced) or triaged with scope and owner.\n\n\n\n\n24.7.5 E. (Optional) Add a CI artifact step to upload the manifest\nAppend to .github/workflows/ci.yml:\n- name: Upload manifest\n  uses: actions/upload-artifact@v4\n  with:\n    name: repro-manifest\n    path: reports/manifest.json",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Session 24 — Reproducibility audit & optional FastAPI</span>"
    ]
  },
  {
    "objectID": "lec24.html#instructor-checklist-before-class",
    "href": "lec24.html#instructor-checklist-before-class",
    "title": "24  Session 24 — Reproducibility audit & optional FastAPI",
    "section": "24.8 Instructor checklist (before class)",
    "text": "24.8 Instructor checklist (before class)\n\nDry‑run scripts/repro_audit.py on a fresh runtime; verify it passes.\nEnsure projectname.cli works without internet (no hidden API calls).\nBe ready to demo FastAPI locally if time permits.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Session 24 — Reproducibility audit & optional FastAPI</span>"
    ]
  },
  {
    "objectID": "lec24.html#grading-passrevise",
    "href": "lec24.html#grading-passrevise",
    "title": "24  Session 24 — Reproducibility audit & optional FastAPI",
    "section": "24.9 Grading (pass/revise)",
    "text": "24.9 Grading (pass/revise)\n\nscripts/repro_audit.py runs and writes reports/manifest.json.\nrequirements-lock.txt present and hash recorded in manifest.\nAt least one issue opened (if a failure) or a note “All checks passed” with manifest attached.\nCHANGELOG.md updated by script; tag v1.0-rc exists and is pushed.\nREADME has a clear repro statement.\n\n\n\n24.9.1 Notes and gotchas\n\nTolerance for metric equality is set very tight (1e-12). If your environment shows tiny FP drift, relax to 1e-9.\nIf you rely on GPU kernels that are non‑deterministic, keep the audit on CPU or replace those ops.\nIf any models/…pt checkpoints are referenced, keep them out of the audit path (we only audit scoring with lin_lags here to keep CI fast).\n\nThis session gets you to a release‑candidate state: a repo that fresh‑clones and scores deterministically, with a manifest, a changelog, and an optional service wrapper, ready for Session 25’s communication & poster work.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Session 24 — Reproducibility audit & optional FastAPI</span>"
    ]
  },
  {
    "objectID": "lec25.html",
    "href": "lec25.html",
    "title": "25  Session 25 — Poster + Abstract Workshop",
    "section": "",
    "text": "25.1 Session 25 — Poster + Abstract Workshop (75 min)\nBelow is a complete lecture package for Session 25 — Poster + Abstract Workshop (75 minutes). It includes a timed agenda, instructor talking points (slides outline), a Colab‑friendly in‑class lab with copy‑paste code that generates three polished figures and a Quarto poster skeleton, plus homework with copy‑paste code to validate the abstract length and render a draft poster.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Session 25 — Poster + Abstract Workshop</span>"
    ]
  },
  {
    "objectID": "lec25.html#session-25-poster-abstract-workshop-75-min",
    "href": "lec25.html#session-25-poster-abstract-workshop-75-min",
    "title": "25  Session 25 — Poster + Abstract Workshop",
    "section": "",
    "text": "25.1.1 Learning goals\nBy the end of class, students can:\n\nCraft a concise story arc (Problem → Data → Method → Evaluation → Results → Limits → Takeaways).\nProduce and polish three key figures:\n\nModel comparison (baseline vs LSTM/GRU vs tiny‑Transformer/TS‑GPT).\nRegime breakdown (errors by volatility regime).\nAblation (TS‑GPT hyperparam sweep).\n\nAssemble a one‑page poster (Quarto HTML or PDF) and draft a 250‑word abstract.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Session 25 — Poster + Abstract Workshop</span>"
    ]
  },
  {
    "objectID": "lec25.html#agenda-75-min",
    "href": "lec25.html#agenda-75-min",
    "title": "25  Session 25 — Poster + Abstract Workshop",
    "section": "25.2 Agenda (75 min)",
    "text": "25.2 Agenda (75 min)\n\n(10 min) Slides: poster anatomy, audience & narrative, figure design rules.\n(10 min) Slides: abstract structure, model‑card notes & caveats.\n(40 min) In‑class lab: generate three figures → create reports/poster.qmd → render draft.\n(15 min) Share & critique: 3‑minute lightning review per team; assign homework.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Session 25 — Poster + Abstract Workshop</span>"
    ]
  },
  {
    "objectID": "lec25.html#slides-talking-points-drop-into-your-deck",
    "href": "lec25.html#slides-talking-points-drop-into-your-deck",
    "title": "25  Session 25 — Poster + Abstract Workshop",
    "section": "25.3 Slides — talking points (drop into your deck)",
    "text": "25.3 Slides — talking points (drop into your deck)\n\n25.3.1 1) Poster anatomy (for non‑experts)\n\nTitle (problem & claim), Authors/Affiliation, Contact.\nLeft column: Problem & Data (1–2 bullets each), Method sketch (one diagram).\nCenter: Key results (3 figures), 1–2 sentences per figure (what/why it matters).\nRight column: Limits (leakage, data shifts), Ethics & caveats, Takeaways.\n\n\n\n25.3.2 2) Figure design (fast rules)\n\nUse clear labels/units, readable fonts (≥ 11–12 pt on poster), concise legends.\nOrder bars left→right by magnitude; add error bars where possible.\nAvoid clutter: minimal grid; colorblind‑safe if you pick colors; don’t rely only on color (shapes/line styles help).\nAlways include a 10‑word caption and a one‑line takeaway near the figure.\n\n\n\n25.3.3 3) Abstract (≈250 words, plain English)\n\nMotivation in 1–2 sentences; data & setup in 1–2; methods in 2–3; results with numbers; limits and next steps.\nNo internal jargon; define acronyms (e.g., MAE = mean absolute error) on first use.\n\n\n\n25.3.4 4) Model card essentials (1 small box on the poster)\n\nIntended use (educational baseline forecasting).\nData (tickers & dates, pre‑processing, leakage controls).\nMetrics (MAE, sMAPE, by regime).\nLimitations (non‑stationarity, survivorship bias, no live trading).",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Session 25 — Poster + Abstract Workshop</span>"
    ]
  },
  {
    "objectID": "lec25.html#inclass-lab-40-min-colabfriendly",
    "href": "lec25.html#inclass-lab-40-min-colabfriendly",
    "title": "25  Session 25 — Poster + Abstract Workshop",
    "section": "25.4 In‑class lab (40 min, Colab‑friendly)",
    "text": "25.4 In‑class lab (40 min, Colab‑friendly)\n\nRun each block as its own cell. Change REPO_NAME if needed.\n\n\n25.4.1 0) Setup & folders\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME = \"unified-stocks-teamX\"  # &lt;- change to your repo name\nBASE_DIR  = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR  = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, sys, platform\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"reports\",\"docs/figs\",\"scripts\",\"tests\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd(), \"| Python:\", sys.version.split()[0], \"| OS:\", platform.system())\n\n\n25.4.2 1) Script to build poster figures (create scripts/make_poster_figs.py)\n%%bash\ncat &gt; scripts/make_poster_figs.py &lt;&lt; 'PY'\n#!/usr/bin/env python\nfrom __future__ import annotations\nimport os, math, glob, json\nfrom pathlib import Path\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n\nFIGDIR = Path(\"docs/figs\"); FIGDIR.mkdir(parents=True, exist_ok=True)\nREPDIR = Path(\"reports\"); REPDIR.mkdir(exist_ok=True)\n\ndef _safe_read_csv(path: str|Path) -&gt; pd.DataFrame|None:\n    p = Path(path)\n    try:\n        if p.exists():\n            return pd.read_csv(p)\n    except Exception as e:\n        print(\"WARN cannot read\", p, e)\n    return None\n\ndef _synthesize_predictions(n=800, tickers=(\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\",\"META\")):\n    rng = np.random.default_rng(0)\n    frames=[]\n    for t in tickers:\n        y = rng.normal(0, 0.012, size=n).astype(\"float32\")\n        e_lin = rng.normal(0, 0.009, size=n).astype(\"float32\")\n        df = pd.DataFrame({\"ticker\":t, \"y_true\":y, \"yhat\":y+e_lin})\n        frames.append(df)\n    out = pd.concat(frames, ignore_index=True)\n    out[\"date\"] = pd.date_range(\"2023-01-03\", periods=len(out), freq=\"B\")\n    out[\"method\"] = \"lin_lags\"\n    return out\n\ndef _mae(y,yhat): return float(np.mean(np.abs(np.asarray(y)-np.asarray(yhat))))\n\ndef model_compare():\n    \"\"\"Build a bar chart comparing MAE across models (aggregate & per-ticker if available).\"\"\"\n    # Load LIN_LAGS predictions from CLI\n    lin = _safe_read_csv(\"reports/cli_linlags.csv\")\n    if lin is None or not {\"y_true\",\"yhat\",\"ticker\"}.issubset(lin.columns):\n        print(\"Using synthetic LIN_LAGS predictions.\")\n        lin = _synthesize_predictions()\n    lin[\"model\"] = \"lin_lags\"\n    m_lin = _mae(lin[\"y_true\"], lin[\"yhat\"])\n\n    # GRU (per-ticker metrics if available)\n    gru_pt = _safe_read_csv(\"reports/unified_gru_split1_per_ticker.csv\")\n    m_gru = gru_pt[\"mae\"].mean() if (gru_pt is not None and \"mae\" in gru_pt.columns and len(gru_pt)&gt;0) else None\n\n    # TS-GPT (aggregate metrics)\n    tsgpt = _safe_read_csv(\"reports/tsgpt_split1_metrics.csv\")\n    if tsgpt is not None and \"val_mae\" in tsgpt.columns and len(tsgpt)&gt;0:\n        m_tsgpt = float(tsgpt[\"val_mae\"].iloc[-1])\n    else:\n        m_tsgpt = None\n\n    rows=[{\"model\":\"lin_lags\",\"val_mae\":m_lin}]\n    if m_gru is not None:   rows.append({\"model\":\"gru\",\"val_mae\":m_gru})\n    if m_tsgpt is not None: rows.append({\"model\":\"tsgpt\",\"val_mae\":m_tsgpt})\n    comp = pd.DataFrame(rows).sort_values(\"val_mae\")\n    comp.to_csv(REPDIR/\"model_compare.csv\", index=False)\n\n    # Bar plot\n    plt.figure(figsize=(6,3.2))\n    x = np.arange(len(comp))\n    plt.bar(x, comp[\"val_mae\"].values)\n    plt.xticks(x, comp[\"model\"].tolist())\n    plt.ylabel(\"Validation MAE\")\n    plt.title(\"Model Comparison (lower is better)\")\n    plt.tight_layout()\n    plt.savefig(FIGDIR/\"model_compare.png\", dpi=160)\n    plt.close()\n    print(\"Wrote\", FIGDIR/\"model_compare.png\")\n\ndef regime_breakdown():\n    \"\"\"Compute MAE by volatility regime using LIN_LAGS predictions + returns variance proxy.\"\"\"\n    lin = _safe_read_csv(\"reports/cli_linlags.csv\")\n    base = None\n    # Try to load a returns file to compute rolling volatility\n    for cand in [\"data/processed/features_v1.parquet\",\"data/processed/returns.parquet\"]:\n        try:\n            if Path(cand).exists():\n                base = pd.read_parquet(cand)\n                break\n        except Exception: pass\n    if lin is None:\n        lin = _synthesize_predictions(n=400)\n    if base is None or not {\"ticker\",\"date\"}.issubset(base.columns):\n        # fabricate a simple volatility proxy from y_true\n        print(\"Volatility base missing — using synthetic regimes from residual std.\")\n        tmp = lin.copy()\n        tmp[\"resid\"] = lin[\"y_true\"] - lin[\"yhat\"]\n        tmp[\"vol\"] = tmp.groupby(\"ticker\")[\"resid\"].transform(lambda s: s.rolling(20, min_periods=20).std())\n        base = tmp[[\"ticker\",\"date\",\"vol\"]]\n    else:\n        base = base.sort_values([\"ticker\",\"date\"]).copy()\n        if \"log_return\" in base.columns:\n            base[\"vol\"] = base.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).std().reset_index(level=0,drop=True)\n        elif \"r_1d\" in base.columns:\n            base[\"vol\"] = base.groupby(\"ticker\")[\"r_1d\"].rolling(20, min_periods=20).std().reset_index(level=0,drop=True)\n        else:\n            base[\"vol\"] = base.groupby(\"ticker\").cumcount() * 0.0  # fallback\n        base = base[[\"ticker\",\"date\",\"vol\"]]\n    # Merge\n    df = lin.merge(base, on=[\"ticker\",\"date\"], how=\"left\")\n    q1, q2 = df[\"vol\"].quantile([1/3, 2/3])\n    def regime(v):\n        if v&lt;=q1: return \"low\"\n        if v&lt;=q2: return \"med\"\n        return \"high\"\n    df[\"regime\"] = df[\"vol\"].apply(regime)\n    agg = df.groupby(\"regime\").apply(lambda g: _mae(g[\"y_true\"], g[\"yhat\"])).reset_index(name=\"mae\")\n    agg = agg.sort_values(\"mae\")\n    agg.to_csv(REPDIR/\"regime_breakdown.csv\", index=False)\n\n    # Bar plot\n    plt.figure(figsize=(6,3.2))\n    x = np.arange(len(agg))\n    plt.bar(x, agg[\"mae\"].values)\n    plt.xticks(x, agg[\"regime\"].tolist())\n    plt.ylabel(\"MAE\")\n    plt.title(\"Error by Volatility Regime\")\n    plt.tight_layout()\n    plt.savefig(FIGDIR/\"regime_breakdown.png\", dpi=160)\n    plt.close()\n    print(\"Wrote\", FIGDIR/\"regime_breakdown.png\")\n\ndef ablation_plot():\n    \"\"\"Plot a simple heatmap for TS-GPT ablations if available; else synthesize.\"\"\"\n    paths = [p for p in [\"reports/tsgpt_ablation_summary.csv\",\"reports/tinygpt_ablation_summary.csv\"] if Path(p).exists()]\n    if paths:\n        ab = pd.read_csv(paths[0])\n    else:\n        # synthesize small ablation grid\n        rng = np.random.default_rng(1)\n        ab = pd.DataFrame([\n            {\"ctx\": c, \"n_head\": h, \"dropout\": d, \"val_mae\": 0.010 + rng.normal(0,0.0008)}\n            for c in [32,64] for h in [2,4] for d in [0.0,0.1]\n        ])\n    # Prefer ctx vs n_head for each dropout (two panels by saving two images)\n    for d in sorted(ab[\"dropout\"].unique()):\n        sub = ab[ab[\"dropout\"]==d].copy()\n        ctxs = sorted(sub[\"ctx\"].unique())\n        heads = sorted(sub[\"n_head\"].unique())\n        grid = np.full((len(ctxs), len(heads)), np.nan)\n        for i,c in enumerate(ctxs):\n            for j,h in enumerate(heads):\n                rows = sub[(sub[\"ctx\"]==c) & (sub[\"n_head\"]==h)]\n                if len(rows): grid[i,j] = float(rows.iloc[-1][\"val_mae\"])\n        plt.figure(figsize=(5.2,3.8))\n        plt.imshow(grid, aspect=\"auto\", origin=\"lower\")\n        plt.colorbar(label=\"Val MAE\")\n        plt.xticks(np.arange(len(heads)), [str(h) for h in heads]); plt.xlabel(\"n_head\")\n        plt.yticks(np.arange(len(ctxs)), [str(c) for c in ctxs]);   plt.ylabel(\"context length\")\n        plt.title(f\"TS-GPT Ablation (dropout={d})\")\n        plt.tight_layout()\n        fn = FIGDIR/f\"ablation_dropout_{str(d).replace('.','_')}.png\"\n        plt.savefig(fn, dpi=160)\n        plt.close()\n        print(\"Wrote\", fn)\n    ab.to_csv(REPDIR/\"ablation_clean.csv\", index=False)\n\ndef main():\n    model_compare()\n    regime_breakdown()\n    ablation_plot()\n    print(\"Done. Figures in\", FIGDIR)\n\nif __name__ == \"__main__\":\n    main()\nPY\nchmod +x scripts/make_poster_figs.py\nRun it:\n%%bash\npython scripts/make_poster_figs.py\nls -l docs/figs | head -n 10\n\n\n25.4.3 2) Create a Quarto poster skeleton (reports/poster.qmd)\n%%bash\ncat &gt; reports/poster.qmd &lt;&lt; 'QMD'\n---\ntitle: \"Unified Stocks: Tiny Transformers vs Classical Baselines\"\nauthor: \"Team — Data Science Productivity Tools\"\ndate: \"`r format(Sys.Date(), '%Y-%m-%d')`\"\nformat:\n  html:\n    page-layout: full\n    toc: false\n    number-sections: false\n    theme: default\n    df-print: paged\n    smooth-scroll: true\nengine: knitr\neditor: source\n---\n\n::: {.callout-note}\n**Educational project (not trading advice).**  \nData and results are for demonstration of tools and methodology.\n:::\n\n## Problem\nShort‑horizon return forecasting is noisy. We build a **unified model** across multiple tickers and compare **classical baselines** with a **tiny Transformer** under strict leakage controls.\n\n## Data\n- Daily prices for a static list of tickers.  \n- Features: lags, rolling z‑scores, 20‑day volatility, calendar effects.  \n- Splits: rolling‑origin with embargo; metrics reported on validation slices.\n\n## Methods (1‑paragraph sketch)\n- Baselines: **naive** (ŷ = return_t), **lin_lags** (per‑ticker linear regression).  \n- Sequence models: **GRU** (multi‑asset with ticker IDs) and **TS‑GPT** (feature projection + causal attention).  \n- Loss: MAE/Huber. No look‑ahead; scaler fit on TRAIN only.\n\n## Results\n\n### Model comparison (MAE)\n![Comparison of validation MAE across models. Lower is better.](../docs/figs/model_compare.png){fig-alt=\"Bar chart: models vs MAE\" width=\"70%\"}\n\n**Takeaway.** The unified sequence model improves over lin_lags on this split (magnitude varies by dataset).\n\n### Error by volatility regime\n![MAE by regime (low/med/high volatility).](../docs/figs/regime_breakdown.png){fig-alt=\"Bar chart: volatility regimes vs MAE\" width=\"70%\"}\n\n**Takeaway.** Errors increase in high‑volatility regimes; compare models at equal regimes to avoid apples‑to‑oranges.\n\n### TS‑GPT ablation (context × heads)\n![Validation MAE heatmaps across context and heads (one per dropout).](../docs/figs/ablation_dropout_0_0.png){fig-alt=\"Heatmap: ctx vs n_head, dropout=0.0\" width=\"45%\"}\n![Validation MAE heatmaps across context and heads (one per dropout).](../docs/figs/ablation_dropout_0_1.png){fig-alt=\"Heatmap: ctx vs n_head, dropout=0.1\" width=\"45%\"}\n\n**Takeaway.** Longer context or more heads can help slightly, but returns are noisy—watch for overfitting.\n\n## Limits & Ethics\n- Non‑stationarity; survivorship bias minimized via static list and fixed history but not eliminated.  \n- Not suitable for trading; missing costs/impact; academic metrics only.\n\n## Model Card (short)\n- **Intended use:** teaching pipeline/tools.  \n- **Data:** daily OHLCV; engineered features; static ticker universe.  \n- **Metrics:** MAE, sMAPE, regime‑wise MAE.  \n- **Limitations:** no risk management; unstable out‑of‑sample.\n\n## References\n- Course repository and documentation; standard ML/TS texts.\n\n## Contact\n- Team GitHub and email here.\nQMD\nRender (HTML; PDF optional if LaTeX is available):\n%%bash\nquarto render reports/poster.qmd || echo \"If Quarto not available here, render locally later.\"\n\n\n25.4.4 3) Seed an abstract draft file and a word‑count checker\n%%bash\ncat &gt; reports/abstract.md &lt;&lt; 'MD'\nWe study short‑horizon stock return forecasting as an educational exercise in reproducible data science tooling. Using a static set of equities and daily bars, we engineer causal features (lags, rolling volatility, calendar effects) and enforce time‑based splits with an embargo to prevent leakage. We compare per‑ticker linear baselines to unified sequence models, including a GRU with ticker embeddings and a tiny Transformer adapted for real‑valued time series (TS‑GPT).\n\nModels are trained with MAE/Huber loss on expanding windows; evaluation uses MAE and sMAPE aggregated across tickers and by volatility regimes. On our validation split, the unified sequence model matches or modestly outperforms the linear baseline, especially in medium‑volatility regimes; effects shrink in high‑volatility periods. Ablations suggest small gains from longer context, with diminishing returns beyond 64 steps.\n\nThis course project emphasizes tools over profits: deterministic pipelines, tests, continuous integration, and a Typer CLI that reproduces results from a fresh clone. The work is limited by non‑stationarity, survivorship bias, and absence of transaction costs. Future work includes multi‑horizon forecasting, macro features, uncertainty estimates, and broader asset coverage. This is not trading advice.\nMD\n\ncat &gt; scripts/check_abstract_length.py &lt;&lt; 'PY'\n#!/usr/bin/env python\nimport re, sys\nfrom pathlib import Path\n\ndef count_words(text: str) -&gt; int:\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    if not text: return 0\n    return len(text.split(\" \"))\n\ndef main(path=\"reports/abstract.md\"):\n    p = Path(path)\n    if not p.exists():\n        print(f\"Missing {p}\", file=sys.stderr); sys.exit(2)\n    words = count_words(p.read_text(encoding=\"utf-8\"))\n    print(f\"{p}: {words} words\")\n    if not (200 &lt;= words &lt;= 300):\n        print(\"Abstract must be between 200 and 300 words.\", file=sys.stderr)\n        sys.exit(2)\n\nif __name__ == \"__main__\":\n    main()\nPY\nchmod +x scripts/check_abstract_length.py\npython scripts/check_abstract_length.py || true\n\n\n25.4.5 4) Tiny test to ensure figures exist\n%%bash\ncat &gt; tests/test_poster_assets.py &lt;&lt; 'PY'\nfrom pathlib import Path\n\ndef test_poster_figs_exist():\n    for fn in [\"model_compare.png\", \"regime_breakdown.png\"]:\n        assert (Path(\"docs/figs\")/fn).exists(), f\"Missing {fn} — run scripts/make_poster_figs.py\"\nPY\npytest -q -k poster_assets || true",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Session 25 — Poster + Abstract Workshop</span>"
    ]
  },
  {
    "objectID": "lec25.html#wrapup-prompts-15-min",
    "href": "lec25.html#wrapup-prompts-15-min",
    "title": "25  Session 25 — Poster + Abstract Workshop",
    "section": "25.5 Wrap‑up prompts (15 min)",
    "text": "25.5 Wrap‑up prompts (15 min)\n\nLightning critique: Each team shows the 3 figures (30–60 seconds each).\nAsk: “What is the one sentence the viewer should remember?”\nConfirm the poster story arc is visible from 6 feet away: bold headings; few words; figures big.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Session 25 — Poster + Abstract Workshop</span>"
    ]
  },
  {
    "objectID": "lec25.html#homework-due-next-session",
    "href": "lec25.html#homework-due-next-session",
    "title": "25  Session 25 — Poster + Abstract Workshop",
    "section": "25.6 Homework (due next session)",
    "text": "25.6 Homework (due next session)\nDeliverables (push to repo):\n\nreports/poster.html (and reports/poster.pdf if you can render PDF) with the 3 figures embedded.\nreports/abstract.md (200–300 words) that matches the poster’s story.\nA short commit message: docs: poster draft + abstract (v1).\n\n\n25.6.1 A. Makefile targets (append to Makefile)\n.PHONY: poster-figs poster render-poster abstract-check\nposter-figs: ## Build all poster figures\n\\tpython scripts/make_poster_figs.py\n\nposter: poster-figs ## Render HTML poster\n\\tquarto render reports/poster.qmd\n\nrender-poster: poster\n\nabstract-check: ## Ensure abstract word count is 200-300\n\\tpython scripts/check_abstract_length.py\n\n\n25.6.2 B. Optional CI snippet (run only if Quarto available)\n# Append to .github/workflows/ci.yml\n- name: Poster assets\n  run: |\n    python scripts/make_poster_figs.py\n    python scripts/check_abstract_length.py\n\n\n25.6.3 C. (Optional) Extra figure (calibration / residuals)\nAdd this cell if you also saved predictions for GRU/TS‑GPT (e.g., reports/tsgpt_preds_split1.csv with date,ticker,y_true,yhat):\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt, pathlib\npred = None\nfor cand in [\"reports/tsgpt_preds_split1.csv\",\"reports/cli_linlags.csv\"]:\n    if pathlib.Path(cand).exists():\n        pred = pd.read_csv(cand); break\nif pred is not None and {\"y_true\",\"yhat\"}.issubset(pred.columns):\n    e = pred[\"y_true\"] - pred[\"yhat\"]\n    plt.figure(figsize=(6,3.2))\n    plt.hist(e.values, bins=50)\n    plt.xlabel(\"Residual\"); plt.ylabel(\"Count\"); plt.title(\"Residual distribution\")\n    plt.tight_layout(); pathlib.Path(\"docs/figs\").mkdir(parents=True, exist_ok=True)\n    plt.savefig(\"docs/figs/residuals_hist.png\", dpi=160); plt.close()\nThen include in poster.qmd:\n![Residual distribution (optional).](../docs/figs/residuals_hist.png){width=\"55%\"}",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Session 25 — Poster + Abstract Workshop</span>"
    ]
  },
  {
    "objectID": "lec25.html#instructor-notes-facilitation-tips",
    "href": "lec25.html#instructor-notes-facilitation-tips",
    "title": "25  Session 25 — Poster + Abstract Workshop",
    "section": "25.7 Instructor notes / facilitation tips",
    "text": "25.7 Instructor notes / facilitation tips\n\nEncourage big, legible figures. Shrink text elsewhere, not the plots.\nIf a figure doesn’t clearly answer a question, cut it.\nRemind to state uncertainty/limits explicitly (no trading claims).\nFor the abstract, ban boilerplate like “significant improvement” without numbers.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Session 25 — Poster + Abstract Workshop</span>"
    ]
  },
  {
    "objectID": "lec25.html#grading-passrevise",
    "href": "lec25.html#grading-passrevise",
    "title": "25  Session 25 — Poster + Abstract Workshop",
    "section": "25.8 Grading (pass/revise)",
    "text": "25.8 Grading (pass/revise)\n\nPoster draft builds from a fresh clone using make poster (figures present and render succeeds).\nThree key figures are present and readable, each with a concise takeaway.\nAbstract is 200–300 words and consistent with the figures.\nRepo contains scripts/make_poster_figs.py and scripts/check_abstract_length.py; optional test passes.\n\n\n\n25.8.1 Appendix — Figure caption starter lines (you can paste into the poster)\n\nModel comparison: “TS‑GPT reduces validation MAE vs lin_lags on Split 1; improvements are modest and vary by ticker.”\nRegime breakdown: “All models degrade in high volatility; report metrics by regime to avoid overstating performance.”\nAblation: “Context 64 and heads 4 are a reasonable sweet spot under tight budgets; gains diminish beyond this.”\n\nThis package gives you everything you need to run a productive poster + abstract workshop in 75 minutes and leave students with a tangible draft ready for polishing.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Session 25 — Poster + Abstract Workshop</span>"
    ]
  },
  {
    "objectID": "lec26.html",
    "href": "lec26.html",
    "title": "26  Session 26 — In‑class Presentations & Continuation Pla",
    "section": "",
    "text": "26.1 Session 26 — In‑class Presentations & Continuation Plan (75 min)\nBelow is a complete lecture package for Session 26 — In‑class Presentations & Continuation Plan (75 minutes). It includes a timed agenda, slide talking points, a Colab‑friendly in‑class lab with copy‑paste code (presentation checklists, peer‑feedback forms, automated backlog builder, issue templates, and a hand‑off zip), plus homework with code to tag v1.0, publish a continuation backlog, and prepare for Spring symposium work.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Session 26 — In‑class Presentations & Continuation Pla</span>"
    ]
  },
  {
    "objectID": "lec26.html#session-26-inclass-presentations-continuation-plan-75-min",
    "href": "lec26.html#session-26-inclass-presentations-continuation-plan-75-min",
    "title": "26  Session 26 — In‑class Presentations & Continuation Pla",
    "section": "",
    "text": "26.1.1 Learning goals\nBy the end of class, students can:\n\nDeliver a focused 10‑minute talk that communicates problem, methods, results, and limits.\nProvide / receive actionable peer feedback.\nConvert results into a prioritized backlog (issues + milestones) to continue toward the symposium.\nProduce a handoff bundle (poster, abstract, artifacts, manifest, backlog) and finalize v1.0.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Session 26 — In‑class Presentations & Continuation Pla</span>"
    ]
  },
  {
    "objectID": "lec26.html#agenda-75-min",
    "href": "lec26.html#agenda-75-min",
    "title": "26  Session 26 — In‑class Presentations & Continuation Pla",
    "section": "26.2 Agenda (75 min)",
    "text": "26.2 Agenda (75 min)\n\n(5 min) Setup & expectations (timing, feedback rules, rubric).\n(30–40 min) Talks (≈10 min per team + 5 min Q&A — adapt to class size).\n(20 min) In‑class lab: feedback capture → automated backlog + issues → handoff zip.\n(10 min) Wrap‑up: commit plan, roles, milestones; homework brief.\n\nIf you have a single team, do a 15–20 min talk and deeper Q&A; then use all remaining time for backlog + handoff.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Session 26 — In‑class Presentations & Continuation Pla</span>"
    ]
  },
  {
    "objectID": "lec26.html#slides-talking-points-drop-into-your-deck",
    "href": "lec26.html#slides-talking-points-drop-into-your-deck",
    "title": "26  Session 26 — In‑class Presentations & Continuation Pla",
    "section": "26.3 Slides — talking points (drop into your deck)",
    "text": "26.3 Slides — talking points (drop into your deck)\n\n26.3.1 Presentation structure (10 minutes)\n\nTitle & claim (30s): One sentence problem + one sentence result.\nSetup (1–2 min): Data (tickers/dates), leakage controls (rolling origin + embargo), metrics (MAE, sMAPE).\nMethods (2–3 min): Baselines, GRU/TS‑GPT, any ablations.\nResults (3–4 min): Show 3 figures (model comparison, regime breakdown, ablation). Give one takeaway each.\nLimits & ethics (1 min): Non‑stationarity, survivorship bias; not trading advice.\nNext steps (30–60s): 3 prioritized items and why.\n\n\n\n26.3.2 Q&A best practices\n\nRepeat the question; answer with evidence; admit unknowns; propose next test.\nAvoid “we might” without a concrete follow‑up; anchor to a metric, data slice, or ablation.\n\n\n\n26.3.3 Reproducibility handoff (what the next team needs)\n\nExact commands (fresh‑clone steps), versions (lockfile), manifests, data checksums, and known deviations.\nA backlog labeled by Data / Features / Modeling / Evaluation / Ops / Docs.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Session 26 — In‑class Presentations & Continuation Pla</span>"
    ]
  },
  {
    "objectID": "lec26.html#inclass-lab-20-min-colabfriendly",
    "href": "lec26.html#inclass-lab-20-min-colabfriendly",
    "title": "26  Session 26 — In‑class Presentations & Continuation Pla",
    "section": "26.4 In‑class lab (20 min, Colab‑friendly)",
    "text": "26.4 In‑class lab (20 min, Colab‑friendly)\n\nRun each block as its own cell. Update REPO_NAME if needed.\n\n\n26.4.1 0) Mount Drive & cd into your repo\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME = \"unified-stocks-teamX\"  # &lt;- change\nBASE_DIR  = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR  = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, sys, platform, time\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"reports\",\"docs/figs\",\"scripts\",\"tests\",\".github/ISSUE_TEMPLATE\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd(), \"| Python:\", sys.version.split()[0], \"| OS:\", platform.system())\n\n\n26.4.2 1) Presentation checklist & peer‑feedback forms\nCreate a presenter checklist and a peer feedback form you can fill post‑talk.\n%%bash\ncat &gt; docs/present_checklist.md &lt;&lt; 'MD'\n# Presentation Checklist (10 minutes total)\n- [ ] Title & one‑sentence claim (≤ 30s)\n- [ ] Data & leakage controls (1–2 min): static universe, rolling origin, embargo\n- [ ] Methods (2–3 min): baselines; GRU/TS‑GPT; ablations\n- [ ] Results (3–4 min): 3 figures with one takeaway each\n- [ ] Limits & ethics (1 min)\n- [ ] Next steps (3 bullets; priority + rationale)\nMD\n\ncat &gt; docs/peer_feedback_template.md &lt;&lt; 'MD'\n# Peer Feedback (fill one per talk)\n**One insight:**  \n**One question:**  \n**One suggestion:**  \n**Clarity (1–5):**  \n**Rigor (1–5):**  \n**Reproducibility (1–5):**  \n**Top risk:**  \nMD\necho \"Wrote docs/present_checklist.md and docs/peer_feedback_template.md\"\n\n\n26.4.3 2) Automated continuation backlog builder\nThis script looks at your saved outputs (model comparison, regime breakdown, ablation summaries, predictions) and generates: • NEXT_STEPS.md (prioritized checklists), • reports/issue_backlog.csv (ready to paste into GitHub as issues), • docs/figs/opportunity_by_ticker.png (where to focus), • reports/roadmap_milestones.md (3‑milestone mini‑roadmap).\n%%bash\ncat &gt; scripts/make_continuation_backlog.py &lt;&lt; 'PY'\n#!/usr/bin/env python\nfrom __future__ import annotations\nimport os, math, json\nfrom pathlib import Path\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n\nFIGDIR = Path(\"docs/figs\"); FIGDIR.mkdir(parents=True, exist_ok=True)\nREPDIR = Path(\"reports\"); REPDIR.mkdir(parents=True, exist_ok=True)\n\ndef _safe_read_csv(p): \n    p=Path(p); \n    return pd.read_csv(p) if p.exists() else None\ndef _safe_read_parquet(p):\n    p=Path(p)\n    if p.exists():\n        try: return pd.read_parquet(p)\n        except Exception: return None\n    return None\ndef _mae(a,b): \n    a=np.asarray(a); b=np.asarray(b); \n    return float(np.mean(np.abs(a-b)))\n\ndef load_artifacts():\n    comp = _safe_read_csv(\"reports/model_compare.csv\")\n    regime = _safe_read_csv(\"reports/regime_breakdown.csv\")\n    ab = _safe_read_csv(\"reports/ablation_clean.csv\")\n    lin = _safe_read_csv(\"reports/cli_linlags.csv\")      # per-row predictions if available\n    tsgpt_pred = _safe_read_csv(\"reports/tsgpt_preds_split1.csv\")\n    feats = None\n    for p in [\"data/processed/features_v1.parquet\",\"data/processed/returns.parquet\"]:\n        feats = _safe_read_parquet(p) or feats\n    return comp, regime, ab, lin, tsgpt_pred, feats\n\ndef opportunity_by_ticker(lin, tsgpt_pred):\n    \"\"\"\n    Compute per-ticker 'opportunity' = MAE_lin - MAE_model.\n    If TS-GPT preds unavailable, returns lin MAE only.\n    \"\"\"\n    if lin is None or not {\"ticker\",\"y_true\",\"yhat\"}.issubset(lin.columns):\n        return pd.DataFrame(columns=[\"ticker\",\"mae_lin\",\"mae_model\",\"opportunity\"])\n    lin_mae = lin.groupby(\"ticker\").apply(lambda g: _mae(g[\"y_true\"], g[\"yhat\"])).rename(\"mae_lin\").reset_index()\n    if tsgpt_pred is not None and {\"ticker\",\"y_true\",\"yhat\"}.issubset(tsgpt_pred.columns):\n        mod_mae = tsgpt_pred.groupby(\"ticker\").apply(lambda g: _mae(g[\"y_true\"], g[\"yhat\"])).rename(\"mae_model\").reset_index()\n        out = lin_mae.merge(mod_mae, on=\"ticker\", how=\"left\")\n        out[\"opportunity\"] = out[\"mae_lin\"] - out[\"mae_model\"]\n    else:\n        out = lin_mae\n        out[\"mae_model\"] = np.nan\n        out[\"opportunity\"] = np.nan\n    return out.sort_values(\"opportunity\", ascending=False)\n\ndef infer_actions(comp, regime, ab, opp):\n    actions = {\"Data\":[], \"Features\":[], \"Modeling\":[], \"Evaluation\":[], \"Ops\":[], \"Docs\":[]}\n    # Baseline vs model\n    if comp is not None and \"val_mae\" in comp.columns:\n        comp_sorted = comp.sort_values(\"val_mae\")\n        best = comp_sorted.iloc[0][\"model\"]\n        actions[\"Modeling\"].append(f\"Promote `{best}` as current champion; lock its config in config/config.yaml\")\n    # Regime pain points\n    if regime is not None and \"regime\" in regime.columns:\n        worst = regime.sort_values(\"mae\", ascending=False).iloc[0][\"regime\"]\n        actions[\"Features\"].append(f\"Add regime-aware features or losses (worst regime: **{worst}**); try quantile loss\")\n        actions[\"Evaluation\"].append(\"Report metrics **by regime** in the Quarto report (not only aggregate)\")\n    # Ablation hints\n    if ab is not None and {\"ctx\",\"n_head\",\"val_mae\"}.issubset(ab.columns):\n        best_row = ab.sort_values(\"val_mae\").iloc[0]\n        actions[\"Modeling\"].append(f\"Use context={int(best_row['ctx'])}, heads={int(best_row['n_head'])} as starting point; test +/- 50% context\")\n    else:\n        actions[\"Modeling\"].append(\"Run minimal ablation (context {32,64,96}, heads {2,4}) with fixed seed\")\n    # Opportunity by ticker\n    if len(opp):\n        tops = opp.head(5)[\"ticker\"].tolist()\n        actions[\"Data\"].append(f\"Deep-dive top underperformers or highest-opportunity tickers: {', '.join(map(str,tops))}\")\n        actions[\"Modeling\"].append(\"Consider per-ticker adapters or fine-tuning on worst tickers\")\n    # Ops & Docs\n    actions[\"Ops\"] += [\n        \"Schedule CI task to rebuild features weekly (dry run only)\",\n        \"Cache data checksums and fail CI if changed unexpectedly\"\n    ]\n    actions[\"Docs\"] += [\n        \"Expand Model Card with training data dates and static universe rationale\",\n        \"Add 'How to reproduce' section with exact CLI commands\"\n    ]\n    return actions\n\ndef write_next_steps(actions):\n    md = [\"# NEXT_STEPS (Symposium Continuation Backlog)\\n\"]\n    md.append(\"_Prioritize P0 (this week), P1 (this month), P2 (nice-to-have)._ \\n\")\n    for cat in [\"Data\",\"Features\",\"Modeling\",\"Evaluation\",\"Ops\",\"Docs\"]:\n        md.append(f\"\\n## {cat}\\n\")\n        for a in actions.get(cat, []):\n            md.append(f\"- [ ] {a}  \\n  - Priority: P1  \\n  - Owner: TBA  \\n  - Evidence: link to figure/result\")\n    Path(\"NEXT_STEPS.md\").write_text(\"\\n\".join(md))\n    return \"NEXT_STEPS.md\"\n\ndef write_issue_csv(actions):\n    rows=[]\n    for cat, items in actions.items():\n        for a in items:\n            rows.append({\n                \"title\": f\"[{cat}] {a[:80]}\",\n                \"body\": a + \"\\n\\nSee NEXT_STEPS.md\",\n                \"labels\": f\"{cat.lower()},backlog\",\n                \"milestone\": \"Symposium\"\n            })\n    df = pd.DataFrame(rows)\n    out = REPDIR/\"issue_backlog.csv\"\n    df.to_csv(out, index=False)\n    return out\n\ndef plot_opportunity(opp):\n    if not len(opp): return None\n    opp2 = opp.copy()\n    opp2[\"label\"] = opp2[\"ticker\"].astype(str)\n    plt.figure(figsize=(7.2,3.4))\n    x = np.arange(len(opp2))\n    vals = opp2[\"opportunity\"].values\n    plt.bar(x, vals)\n    plt.xticks(x, opp2[\"label\"].tolist(), rotation=45, ha=\"right\")\n    plt.axhline(0, lw=1)\n    plt.ylabel(\"MAE_baseline - MAE_model\")\n    plt.title(\"Opportunity by ticker (positive = model beats baseline)\")\n    plt.tight_layout()\n    fn = FIGDIR/\"opportunity_by_ticker.png\"\n    plt.savefig(fn, dpi=160); plt.close()\n    return fn\n\ndef write_roadmap():\n    text = \"\"\"# Mini Roadmap (to Spring Symposium)\n\n## Milestone M1 (2–3 weeks): Stabilize champion model & metrics\n- [ ] Lock config; re‑run split1 with seeds; freeze artifacts\n- [ ] Regime‑wise reporting in Quarto\n- [ ] Draft updated poster figures\n\n## Milestone M2 (2–3 weeks): Targeted improvements\n- [ ] Focus on top 3 tickers/regimes with worst performance\n- [ ] Small ablation: context ±50%, heads {2,4}, dropout {0.0,0.1}\n- [ ] Optional: add one macro series (rates or volatility)\n\n## Milestone M3 (1–2 weeks): Packaging & rehearsal\n- [ ] Update NEXT_STEPS and close P0 tasks\n- [ ] Rehearse 10‑min talk; finalize poster\n\"\"\"\n    p = REPDIR/\"roadmap_milestones.md\"\n    p.write_text(text); return p\n\ndef main():\n    comp, regime, ab, lin, tsgpt_pred, feats = load_artifacts()\n    opp = opportunity_by_ticker(lin, tsgpt_pred)\n    fn = plot_opportunity(opp)\n    actions = infer_actions(comp, regime, ab, opp)\n    ns = write_next_steps(actions)\n    csv = write_issue_csv(actions)\n    rm = write_roadmap()\n    print(\"Wrote\", ns, csv, rm, \"and figure:\", fn)\n\nif __name__ == \"__main__\":\n    main()\nPY\nchmod +x scripts/make_continuation_backlog.py\npython scripts/make_continuation_backlog.py\nls -1 NEXT_STEPS.md reports/issue_backlog.csv reports/roadmap_milestones.md || true\n\n\n26.4.4 3) Create GitHub issue templates and PR template\nThese help structure work after today.\n%%bash\ncat &gt; .github/ISSUE_TEMPLATE/feature_request.md &lt;&lt; 'MD'\n---\nname: \"Feature request / Research task\"\nabout: Propose a new feature or experiment\nlabels: \"features,backlog\"\n---\n**What & why**\n- \n\n**Definition of done**\n- \n\n**Evidence / figure link**\n- \n\n**Risks / leakage concerns**\n- \nMD\n\ncat &gt; .github/ISSUE_TEMPLATE/bug_report.md &lt;&lt; 'MD'\n---\nname: Bug report\nabout: Report a reproducibility or CI failure\nlabels: \"bug,ops\"\n---\n**What happened**\n- \n\n**Repro steps**\n- \n\n**Expected vs actual**\n- \n\n**Logs / manifest snippet**\n- \nMD\n\ncat &gt; .github/PULL_REQUEST_TEMPLATE.md &lt;&lt; 'MD'\n## What does this PR change?\n-\n\n## How to reproduce the results (exact commands)\n-\n\n## Checklist\n- [ ] CI green\n- [ ] Updated report/figures if metrics changed\n- [ ] No leakage introduced; tests still pass\n- [ ] Linked issues closed\nMD\necho \"Wrote issue and PR templates.\"\n\n\n26.4.5 4) Handoff bundle (zip)\nPackage poster, abstract, manifest, backlog, figures, and key reports into a single zip to share.\n%%bash\ncat &gt; scripts/make_handoff_zip.py &lt;&lt; 'PY'\n#!/usr/bin/env python\nfrom pathlib import Path\nimport zipfile, time\n\nINCLUDE = [\n  \"README.md\",\n  \"CHANGELOG.md\",\n  \"NEXT_STEPS.md\",\n  \"reports/poster.html\",\n  \"reports/abstract.md\",\n  \"reports/manifest.json\",\n  \"reports/model_compare.csv\",\n  \"reports/regime_breakdown.csv\",\n  \"reports/ablation_clean.csv\",\n  \"reports/tsgpt_split1_metrics.csv\",\n  \"reports/cli_linlags.csv\",\n  \"reports/roadmap_milestones.md\",\n  \"docs/figs/model_compare.png\",\n  \"docs/figs/regime_breakdown.png\",\n  \"docs/figs/ablation_dropout_0_0.png\",\n  \"docs/figs/ablation_dropout_0_1.png\",\n  \"docs/figs/opportunity_by_ticker.png\"\n]\n\ndef main():\n  Path(\"dist\").mkdir(exist_ok=True)\n  stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n  out = Path(\"dist\")/f\"handoff_session26_{stamp}.zip\"\n  with zipfile.ZipFile(out, \"w\", zipfile.ZIP_DEFLATED) as z:\n    for p in INCLUDE:\n      fp = Path(p)\n      if fp.exists():\n        z.write(fp, arcname=str(fp))\n  print(\"Wrote\", out)\n\nif __name__ == \"__main__\":\n  main()\nPY\nchmod +x scripts/make_handoff_zip.py\npython scripts/make_handoff_zip.py\nls -l dist | head -n 3\n\n\n26.4.6 5) Makefile additions (quality of life)\n# Append to Makefile\n.PHONY: backlog handoff\nbacklog: ## Generate NEXT_STEPS and issue_backlog.csv\n\\tpython scripts/make_continuation_backlog.py\n\nhandoff: ## Build a zip with poster, abstract, manifest, backlog, figures\n\\tpython scripts/make_handoff_zip.py\n\n\n26.4.7 6) Tiny test: ensure backlog artifacts exist\n%%bash\ncat &gt; tests/test_backlog_exists.py &lt;&lt; 'PY'\nfrom pathlib import Path\ndef test_backlog_artifacts():\n    assert Path(\"NEXT_STEPS.md\").exists(), \"Run: make backlog\"\n    assert Path(\"reports/issue_backlog.csv\").exists(), \"Missing reports/issue_backlog.csv\"\nPY\npytest -q -k backlog_exists || true",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Session 26 — In‑class Presentations & Continuation Pla</span>"
    ]
  },
  {
    "objectID": "lec26.html#wrapup-10-min",
    "href": "lec26.html#wrapup-10-min",
    "title": "26  Session 26 — In‑class Presentations & Continuation Pla",
    "section": "26.5 Wrap‑up (10 min)",
    "text": "26.5 Wrap‑up (10 min)\n\nShow NEXT_STEPS.md highlights (3 P0 tasks). Assign owners and tentative dates.\nConfirm your handoff zip exists in dist/.\nAgree on communication cadence (weekly check‑in, single source of truth is the repo).",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Session 26 — In‑class Presentations & Continuation Pla</span>"
    ]
  },
  {
    "objectID": "lec26.html#homework-final",
    "href": "lec26.html#homework-final",
    "title": "26  Session 26 — In‑class Presentations & Continuation Pla",
    "section": "26.6 Homework (final)",
    "text": "26.6 Homework (final)\nDeliverables (push before tagging):\n\nTag v1.0 (final poster & abstract committed).\nContinuation backlog present: NEXT_STEPS.md + reports/issue_backlog.csv.\nHandoff zip built in dist/ (commit the manifest of its contents; the zip itself can be attached to a GitHub Release).\nOpen 3–5 GitHub issues from the CSV (copy/paste), assign labels & owners, and add them to a Project board (optional via GitHub UI).\n\n\n26.6.1 A. Commands to finalize and tag v1.0\n%%bash\n# Ensure poster & abstract render and backlog exists\nmake poster || echo \"Render poster locally if Quarto missing.\"\nmake backlog\ngit add NEXT_STEPS.md reports/issue_backlog.csv\ngit commit -m \"docs: NEXT_STEPS and backlog for continuation\"\ngit tag -a v1.0 -m \"Final course release (poster, abstract, backlog)\"\ngit push origin v1.0\n\n\n26.6.2 B. (Optional) Create issues automatically if GitHub CLI (gh) is available\n%%bash\n# Requires: gh auth login (once)\npython - &lt;&lt;'PY'\nimport csv, os, subprocess\npath = \"reports/issue_backlog.csv\"\nif not os.path.exists(path):\n    raise SystemExit(\"Missing reports/issue_backlog.csv\")\nwith open(path) as f:\n    rows = list(csv.DictReader(f))\nfor r in rows[:5]:  # open first 5 to avoid spam\n    title = r[\"title\"]; body = r[\"body\"]; labels = r[\"labels\"]\n    cmd = [\"gh\",\"issue\",\"create\",\"--title\",title,\"--body\",body,\"--label\",labels]\n    print(\" \".join(cmd))\n    try:\n        subprocess.check_call(cmd)\n    except Exception as e:\n        print(\"Skipping (gh not configured):\", e)\nPY\n\n\n26.6.3 C. (Optional) Project board starter (manual UI or gh project)\n\nColumns: Backlog, In progress, Review, Done.\nAdd automation: close issue → moves to Done.\n\n\n\n26.6.4 D. Handoff verification script\nRun once to confirm the bundle:\n%%bash\nmake handoff\npython - &lt;&lt;'PY'\nfrom zipfile import ZipFile; import glob\nz = sorted(glob.glob(\"dist/handoff_session26_*.zip\"))[-1]\nwith ZipFile(z) as f:\n    print(\"ZIP entries:\", len(f.infolist()))\nPY",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Session 26 — In‑class Presentations & Continuation Pla</span>"
    ]
  },
  {
    "objectID": "lec26.html#instructor-facilitation-tips",
    "href": "lec26.html#instructor-facilitation-tips",
    "title": "26  Session 26 — In‑class Presentations & Continuation Pla",
    "section": "26.7 Instructor facilitation tips",
    "text": "26.7 Instructor facilitation tips\n\nTimebox talks strictly; use a soft chime at 9 minutes.\nFor feedback, enforce “one insight, one question, one suggestion”.\nPush teams to prioritize (P0 vs P1 vs P2); avoid “we will do everything.”\nIf results are weak, frame a credible plan: fix leakage risks, add robust baseline, improve regime analysis.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Session 26 — In‑class Presentations & Continuation Pla</span>"
    ]
  },
  {
    "objectID": "lec26.html#grading-pass-revise",
    "href": "lec26.html#grading-pass-revise",
    "title": "26  Session 26 — In‑class Presentations & Continuation Pla",
    "section": "26.8 Grading (pass / revise)",
    "text": "26.8 Grading (pass / revise)\n\nTalk delivered within 10 minutes; Q&A handled professionally.\nBacklog artifacts exist and are sensible (NEXT_STEPS.md, issue_backlog.csv, roadmap).\nHandoff zip produced and contains poster, abstract, figures, and manifest.\nRepo tags v1.0; README includes a short “Next Steps” section (can link to NEXT_STEPS.md).\n\n\n\n26.8.1 Appendix — 1‑slide talk template (you can paste into your deck)\n\nTitle & claim\nData & controls (static tickers; rolling origin + embargo)\nMethod (lin_lags vs GRU vs TS‑GPT; loss)\nResults (3 figs; bold takeaways)\nLimits & ethics\nNext steps (3 bullets)\n\nThis completes Session 26 and hands off a clean, reproducible project with a realistic continuation plan for the Spring symposium.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Session 26 — In‑class Presentations & Continuation Pla</span>"
    ]
  },
  {
    "objectID": "homework/README.html",
    "href": "homework/README.html",
    "title": "STAT 4160: Data Science Productivity Tools",
    "section": "",
    "text": "��--- title: Instructions to submit homework ---   1.  Inside the folder that is linked to your GitHub repo github.comYourRepoName/STAT4160 on your device (local computer or Colab), create a folder named homework if it doesn’t already exist. You should have this folder already if cloned my repository.  2.  Save your homework files in the homework folder. Name it as homework1_&lt;your_init&gt;.ipynb or homework1.qmd, or homework1.png depending on the type of assignment. If you need to submit multiple files for one assignment, first create a subfolder (e.g. homework_1_yw) by mkdir homework/homework_1_yw. Then save your multiple files in this subfolder.  3.  Checkout a new branch for your homework submission. You can name it something like   bash git checkout -b homework1/&lt;your_init&gt;   4.  Add your homework files to the new branch:       bash     git add homework/homework1_&lt;your_init&gt;.ipynb  # this is the path to the homework file or folder       5.  Commit your changes:       bash     git commit -m \"Submit homework1 from &lt;your_init&gt;\"       6.  Push your changes to the remote repository:       bash     git push -u origin homework1/&lt;your_init&gt;       7.  Create a pull request (PR) on GitHub to merge your homework branch into the main branch.  8.  In the pull request description, include any specific instructions or notes for the reviewer.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>README.html</span>"
    ]
  },
  {
    "objectID": "lec_5_pre.html",
    "href": "lec_5_pre.html",
    "title": "28  Lecture 5 (Pre) — Linux Basics",
    "section": "",
    "text": "29 Linux Basics — 75‑Minute Lecture (Colab Edition, using vim)\nAudience: Absolute beginners to Linux\nFormat: using ! (line) or %%bash (cell) magics. Editing demo uses vim",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lecture 5 (Pre) — Linux Basics</span>"
    ]
  },
  {
    "objectID": "lec_5_pre.html#learning-outcomes",
    "href": "lec_5_pre.html#learning-outcomes",
    "title": "28  Lecture 5 (Pre) — Linux Basics",
    "section": "29.1 Learning Outcomes",
    "text": "29.1 Learning Outcomes\nBy the end, students will be able to:\n\nExplain what Linux, the shell, and the filesystem are\nNavigate directories and inspect files\nCreate, copy, move, and delete files/directories safely\nUse wildcards, pipes, and redirection to combine commands\nUnderstand permissions and make a script executable\nEdit a text file in vim (open, insert, save, quit)\nRun shell scripts in Colab and install packages (apt/pip)",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lecture 5 (Pre) — Linux Basics</span>"
    ]
  },
  {
    "objectID": "lec_5_pre.html#colab-setup-23-min",
    "href": "lec_5_pre.html#colab-setup-23-min",
    "title": "28  Lecture 5 (Pre) — Linux Basics",
    "section": "29.2 Colab Setup (2–3 min)",
    "text": "29.2 Colab Setup (2–3 min)\nColab uses a Linux VM with a working directory at /content. To run shell commands in Colab:\n\nOne‑liner: prefix with !\nMulti‑line: start a cell with %%bash\n\n# Verify we’re on Linux\n!uname -a\n!lsb_release -a || cat /etc/os-release\n# Where are we?\n!pwd\n!ls -lah\n\nPersistence: Colab VMs reset. To keep files between sessions, mount Google Drive:\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n!ls -lah /content/drive/MyDrive",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lecture 5 (Pre) — Linux Basics</span>"
    ]
  },
  {
    "objectID": "lec_5_pre.html#agenda-timeboxed",
    "href": "lec_5_pre.html#agenda-timeboxed",
    "title": "28  Lecture 5 (Pre) — Linux Basics",
    "section": "29.3 Agenda (timeboxed)",
    "text": "29.3 Agenda (timeboxed)\n\n(0–10) What is Linux? Shell vs kernel. Filesystem anatomy.\n(10–25) Navigation + basic file ops\n(25–35) Permissions & ownership (Colab nuances)\n(35–50) Redirection, pipes, text tools (grep, sort, wc, cut)\n(50–60) vim crash course (editing in a real terminal)\n(60–70) Shell scripts, variables, PATH\n(70–75) Packages (apt/pip) + help/man + wrap‑up",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lecture 5 (Pre) — Linux Basics</span>"
    ]
  },
  {
    "objectID": "lec_5_pre.html#what-is-linux-010-min",
    "href": "lec_5_pre.html#what-is-linux-010-min",
    "title": "28  Lecture 5 (Pre) — Linux Basics",
    "section": "29.4 1) What is Linux? (0–10 min)",
    "text": "29.4 1) What is Linux? (0–10 min)\n\nKernel: talks to hardware; Userland: tools (bash, ls, vim, etc.)\nShell: the program that reads your commands (default in Colab: bash).\nFilesystem: a tree with root / (not “C:”), home ~, current . and parent ...\nCommon paths: /bin, /usr/bin (programs), /home/… (homes), /etc (config), /var (logs), /tmp (temp). In Colab, workspace is /content and you often store persistent data under /content/drive/MyDrive/....\n\nMini‑lab:\n!echo $SHELL\n!whoami\n!pwd\n!ls -lah /",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lecture 5 (Pre) — Linux Basics</span>"
    ]
  },
  {
    "objectID": "lec_5_pre.html#navigation-basic-file-operations-1025-min",
    "href": "lec_5_pre.html#navigation-basic-file-operations-1025-min",
    "title": "28  Lecture 5 (Pre) — Linux Basics",
    "section": "29.5 2) Navigation & Basic File Operations (10–25 min)",
    "text": "29.5 2) Navigation & Basic File Operations (10–25 min)\nCommands\n\npwd (print working directory)\nls -lah (list with details; -a all, -l long, -h human sizes)\ncd DIR (change dir). cd .. (up one), cd ~ (home)\nmkdir -p PATH (make dir; -p to create parents)\ntouch file (create empty file or update timestamp)\ncp SRC DST (copy), mv SRC DST (move/rename)\nrm file (remove), rm -r dir (recursive), rm -i (confirm)\nView files: cat, less, head, tail -n 20\n\nColab demo (use a sandbox folder):\n!mkdir -p /content/play && cd /content/play && pwd\n!touch a.txt b.txt && ls -l\n!echo \"hello\" &gt; a.txt\n!cp a.txt c.txt && mv b.txt docs.txt && ls -l\n!head -n 1 a.txt && tail -n +1 c.txt\n!rm -i c.txt  # you can skip -i in automated runs\n!ls -l\nGlobbing (wildcards)\n\n* any string; ? single char; [abc] any of set; {foo,bar} brace expansion.\n\n!ls -1 /bin/ba* | head\nSearching\n\nfind . -name \"*.txt\"\ngrep -R \"pattern\" .",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lecture 5 (Pre) — Linux Basics</span>"
    ]
  },
  {
    "objectID": "lec_5_pre.html#permissions-ownership-2535-min",
    "href": "lec_5_pre.html#permissions-ownership-2535-min",
    "title": "28  Lecture 5 (Pre) — Linux Basics",
    "section": "29.6 3) Permissions & Ownership (25–35 min)",
    "text": "29.6 3) Permissions & Ownership (25–35 min)\nls -l output: -rwxr-xr--  1 owner group size date name\n\nFirst char - file / d directory\nTriplets rwx for user/group/others (read/write/execute)\nChange mode: chmod u+x script.sh (add execute for user)\n\n\nColab note: you usually run as root, so ownership quirks differ from multi‑user systems. Still practice chmod and safe deletions.\n\nMini‑lab:\n%%bash\ncd /content/play\ncat &gt; hello.sh &lt;&lt;'EOF'\n#!/usr/bin/env bash\necho \"Hello from a script\"\nEOF\nls -l hello.sh\nchmod u+x hello.sh\n./hello.sh",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lecture 5 (Pre) — Linux Basics</span>"
    ]
  },
  {
    "objectID": "lec_5_pre.html#redirection-pipes-essential-text-tools-3550-min",
    "href": "lec_5_pre.html#redirection-pipes-essential-text-tools-3550-min",
    "title": "28  Lecture 5 (Pre) — Linux Basics",
    "section": "29.7 4) Redirection, Pipes, & Essential Text Tools (35–50 min)",
    "text": "29.7 4) Redirection, Pipes, & Essential Text Tools (35–50 min)\nRedirection\n\n&gt; overwrite, &gt;&gt; append, 2&gt; stderr, &&gt; stdout+stderr\n| pipe: feed output of left command into right command\n\nTools\n\ngrep PATTERN (search); -n show line numbers, -i case‑insensitive\nwc -l (line count), sort, uniq -c, cut -d, -f1, tr, tee\n\nMini‑lab:\n%%bash\ncd /content/play\n# Make a small CSV\ncat &gt; stocks.csv &lt;&lt;'EOF'\nticker,price\nAAPL,180\nMSFT,420\nAAPL,181\nNVDA,120\nAAPL,182\nEOF\n# Count rows, list unique tickers with counts\nwc -l stocks.csv\ncut -d, -f1 stocks.csv | tail -n +2 | sort | uniq -c | sort -nr\n# Filter rows with AAPL and take last 2\ngrep '^AAPL,' stocks.csv | tail -n 2",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lecture 5 (Pre) — Linux Basics</span>"
    ]
  },
  {
    "objectID": "lec_5_pre.html#vim-crash-course-5060-min",
    "href": "lec_5_pre.html#vim-crash-course-5060-min",
    "title": "28  Lecture 5 (Pre) — Linux Basics",
    "section": "29.8 5) vim Crash Course (50–60 min)",
    "text": "29.8 5) vim Crash Course (50–60 min)\nGoal: basic edits: open → insert → save → quit.\nInstall (if needed):\n!apt-get update -qq && apt-get install -y -qq vim\n\nColab caution: vim is fully interactive; works best in a Terminal session (Colab’s Connect to a terminal feature or a local terminal/WSL). If you can’t open a real terminal, use the heredoc technique shown earlier to create/edit files.\n\nvim basics\n\nLaunch: vim hello.txt\nModes: Normal (default), Insert (i), Command (:)\nInsert text: press i, type; ESC to leave insert\nSave & quit: :wq • Quit without saving: :q! • Save: :w\nNavigation: arrows or h j k l; jump to line :42\nSearch: /pattern then n/N\n\nExercise (in a real terminal):\n\nvim notes.txt\nPress i, type a few lines\nESC, then :wq\ncat notes.txt",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lecture 5 (Pre) — Linux Basics</span>"
    ]
  },
  {
    "objectID": "lec_5_pre.html#shell-scripts-variables-path-6070-min",
    "href": "lec_5_pre.html#shell-scripts-variables-path-6070-min",
    "title": "28  Lecture 5 (Pre) — Linux Basics",
    "section": "29.9 6) Shell Scripts, Variables, PATH (60–70 min)",
    "text": "29.9 6) Shell Scripts, Variables, PATH (60–70 min)\nVariables\nNAME=Alice\necho \"Hi $NAME\"\nScript + shebang\n#!/usr/bin/env bash\nset -euo pipefail\nNAME=${1:-World}\necho \"Hello, $NAME!\"\nColab demo:\n%%bash\ncd /content/play\ncat &gt; greet.sh &lt;&lt;'EOF'\n#!/usr/bin/env bash\nset -euo pipefail\nNAME=${1:-World}\necho \"Hello, $NAME!\"\nEOF\nchmod u+x greet.sh\n./greet.sh\n./greet.sh Linux\nPATH\n\nShows where the shell looks for executables.\n\n!echo $PATH\n!which python\n\nRun from current dir with ./script.sh (current dir isn’t automatically in PATH).",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lecture 5 (Pre) — Linux Basics</span>"
    ]
  },
  {
    "objectID": "lec_5_pre.html#packages-help-7075-min",
    "href": "lec_5_pre.html#packages-help-7075-min",
    "title": "28  Lecture 5 (Pre) — Linux Basics",
    "section": "29.10 7) Packages & Help (70–75 min)",
    "text": "29.10 7) Packages & Help (70–75 min)\nSystem packages (apt)\n!apt-get update -qq\n!apt-get install -y -qq tree  # example\n!tree -L 2 /content/play\nPython packages (pip)\n%pip install -q pandas\npython -c \"import pandas as pd; print(pd.__version__)\"\nGetting help\n\ncmd --help (portable) • man cmd (manual; may be minimal in Colab)\napropos to search manuals; tldr if installed for concise examples",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lecture 5 (Pre) — Linux Basics</span>"
    ]
  },
  {
    "objectID": "lec_5_pre.html#minilabs-for-practice-homework",
    "href": "lec_5_pre.html#minilabs-for-practice-homework",
    "title": "28  Lecture 5 (Pre) — Linux Basics",
    "section": "29.11 Mini‑Labs (for practice / homework)",
    "text": "29.11 Mini‑Labs (for practice / homework)\n\nFind & count: In /usr/bin, count how many programs start with g.\n!ls -1 /usr/bin/g* 2&gt;/dev/null | wc -l\nCSV pipeline: From stocks.csv, output the max price per ticker.\n%%bash\ncd /content/play\nawk -F, 'NR&gt;1 {max[$1] = ($2&gt;max[$1] ? $2 : max[$1])} END {for (k in max) print k \",\" max[k]}' stocks.csv | sort\nPermissions: Make a script that fails without execute bit; then add it and rerun.\nvim: Create todo.txt with three lines and save.",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lecture 5 (Pre) — Linux Basics</span>"
    ]
  },
  {
    "objectID": "lec_5_pre.html#onepage-cheat-sheet",
    "href": "lec_5_pre.html#onepage-cheat-sheet",
    "title": "28  Lecture 5 (Pre) — Linux Basics",
    "section": "29.12 One‑Page Cheat Sheet",
    "text": "29.12 One‑Page Cheat Sheet\nNavigation: pwd, ls -lah, cd, mkdir -p, tree (optional)\nFiles: touch, cp, mv, rm [-r], cat, less, head, tail, find, grep -R\nGlobs: *, ?, [abc], {a,b}\nPipes/Redirection: |, &gt;, &gt;&gt;, 2&gt;, &&gt;; tee\nText tools: grep, sort, uniq -c, wc -l, cut, tr, awk\nPermissions: ls -l, chmod u+x, umask\nProcesses: ps aux, top -bn1, kill PID\nEditing (vim): i insert, ESC normal, :w save, :q quit, :wq save+quit, :q! force quit\nScripts: #!/usr/bin/env bash, chmod +x, ./script.sh\nPackages: apt-get update, apt-get install -y PKG; Python: %pip install PKG\nHelp: cmd --help, man cmd, which cmd, type cmd",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lecture 5 (Pre) — Linux Basics</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "29  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hyndman, Rob J., and George Athanasopoulos. 2021. Forecasting:\nPrinciples and Practice. 3rd ed. https://otexts.com/fpp3/.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "lec8.html#slides",
    "href": "lec8.html#slides",
    "title": "8  Session 8 — SQL II: Window Functions & pandas.read_sql Workflows",
    "section": "8.3 Slides",
    "text": "8.3 Slides\nWhat’s a window?\n\nA window lets an aggregate or analytic function see a row + its neighbors without collapsing rows.\nTemplate:\nfunc(expr) OVER (\n   PARTITION BY key\n   ORDER BY time\n   ROWS BETWEEN N PRECEDING AND CURRENT ROW\n)\nPARTITION BY = per‑group window; ORDER BY = sequence; ROWS = how many rows to include.\n\nWindow vs GROUP BY\n\nGROUP BY returns one row per group; OVER (…) returns one row per input row with extra columns.\n\nTime‑series patterns\n\nLags: LAG(x, k) → previous k rows ⇒ features at t that use only info ≤ t.\nLeads: LEAD(x, k) → future k rows ⇒ labels (don’t leak into features).\nRolling stats: AVG(x) OVER (… ROWS BETWEEN w-1 PRECEDING AND CURRENT ROW); rolling variance via AVG(x*x) - AVG(x)^2.\nTop‑k per group: compute ROW_NUMBER() OVER (PARTITION BY key ORDER BY score DESC) and filter WHERE rn&lt;=k.\n\nROWS vs RANGE\n\nUse ROWS for fixed‑length windows on ordered rows (what we need).\nTime‑based windows (e.g., last 30 calendar days) require different techniques in SQLite (correlated subquery); we’ll note but not use today.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Session 8 — SQL II: Window Functions & `pandas.read_sql` Workflows</span>"
    ]
  },
  {
    "objectID": "lec9.html#key-points",
    "href": "lec9.html#key-points",
    "title": "9  Session 9 — Cleaning, Joins, and Parquet",
    "section": "9.7 Key Points",
    "text": "9.7 Key Points\n\nNullable ints: astype(\"Int64\") keeps NAs; plain int64 will fail if NAs exist.\nCategoricals & partitions: When reading partitioned Parquet, ticker may come back as object. Re‑cast to category after read if needed.\nCompression choice: zstd gives good ratio/speed; snappy is more ubiquitous.\nPrecision: float32 is fine; for production finance, consider float64 and explicit rounding.\n\n\n\n9.7.1 Optional (for your Makefile later)\nAdd quick targets: (if directly editing Makefile in Colab: removing python, and indent all lines below the target by four spaces).\n.PHONY: prices-parquet returns-parquet\nprices-parquet:  ## Clean raw prices and save processed Parquet(s)\n\\tpython - &lt;&lt;'PY'\nimport pandas as pd, glob, pathlib, numpy as np, re, json\nfrom pathlib import Path\n# (Paste the functions from the lab: standardize_columns, clean_prices, join_meta)\n# Then read raw -&gt; clean -&gt; write parquet as in the lab\nPY\n\nreturns-parquet: ## Build returns.parquet with r_1d + calendar features\n\\tpython - &lt;&lt;'PY'\nimport pandas as pd, numpy as np\np=\"data/processed/prices.parquet\"; r=pd.read_parquet(p).sort_values([\"ticker\",\"date\"])\nr[\"log_return\"]=r.groupby(\"ticker\")[\"adj_close\"].apply(lambda s: np.log(s/s.shift(1))).reset_index(level=0, drop=True)\nr[\"r_1d\"]=r.groupby(\"ticker\")[\"log_return\"].shift(-1)\nr[\"weekday\"]=r[\"date\"].dt.weekday.astype(\"int8\"); r[\"month\"]=r[\"date\"].dt.month.astype(\"int8\")\nr[[\"date\",\"ticker\",\"log_return\",\"r_1d\",\"weekday\",\"month\"]].to_parquet(\"data/processed/returns.parquet\", compression=\"zstd\", index=False)\nprint(\"Wrote data/processed/returns.parquet\")\nPY\nYou now have a clean, tidy Parquet foundation the later sessions (evaluation & modeling) can rely on.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Session 9 — Cleaning, Joins, and Parquet</span>"
    ]
  },
  {
    "objectID": "lec9.html#slides",
    "href": "lec9.html#slides",
    "title": "9  Session 9 — Cleaning, Joins, and Parquet",
    "section": "9.3 Slides",
    "text": "9.3 Slides\n\n9.3.1 Tidy schema for price data\n\nOne row = one ticker‑day.\nMinimal columns (snake_case): date (datetime64[ns]), ticker (category), open/high/low/close/adj_close (float32/64), volume (Int64).\nOptional metadata (from a separate table): name (string), sector (category).\n\n\n\n9.3.2 Idiomatic pandas: merge, assign, pipe\n\nmerge: combine frames by keys (e.g., prices ⟵left join⟶ tickers).\nassign: add/transform columns without breaking the chain: df = df.assign(adj_close=lambda d: d['adj_close'].fillna(d['close'])).\npipe: compose small, testable transforms: df = (raw.pipe(standardize_columns).pipe(clean_prices).pipe(join_meta, meta=meta)).\n\n\n\n9.3.3 Dtypes that help\n\nCategorical (category): compact & fast for low‑cardinality strings (ticker, sector).\nNullable integers (Int64, Int32): keep missing values and integer semantics (volume).\nString (string[python]): consistent string semantics (avoid object).\nFloats: float32 can halve memory, but consider numeric precision.\n\n\n\n9.3.4 Parquet: why & how\n\nColumnar, compressed, preserves schema better than CSV.\nFilters & projection: read only needed columns/rows (esp. with partitioned datasets).\nPartitioning by ticker/ yields fast reads of a subset (e.g., a single ticker).\nTypical settings: engine=pyarrow, compression=zstd or snappy.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Session 9 — Cleaning, Joins, and Parquet</span>"
    ]
  },
  {
    "objectID": "lec10.html#slide",
    "href": "lec10.html#slide",
    "title": "10  Rolling Windows, Resampling, and Leakage‑Safe Features",
    "section": "11.2 Slide",
    "text": "11.2 Slide\nFeature timing = everything\n\nPredict \\(r_{t+1}\\) using info up to and including time t.\nRule: compute any rolling stat at \\(t\\) from data \\(\\le t\\), then shift by 1 if that stat includes the current target variable.\n\nCore pandas patterns\n\nLags: s.shift(k) (past), never negative shifts.\nRolling: s.rolling(W, min_periods=W).agg(...) and then no extra shift if the rolling window ends at \\(t\\).\nExpanding: long‑memory features (e.g., expanding mean).\nEWM: s.ewm(span=W, adjust=False).mean() for decayed memory.\n\nResampling safely\n\nUse groupby('ticker').resample('W-FRI', on='date') then aggregate:\n\nOHLC: first/open, max/high, min/low, last/adj_close\nVolume: sum\nReturns: compound via np.log(prod(1+r)) or sum of log returns.\n\n\nDtypes\n\nticker = category; calendar ints int8; features float32 (fine for class).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Rolling Windows, Resampling, and Leakage‑Safe Features</span>"
    ]
  },
  {
    "objectID": "lec11.html#slide",
    "href": "lec11.html#slide",
    "title": "11  Session 11 — APIs with requests: Secrets, Retries, and Caching",
    "section": "11.3 Slide",
    "text": "11.3 Slide\nRequests pattern\n\nSession + HTTPAdapter + Retry → robust.\nValidate: status code, content type; guard against partial data.\n\nSecrets\n\n.env.template committed; .env untracked.\nLoad with dotenv.load_dotenv(). Access via os.getenv(\"FRED_API_KEY\").\n\nCaching\n\nFile cache: key by URL+params hash.\nDB cache: cache (key TEXT PRIMARY KEY, value BLOB, fetched_at).\n\nAlignment\n\nAfter download, normalize to date and join on date.\nStore to SQLite table with a composite key (series_id, date).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Session 11 — APIs with `requests`: Secrets, Retries, and Caching</span>"
    ]
  },
  {
    "objectID": "lec11.html#learning-goals",
    "href": "lec11.html#learning-goals",
    "title": "11  Session 11 — APIs with requests: Secrets, Retries, and Caching",
    "section": "",
    "text": "Call a REST API with requests + robust retry/backoff.\nManage secrets with .env and never commit keys.\nCache responses (file or SQLite) and align external series by date.\nSave enriched data to SQLite and Parquet.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Session 11 — APIs with `requests`: Secrets, Retries, and Caching</span>"
    ]
  },
  {
    "objectID": "lec10.html#key-points",
    "href": "lec10.html#key-points",
    "title": "10  Rolling Windows, Resampling, and Leakage‑Safe Features",
    "section": "11.5 Key points",
    "text": "11.5 Key points\n\nKeep a one‑page “no leakage” checklist handy and point to it often.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Rolling Windows, Resampling, and Leakage‑Safe Features</span>"
    ]
  },
  {
    "objectID": "lec11.html#key-points",
    "href": "lec11.html#key-points",
    "title": "11  Session 11 — APIs with requests: Secrets, Retries, and Caching",
    "section": "11.7 Key points",
    "text": "11.7 Key points\n\nHavehave a prepared .env with a working FRED key to avoid classroom delays.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Session 11 — APIs with `requests`: Secrets, Retries, and Caching</span>"
    ]
  },
  {
    "objectID": "lec12.html#slide",
    "href": "lec12.html#slide",
    "title": "12  Session 12 — HTML Scraping: Ethics & Resilience",
    "section": "12.2 Slide",
    "text": "12.2 Slide\nEthics + resilience\n\nCheck robots.txt; identify disallow rules.\nSet a clear User‑Agent and sleep between requests.\nCache HTML locally; don’t hammer sites.\nExpect structure to change; write defensive code.\n\nParsing patterns\n\nPrefer table selectors; use read_html for well‑formed tables.\nClean headers → snake_case; drop footnotes; trim whitespace.\nNormalize keys (e.g., ticker symbols: map . ↔︎ - if needed).\n\nProvenance\n\nSave source_url, fetched_at, and a checksum alongside the CSV.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Session 12 — HTML Scraping: Ethics & Resilience</span>"
    ]
  },
  {
    "objectID": "lec12.html#key-points",
    "href": "lec12.html#key-points",
    "title": "12  Session 12 — HTML Scraping: Ethics & Resilience",
    "section": "12.6 Key points",
    "text": "12.6 Key points\n\nIf Wikipedia blocks requests, switch to pandas.read_html (shown) or use a small pre‑saved HTML in data/static/ to demonstrate parsing.\n\nThe sessions 10-12 carry you from solid feature engineering → external data integration → web scraping with ethics, setting up a strong foundation for the testing/CI weeks that follow.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Session 12 — HTML Scraping: Ethics & Resilience</span>"
    ]
  },
  {
    "objectID": "lec13.html#slides",
    "href": "lec13.html#slides",
    "title": "13  Session 13: pytest + Data Validation",
    "section": "13.3 Slides",
    "text": "13.3 Slides\n\n13.3.1 What to test (fast, crisp)\n\nContract tests for data:\n\nSchema: required columns exist; dtypes sane (ticker categorical, calendar ints).\nNulls: no NAs in training‑critical cols.\nSemantics: r_1d is lead of log_return; rolling features computed from past only.\nKeys: no duplicate (ticker, date); dates strictly increasing within ticker.\n\nKeep tests under ~5s total (CI budget). Avoid long recomputations; sample/take head.\n\n\n\n13.3.2 Pandera vs custom checks\n\nPandera: declarative schema; optional dependency; good for column existence + ranges.\nCustom: essential for domain logic (look‑ahead bans, exact rolling formulas).\n\n\n\n13.3.3 Logging basics\n\nUse logging.getLogger(__name__); set level via env (LOGLEVEL=INFO).\nLog counts, ranges, and any data drops inside build scripts.\nIn tests: use caplog to assert a warning is emitted for suspicious conditions.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13: pytest + Data Validation</span>"
    ]
  },
  {
    "objectID": "lec13.html#key-points",
    "href": "lec13.html#key-points",
    "title": "13  Session 13: pytest + Data Validation",
    "section": "13.6 Key points:",
    "text": "13.6 Key points:\n\nFast tests only for CI; keep heavy, long recomputations out.\nNo look‑ahead and unique (ticker,date) are non‑negotiable contracts.\nLogging is a first‑class tool—tests can assert on warnings you emit.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13: pytest + Data Validation</span>"
    ]
  },
  {
    "objectID": "lec13.html#session-13-pytest-data-validation",
    "href": "lec13.html#session-13-pytest-data-validation",
    "title": "13  Session 13: pytest + Data Validation",
    "section": "",
    "text": "13.1.1 Learning goals\nBy the end of class, students can:\n\nWrite fast, high‑signal tests for data pipelines (shapes, dtypes, nulls, no look‑ahead).\nValidate a DataFrame with Pandera (schema + value checks) or custom checks only.\nUse logging effectively and capture logs in tests.\nRun tests in Colab / locally and prepare for CI in Session 14.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13: pytest + Data Validation</span>"
    ]
  },
  {
    "objectID": "lec13.html#agenda",
    "href": "lec13.html#agenda",
    "title": "13  Session 13: pytest + Data Validation",
    "section": "13.2 Agenda",
    "text": "13.2 Agenda\n\nWhat to test (and not), “data tests” vs unit tests, speed budget\nPandera schemas & custom checks; tolerance and stability\nLogging basics (logging, levels, handlers); testing logs with caplog\nIn‑class lab: add tests/test_features.py (+ optional Pandera test), fixtures, config; run & fix\nWrap‑up + homework briefing\n\n\n\n13.2.1 What to test (fast, crisp)\n\nContract tests for data:\n\nSchema: required columns exist; dtypes sane (ticker categorical, calendar ints).\nNulls: no NAs in training‑critical cols.\nSemantics: r_1d is lead of log_return; rolling features computed from past only.\nKeys: no duplicate (ticker, date); dates strictly increasing within ticker.\n\nKeep tests under ~5s total (CI budget). Avoid long recomputations; sample/take head.\n\n\n\n13.2.2 Pandera vs custom checks\n\nPandera: declarative schema; optional dependency; good for column existence + ranges.\nCustom: essential for domain logic (look‑ahead bans, exact rolling formulas).\n\n\n\n13.2.3 Logging basics\n\nUse logging.getLogger(__name__); set level via env (LOGLEVEL=INFO).\nLog counts, ranges, and any data drops inside build scripts.\nIn tests: use caplog to assert a warning is emitted for suspicious conditions.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13: pytest + Data Validation</span>"
    ]
  },
  {
    "objectID": "lec14.html#session-14-precommit-github-actions-ci",
    "href": "lec14.html#session-14-precommit-github-actions-ci",
    "title": "14  pre‑commit & GitHub Actions CI",
    "section": "",
    "text": "14.1.1 Learning goals\nStudents will be able to:\n\nConfigure pre‑commit to run Black, Ruff (lint + import sort), and nbstripout on every commit.\nKeep commits clean and notebook outputs stripped.\nAdd a fast GitHub Actions CI workflow that runs pre‑commit hooks and pytest on each PR (Pull Request).\nKeep CI runtime under ~3–4 minutes with caching and a lean dependency set.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>pre‑commit & GitHub Actions CI</span>"
    ]
  },
  {
    "objectID": "lec14.html#agenda",
    "href": "lec14.html#agenda",
    "title": "14  pre‑commit & GitHub Actions CI",
    "section": "14.2 Agenda",
    "text": "14.2 Agenda\n\nwhy pre‑commit; the “quality gate”; anatomy of a fast CI\nBlack vs Ruff; when nbstripout matters; what belongs in CI\nIn‑class lab: configure pre‑commit (Black, Ruff, nbstripout) → run locally → add CI workflow → local dry‑run\nWrap‑up + homework briefing\nBuffer",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>pre‑commit & GitHub Actions CI</span>"
    ]
  },
  {
    "objectID": "lec14.html#inclass-lab-colabfriendly",
    "href": "lec14.html#inclass-lab-colabfriendly",
    "title": "14  pre‑commit & GitHub Actions CI",
    "section": "14.4 In‑class lab (Colab‑friendly)",
    "text": "14.4 In‑class lab (Colab‑friendly)\n\nRun each block as its own Colab cell. Update REPO_NAME to your repo. The cells create and modify files inside your repo.\n\n\n14.4.1 0) Mount Drive & go to repo\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"  # &lt;- change if needed\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\".github/workflows\",\"tests\",\"scripts\",\"reports\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\n\n14.4.2 1) Install tools locally (for this Colab runtime)\n!pip -q install pre-commit black ruff nbstripout pytest\n\n\n14.4.3 2) Add tool config to pyproject.toml (Black + Ruff)\n\nIf you don’t have a pyproject.toml, this cell will create a minimal one; otherwise it appends/updates sections.\n\nfrom pathlib import Path\nimport textwrap, re\n\npyproj = Path(\"pyproject.toml\")\nexisting = pyproj.read_text() if pyproj.exists() else \"\"\n\ndef upsert(section_header, body):\n    global existing\n    pattern = rf\"(?ms)^\\[{re.escape(section_header)}\\]\\s*.*?(?=^\\[|\\Z)\"\n    if re.search(pattern, existing):\n        existing = re.sub(pattern, f\"[{section_header}]\\n{body}\\n\", existing)\n    else:\n        existing += f\"\\n[{section_header}]\\n{body}\\n\"\n\n# Black\nupsert(\"tool.black\", textwrap.dedent(\"\"\"\nline-length = 88\ntarget-version = [\"py311\"]\n\"\"\").strip())\n\n# Ruff (modern layout)\nupsert(\"tool.ruff\", textwrap.dedent(\"\"\"\nline-length = 88\ntarget-version = \"py311\"\n\"\"\").strip())\n\nupsert(\"tool.ruff.lint\", textwrap.dedent(\"\"\"\nselect = [\"E\",\"F\",\"I\"]  # flake8 errors, pyflakes, import sort\nignore = [\"E501\"]       # let Black handle line length\n\"\"\").strip())\n\nupsert(\"tool.ruff.lint.isort\", textwrap.dedent(\"\"\"\nknown-first-party = [\"projectname\"]\n\"\"\").strip())\n\npyproj.write_text(existing.strip()+\"\\n\")\nprint(pyproj.read_text())\n\n\n14.4.4 3) Create .pre-commit-config.yaml with hooks (Black, Ruff, nbstripout)\n\nVersions below are stable at time of writing—feel free to bump later.\n\nfrom pathlib import Path\ncfg = Path(\".pre-commit-config.yaml\")\ncfg.write_text(\"\"\"repos:\n  - repo: https://github.com/psf/black\n    rev: 24.4.2\n    hooks:\n      - id: black\n        language_version: python3.12  #match the python version of your system or leave it blank\n\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.5.0\n    hooks:\n      - id: ruff\n        args: [--fix, --exit-non-zero-on-fix]\n      - id: ruff-format\n\n  - repo: https://github.com/kynan/nbstripout\n    rev: 0.7.1\n    hooks:\n      - id: nbstripout\n        files: \\\\.ipynb$\n\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.6.0\n    hooks:\n      - id: end-of-file-fixer\n      - id: trailing-whitespace\n      - id: check-yaml\n      - id: check-added-large-files\n\"\"\")\nprint(cfg.read_text())\n\n\n14.4.5 4) Install the local git hook & run on all files\n!pre-commit install\n!pre-commit run --all-files\n\nThe first run will download hook toolchains (Black, Ruff, etc.), format files, and strip notebook outputs. Commit changes after verifying.\n\n\n\n14.4.6 5) (Optional) Also install git filter for nbstripout\n\nThis is an extra layer; pre‑commit hook above already strips outputs. Use this to guarantee outputs are removed even when bypassing pre‑commit.\n\n!nbstripout --install --attributes .gitattributes\nprint(open(\".gitattributes\").read())\n\n\n14.4.7 6) Add a tiny “bad style” file to see hooks in action\nfrom pathlib import Path\np = Path(\"scripts/bad_style.py\")\np.write_text(\"import os,sys\\n\\n\\ndef add(a,b):\\n  return(a +  b)\\n\")\nprint(\"Wrote:\", p)\n\n# Run hooks just on this file\n!pre-commit run --files scripts/bad_style.py\nprint(open(\"scripts/bad_style.py\").read())\n\nYou should see Black and Ruff fix spacing/imports; trailing whitespace hooks may also fire.\n\n\n\n14.4.8 7) Add a fast GitHub Actions CI workflow (.github/workflows/ci.yml)\n\nThis runs pre‑commit and your tests on Ubuntu with Python 3.11, with pip caching.\n\nfrom pathlib import Path\nwf_dir = Path(\".github/workflows\")\nwf_dir.mkdir(parents=True, exist_ok=True)\n\nwf = wf_dir / \"ci.yml\"\nwf.write_text(\"\"\"name: CI\non:\n  push:\n    branches: [ main, master, develop ]\n  pull_request:\n    branches: [ main, master, develop ]\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\njobs:\n  build:\n    runs-on: ubuntu-latest\n    timeout-minutes: 10\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n          cache: 'pip'\n          cache-dependency-path: |\n            requirements.txt\n            pyproject.toml\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n          pip install pre-commit pytest\n\n      # Run pre-commit (Black, Ruff, nbstripout, etc.)\n      - name: pre-commit\n        uses: pre-commit/action@v3.0.1\n\n      # Run tests (fast only)\n      - name: pytest\n        run: pytest -q --maxfail=1\n\"\"\")\nprint(wf.read_text())\n\n\n14.4.9 8) Add a Makefile convenience (optional but nice)\nfrom pathlib import Path\nmk = Path(\"Makefile\")\ntext = mk.read_text() if mk.exists() else \"\"\nif \"lint\" not in text:\n    text += \"\"\"\n\n.PHONY: lint test ci-local\nlint: ## Run pre-commit hooks on all files\n\\tpre-commit run --all-files\n\ntest: ## Run fast tests\n\\tpytest -q --maxfail=1\n\nci-local: lint test ## Simulate CI locally\n\"\"\"\n    mk.write_text(text)\nprint(mk.read_text())",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>pre‑commit & GitHub Actions CI</span>"
    ]
  },
  {
    "objectID": "lec14.html#wrapup",
    "href": "lec14.html#wrapup",
    "title": "14  pre‑commit & GitHub Actions CI",
    "section": "14.5 Wrap‑up",
    "text": "14.5 Wrap‑up\n\nYou configured pre‑commit with Black, Ruff (lint + import sort), and nbstripout to keep the repo clean.\nYou added a fast CI that runs the same hooks plus pytest on every PR.\nCI time stays small due to caching and a lean dependency set; tests are fast by design (Session 13).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>pre‑commit & GitHub Actions CI</span>"
    ]
  },
  {
    "objectID": "lec14.html#key-points",
    "href": "lec14.html#key-points",
    "title": "14  pre‑commit & GitHub Actions CI",
    "section": "14.8 Key points:",
    "text": "14.8 Key points:\n\nIf pre‑commit says “no files to check” for nbstripout, ensure your file matcher files: \\.ipynb$ is correct and that notebooks are tracked.\nIf Ruff conflicts with Black on formatting: keep Black as the authority, disable E501 in Ruff, and let Ruff handle imports (I) and errors (E, F).\nCI failures from missing deps: ensure your requirements.txt (or pyproject.toml with [project.dependencies]) includes pandas, pyarrow, and pytest if your tests read Parquet.\nKeep CI lean: no data downloads or training; use fixtures and tiny synthetic datasets (Session 13 pattern).\n\nYou now have an automated quality gate—style, lint, and tests run locally and in CI—so your future PRs start green and stay green.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>pre‑commit & GitHub Actions CI</span>"
    ]
  },
  {
    "objectID": "lec10.html#agenda",
    "href": "lec10.html#agenda",
    "title": "10  Rolling Windows, Resampling, and Leakage‑Safe Features",
    "section": "11.1 Agenda",
    "text": "11.1 Agenda\n\nleakage‑free features; lags vs rolling; resampling patterns\ncommon pitfalls (min_periods, alignment, mixed frequencies)\nIn‑class lab: load returns → build features → (optional) weekly aggregates → write features_v1.parquet\nWrap‑up + homework brief\nBuffer\n\n\nFeature timing = everything\n\nPredict \\(r_{t+1}\\) using info up to and including time t.\nRule: compute any rolling stat at \\(t\\) from data \\(\\le t\\), then shift by 1 if that stat includes the current target variable.\n\nCore pandas patterns\n\nLags: s.shift(k) (past), never negative shifts.\nRolling: s.rolling(W, min_periods=W).agg(...) and then no extra shift if the rolling window ends at \\(t\\).\nExpanding: long‑memory features (e.g., expanding mean).\nEWM: s.ewm(span=W, adjust=False).mean() for decayed memory.\n\nResampling safely\n\nUse groupby('ticker').resample('W-FRI', on='date') then aggregate:\n\nOHLC: first/open, max/high, min/low, last/adj_close\nVolume: sum\nReturns: compound via np.log(prod(1+r)) or sum of log returns.\n\n\nDtypes\n\nticker = category; calendar ints int8; features float32 (fine for class).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Rolling Windows, Resampling, and Leakage‑Safe Features</span>"
    ]
  },
  {
    "objectID": "lec15.html#session-15-framing-metrics",
    "href": "lec15.html#session-15-framing-metrics",
    "title": "15  Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)",
    "section": "",
    "text": "15.1.1 Learning goals\nBy the end of class, students can:\n\nSpecify forecast horizon \\(H\\), step (stride), and choose between expanding vs sliding rolling‑origin evaluation with an embargo gap.\nImplement a date‑based splitter that yields (train_idx, val_idx) for all tickers at once.\nCompute MAE, sMAPE, MASE (with a proper training‑window scale), and aggregate per‑ticker and across tickers (macro vs micro/weighted).\nProduce a tidy CSV of baseline results to serve as your course’s ground truth.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)</span>"
    ]
  },
  {
    "objectID": "lec15.html#agenda",
    "href": "lec15.html#agenda",
    "title": "15  Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)",
    "section": "15.2 Agenda",
    "text": "15.2 Agenda\n\nforecasting setup — horizon \\(H\\), step, rolling‑origin (expanding vs sliding), embargo\nmetrics — MAE, sMAPE, MASE; aggregation across tickers (macro vs micro/weighted)\nIn‑class lab: implement a date‑based splitter → compute naive & seasonal‑naive baselines → MAE/sMAPE/MASE per split/ticker → save reports\nWrap‑up & homework brief\nBuffer\n\n\n\n15.2.1 Framing the forecast\n\nTarget: next‑day log return \\(r_{t+1}\\) (you built this as r_1d).\nHorizon \\(H\\): 1 business day.\nStep (stride): how far the origin moves forward each split (e.g., 63 trading days ≈ a quarter).\nRolling‑origin schemes\n\nExpanding: train start fixed; train grows over time.\nSliding (rolling): fixed‑length train window slides forward.\n\nEmbargo: small gap (e.g., 5 days) between train end and validation start to avoid adjacency leakage.\n\n\n\n15.2.2 Metrics (scalar, easy to compare)\n\nMAE: \\(\\frac{1}{n}\\sum |y - \\hat{y}|\\) — robust & interpretable.\nsMAPE: \\(\\frac{2}{n}\\sum \\frac{|y - \\hat{y}|}{(|y| + |\\hat{y}| + \\epsilon)}\\) — scale‑free, safe for near‑zero returns with \\(\\epsilon\\).\nMASE: \\(\\text{MASE}=\\frac{\\text{MAE}_\\text{model}}{\\text{MAE}_\\text{naive (train)}}\\) — &lt;1 means better than naive.\n\nFor seasonality \\(s\\), the naive comparator predicts \\(y_{t+1} \\approx y_{t+1-s}\\) (we’ll use \\(s=5\\) for day‑of‑week seasonality on business days).\nScale is computed on the training window only, per ticker.\n\n\n\n\n15.2.3 Aggregation across tickers\n\nPer‑ticker metrics first → then aggregate.\nMacro average: mean of per‑ticker metrics (each ticker equal weight).\nMicro/weighted: pool all rows (or weight tickers by sample count); for MAE, pooled MAE equals sample‑count weighted average of per‑ticker MAEs.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)</span>"
    ]
  },
  {
    "objectID": "lec15.html#wrapup",
    "href": "lec15.html#wrapup",
    "title": "15  Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)",
    "section": "15.4 Wrap‑up",
    "text": "15.4 Wrap‑up\n\nYou now have a date‑based rolling‑origin splitter with an embargo, and baseline metrics that set a credible reference.\nMASE uses a training‑window naive as scale (per ticker), so you can read “&lt;1 is better than naive” at a glance.\nAggregation: report both macro (per‑ticker average) and micro/weighted (pooled).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)</span>"
    ]
  },
  {
    "objectID": "lec15.html#key-points",
    "href": "lec15.html#key-points",
    "title": "15  Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)",
    "section": "15.6 Key points",
    "text": "15.6 Key points\n\nDefine the problem first (H, step, splits); metrics only make sense after framing.\nMASE &lt; 1 ⇒ better than naïve; report both macro & micro.\nEmbargo helps mitigate adjacency leakage; keep it small but nonzero.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)</span>"
    ]
  }
]