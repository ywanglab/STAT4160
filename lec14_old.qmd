---
title: "Session 14 â€” Capstone: Polish, Rehearsal & Release"
---
Below is a complete lecture package for **Session 14 â€” Capstone: Polish, Rehearsal & Release** (75 minutes). It includes: a timeâ€‘boxed agenda, slide talking points, a **Colabâ€‘friendly inâ€‘class lab with copyâ€‘paste code**, and **homework with copyâ€‘paste code**. By the end of class, the repo will be clean and reproducible, youâ€™ll have a **slide deck** generated from Quarto, a **final checks script**, and a **â€œoneâ€‘clickâ€ Makefile** to rebuild results.

> **Educational use only â€” not trading advice.**
> Assumes the same Driveâ€‘mounted repo (e.g., `unified-stocks-teamX`) with outputs from Sessions 8â€“13 (e.g., `reports/gpt_walkforward_metrics.csv`, `docs/model_card.qmd`).

---

## Session 14 â€” Capstone: Polish, Rehearsal & Release (75 min)

### Learning goals

Students will be able to:

1. Perform **repo hygiene**: clean outputs, largeâ€‘file tracking, manifest, license/citation, tests.
2. Generate a **Quarto slide deck** that pulls metrics/figures directly from `reports/`.
3. Run a **final checks** script and a **oneâ€‘click rebuild** via Makefile.
4. Rehearse a succinct **8â€“10 minute talk** with an evidenceâ€‘first narrative.

---

## Agenda (75 min)

* **(10 min)** Slides: deliverables, â€œwhat good looks likeâ€, release checklist
* **(15 min)** Slides: repo hygiene & reproducibility (LFS, manifests, tests, Makefile â€œallâ€)
* **(30 min)** **Inâ€‘class lab**: finalize Makefile â†’ create slide deck (`docs/talk.qmd`) â†’ add final checks script â†’ render & dryâ€‘run
* **(15 min)** Rehearsal: 8â€‘minute talk flow + Q\&A bank
* **(5 min)** Wrapâ€‘up & homework briefing

---

## Slides / talking points (add to your deck)

**What are we shipping?**

* **Artifacts:** `docs/model_card.html`, `docs/talk.html` (slides), `reports/*.csv`, `models/*.pt/json` (small), `repro/manifest.json`.
* **Repo qualities:** clean README, license, citation, **Makefile `all`**, **tests pass**, **no huge files** in Git history.

**Release checklist**

* âœ… Rebuild from scratch: `make all` completes without manual edits.
* âœ… **No leakage** or â€œlastâ€‘minute notebook hacksâ€.
* âœ… Figures/tables generated from **reports**, not handâ€‘typed in slides.
* âœ… CI or local `pytest` green.
* âœ… Large binaries tracked by **Gitâ€‘LFS** (or excluded and script to rebuild).

**Talk structure (8â€“10 min)**

1. Problem (30s): unified nextâ€‘day returns (education only).
2. Data & features (1 min): SQL windows, lags/rolling, leakage guards.
3. Evaluation (1.5 min): walkâ€‘forward + embargo; micro/macro metrics.
4. Models (2 min): lagâ€‘1 baseline â†’ LSTM â†’ tiny GPT; configs small.
5. Results (2â€“3 min): MAE & Directional Accuracy; sweep; regimes if included.
6. Takeaways & limitations (1 min).
7. Next steps (30s): robustness, calibration, alternative labels.

---

## Inâ€‘class Lab (30 min, Colabâ€‘friendly)

> Run each block as its **own cell**. Adjust `REPO_OWNER/REPO_NAME` first.

### 0) Mount Drive & cd into repo

```python
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

REPO_OWNER = "YOUR_GITHUB_USERNAME_OR_ORG"   # <- change
REPO_NAME  = "unified-stocks-teamX"          # <- change
BASE_DIR   = "/content/drive/MyDrive/dspt25"
REPO_DIR   = f"{BASE_DIR}/{REPO_NAME}"

import os, pathlib, sys
pathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)
assert pathlib.Path(REPO_DIR).exists(), "Repo not found. Clone it first."
os.chdir(REPO_DIR)
print("Working dir:", os.getcwd())
```

### 1) Finalize a **oneâ€‘click Makefile** (adds `all`, `talk`, `final-checks`, `release-zip`)

```bash
%%bash
set -euo pipefail
# Create Makefile if missing
test -f Makefile || echo -e "help:\n\t@echo 'See make help in prior sessions.'" > Makefile

# Append (idempotent): build features â†’ eval GPT â†’ poster figs â†’ model card â†’ talk â†’ final checks â†’ export zip
grep -q "target: all" Makefile || cat >> Makefile <<'MAKEEOF'

# --- Session 14 additions ---
target: all
.PHONY: all talk final-checks release-zip clean-artifacts

all: ## One-click rebuild (features â†’ eval â†’ figs â†’ model card)
\t$(MAKE) features-sql || true
\t$(MAKE) eval-gpt || true
\t$(MAKE) pkg-figs || true
\t@if command -v quarto >/dev/null 2>&1; then make model-card; else echo "Quarto not found; skipping model-card"; fi
\t$(MAKE) talk

talk: ## Render Quarto slides docs/talk.qmd â†’ docs/talk.html
\t@if command -v quarto >/dev/null 2>&1; then quarto render docs/talk.qmd; else echo "Quarto not found; render locally"; fi

final-checks: ## Run scripts/final_checks.py (sizes/LFS/tests/manifest)
\tpython scripts/final_checks.py

release-zip: ## Create a lightweight release zip (no big data, includes docs & reports)
\tpython - <<'PY'
import os, pathlib, zipfile
root = pathlib.Path(".")
out = pathlib.Path("release.zip")
keep = ["README.md","LICENSE","CITATION.cff","pyproject.toml","Makefile",
        "docs/model_card.html","docs/talk.html","docs/figs","reports","repro","models","src","scripts","tests"]
with zipfile.ZipFile(out, "w", zipfile.ZIP_DEFLATED) as z:
    for k in keep:
        p = pathlib.Path(k)
        if not p.exists(): continue
        if p.is_dir():
            for f in p.rglob("*"):
                if f.is_file(): z.write(f, f.as_posix())
        else:
            z.write(p, p.as_posix())
print("Wrote release.zip")
PY

clean-artifacts: ## Remove generated artifacts (safe clean)
\trm -f release.zip
\tfind reports -type f -name '*.csv' -maxdepth 1 -delete || true
\tfind docs/figs -type f -name '*.png' -delete || true

MAKEEOF

echo "Makefile updated."
```

### 2) Create the **Quarto slide deck** (`docs/talk.qmd`) that reads your reports

````python
from pathlib import Path
Path("docs").mkdir(exist_ok=True)
Path("docs/style-talk.css").write_text("""
.reveal .slides { font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; }
.small { font-size: 0.9em; color:#555; }
""")

talk_qmd = r"""---
title: "Unified Tinyâ€‘GPT for Nextâ€‘Day Stock Returns"
subtitle: "Capstone â€” Data Science Productivity Tools"
author: "Your Name"
format:
  revealjs:
    theme: [default]
    slide-number: true
    toc: false
    css: style-talk.css
    incremental: false
execute:
  echo: false
  warning: false
---

## Problem & Scope

Educational project (not trading advice): predict **next-day log return** across multiple tickers with one unified model.

- **Data:** Daily adjusted close/volume.
- **Features:** SQL windows (lags, rolling stats).
- **Splits:** Expanding walkâ€‘forward with **5â€‘day embargo**.

## Pipeline Snapshot

- Build features in SQLite (Session 8).
- Leakageâ€‘safe walkâ€‘forward eval (Session 9).
- Baselines: **Lagâ€‘1**, **LSTM**, **Tinyâ€‘GPT** (Sessions 10â€“12).
- Packaging & docs (Session 13).

## Results â€” Walkâ€‘Forward (mean across splits)

```{python}
import pandas as pd
summary = pd.read_csv("reports/poster_table.csv")
summary
````

## MAE per Split

![](figs/mae_per_split.png)

\::: {.small}
Numbers come from `reports/gpt_walkforward_metrics.csv`, rendered via Makefile.
\:::

## Model & Configs

* Tiny GPT: d\_model=64, heads=4, layers=2, context=32, dropout=0.1
* Early stopping on validation loss; AdamW (lr=1eâ€‘3, wd=1eâ€‘4)
* Trainâ€‘window normalization only.

## Limitations & Ethics

* Volatility regime sensitivity; survivorship bias (fixed tickers).
* Outputs are noisy; **not investment advice**.
* Future work: regime models, calibration, multiâ€‘horizon labels.

## Takeaways

* Clean **evaluation** > flashy models.
* Small attention model can edge lagâ€‘1, but effect sizes are modest.
* Reproducible tooling (SQL, Makefile, Quarto) made iteration fast.

## Thank you

Code & docs in the repo. Q\&A.
"""
Path("docs/talk.qmd").write\_text(talk\_qmd)
print("Wrote docs/talk.qmd")

````

Render (if Quarto is available in your runtime; if not, render locally):
```python
import shutil, subprocess
if shutil.which("quarto"):
    print(subprocess.check_output(["quarto","render","docs/talk.qmd"], text=True))
    print("Rendered docs/talk.html")
else:
    print("Quarto not found. You can still render locally with: quarto render docs/talk.qmd")
````

### 3) Add a **final checks** script

This script guards against common mistakes: missing files, huge binaries, untracked LFS patterns, tests failing, manifest freshness.

```python
from pathlib import Path
import json, subprocess, sys, shutil, os, platform, time

Path("scripts").mkdir(exist_ok=True)
Path("repro").mkdir(exist_ok=True)

final_checks_py = r"""
#!/usr/bin/env python
import os, sys, json, subprocess, platform
from pathlib import Path
from datetime import datetime

ERRORS = []

def warn(msg): print("[WARN]", msg)
def err(msg): 
    print("[FAIL]", msg)
    ERRORS.append(msg)

def have(path): return Path(path).exists()

def check_files():
    must = ["README.md", "LICENSE", "pyproject.toml",
            "reports/gpt_walkforward_metrics.csv",
            "reports/poster_table.csv",
            "docs/talk.qmd"]
    for m in must:
        if not have(m): err(f"Missing required file: {m}")

def check_sizes():
    # flag files > 50MB
    big = []
    for p in Path(".").rglob("*"):
        if p.is_file():
            try:
                sz = p.stat().st_size
                if sz > 50*1024*1024:
                    big.append((p.as_posix(), sz))
            except Exception:
                pass
    if big:
        warn("Large files detected (>50MB):")
        for name, sz in sorted(big, key=lambda x: -x[1])[:10]:
            print(f"  - {name}  ({sz/1e6:.1f} MB)")

def check_lfs():
    # Ensure models and DBs tracked with LFS if present
    gitat = Path(".gitattributes")
    if not gitat.exists():
        warn("No .gitattributes found. If you commit binaries, add LFS patterns.")
        return
    pat = gitat.read_text()
    needed = ["data/*.db", "models/*.pt", "models/*.bin", "models/*.json"]
    for n in needed:
        if n in pat: 
            print("[OK] LFS pattern present:", n)
        else:
            warn(f"LFS pattern missing: {n}")

def run_pytest():
    try:
        out = subprocess.run(["pytest","-q"], capture_output=True, text=True)
        print(out.stdout)
        if out.returncode != 0:
            err("pytest returned non-zero exit code.")
    except FileNotFoundError:
        warn("pytest not installed; skipping tests.")

def write_manifest():
    import pkgutil
    manifest = {
        "generated_utc": datetime.utcnow().isoformat()+"Z",
        "python": platform.python_version(),
        "platform": platform.platform(),
        "packages": {}
    }
    try:
        pip = subprocess.check_output([sys.executable,"-m","pip","freeze"], text=True)
        manifest["pip_freeze"] = pip.strip().splitlines()
    except Exception:
        manifest["pip_freeze"] = []
    Path("repro/manifest.json").write_text(json.dumps(manifest, indent=2))
    print("[OK] Wrote repro/manifest.json")

def main():
    print("== Final checks ==")
    check_files()
    check_sizes()
    check_lfs()
    write_manifest()
    run_pytest()
    if ERRORS:
        print("\nSummary: FAIL")
        for e in ERRORS: print(" -", e)
        sys.exit(1)
    print("\nSummary: PASS")

if __name__ == "__main__":
    main()
"""
Path("scripts/final_checks.py").write_text(final_checks_py)
os.chmod("scripts/final_checks.py", 0o755)
print("Wrote scripts/final_checks.py")
```

Run it:

```bash
%%bash
set -euo pipefail
python scripts/final_checks.py || true
```

> If it flags missing files, finish the earlier sessionsâ€™ steps (especially Session 12â€“13) and reâ€‘run `make all`.

---

## Rehearsal (15 min)

**8â€‘minute talk flow (speaker timing)**

1. **0:00â€“0:30** â€” Title slide & â€œeducation onlyâ€ disclaimer.
2. **0:30â€“1:30** â€” Data & features (1 slide): SQL windows, leakage control.
3. **1:30â€“3:00** â€” Evaluation (1â€“2 slides): walkâ€‘forward, embargo, metrics.
4. **3:00â€“5:00** â€” Models (1â€“2 slides): lagâ€‘1 â†’ LSTM â†’ tiny GPT; configs small.
5. **5:00â€“7:00** â€” Results (2 slides): perâ€‘split MAE bar chart, summary table, 1 sweep figure.
6. **7:00â€“8:00** â€” Limitations & next steps.

**Q\&A bank (practice)**

* *Why not perâ€‘ticker models?* â€” Data fragmentation, variance; unified model uses shared structure.
* *How do you avoid leakage?* â€” Lags/rolling use trainâ€‘side windows; embargo; trainâ€‘window normalization; no lookâ€‘ahead.
* *Why GPT over LSTM?* â€” Comparable capacity; attention handles longer dependencies; measured, not assumed.
* *How stable across regimes?* â€” Provide byâ€‘regime metrics if computed; discuss degradation under volatility.

---

## Wrapâ€‘up (5 min)

* You now have: **`make all`**, **slides**, **model card**, **checks**, and a **release zip**.
* Keep everything **dataâ€‘driven**: slides read from `reports/`.
* For symposium: bring **slides** (HTML or PDF export), **model card** link, and **backup** `release.zip`.

---

## Homework (Capstone Deliverables)

**Goal:** Finalize and tag a â€œv1â€ of your project with slides and checks. Submit a link to the repo and your release zip.

### Part A â€” Complete & Render Slides

* Edit `docs/talk.qmd` to add your names, tweak narrative, and ensure plots/summary read from `reports/`.
* Render:

  ```bash
  quarto render docs/talk.qmd
  ```

  If Quarto isnâ€™t available in Colab, render locally and commit `docs/talk.html`.

### Part B â€” Pass **final checks**

Run:

```bash
make final-checks
```

Fix any failures (missing files, tests, large untracked binaries). Reâ€‘run until **PASS**.

### Part C â€” Oneâ€‘click rebuild

Run:

```bash
make all
```

Verify it completes (features â†’ eval â†’ figs â†’ model card â†’ talk).
Then produce the release package:

```bash
make release-zip
```

Ensure `release.zip` contains **docs**, **reports**, **models** (small), **src**, **scripts**, **Makefile**, **pyproject**, **LICENSE**, **CITATION.cff**.

### Part D â€” (Optional) Notebook scrubbing

Add a small utility to clear outputs from any stray notebooks (keeps repo clean):

```python
# scripts/clear_notebooks.py
#!/usr/bin/env python
import nbformat, sys
from pathlib import Path
for nb in Path(".").rglob("*.ipynb"):
    try:
        obj = nbformat.read(nb, as_version=4)
        for c in obj.cells:
            if "outputs" in c: c["outputs"] = []
            if "execution_count" in c: c["execution_count"] = None
        nbformat.write(obj, nb)
        print("Cleared:", nb)
    except Exception as e:
        print("Skip:", nb, e)
```

Run:

```bash
python scripts/clear_notebooks.py
```

### Part E â€” Submit

* Commit & push all changes.
* Upload `release.zip` to your course LMS or share a drive link.
* Share the repo URL and the **path to `docs/talk.html`**.

**Grading (pass/revise)**

* `make all` succeeds without edits.
* `scripts/final_checks.py` reports **PASS**.
* `docs/talk.html` renders, showing figures/tables sourced from `reports/`.
* `release.zip` present and opens; includes docs/reports/src/scripts.
* README lists how to reproduce; LICENSE & (minimal) CITATION present.

---

## Instructor checklist (before class)

* Run `make all` in a fresh runtime; ensure `docs/talk.html` appears.
* Confirm `scripts/final_checks.py` passes on your exemplar repo.
* Print a oneâ€‘page rubric. Prepare a timer for rehearsals.

## Emphasize while teaching

* Good projects **optimize for clarity & reproducibility** over chasing tiny metric gains.
* Slides are **views** of your reports, not separate sources of truth.
* Your best proof of engineering skill is `make all` succeeding on a clean machine.

**Congratulations on finishing the course ğŸ‘** â€” you now have a compact, reproducible, endâ€‘toâ€‘end DS tooling pipeline and a symposiumâ€‘ready package.
