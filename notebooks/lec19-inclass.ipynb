{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMWOjb6I8jYBxUoTMQz60jQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# from google.colab import drive\n","# drive.flush_and_unmount()           # ignore errors if already unmounted\n","\n","#If cannot remount, simply delete the mounted drive and then remount\n","# rm -rf /content/drive\n"],"metadata":{"id":"H-1K7L9NJHMB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"3b0cacda","outputId":"5d36380e-daac-4a6b-f577-3907b10e676f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763314435286,"user_tz":360,"elapsed":20717,"user":{"displayName":"David W.","userId":"14633192575819064045"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Colab cell\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"01f28b78","executionInfo":{"status":"ok","timestamp":1763314436300,"user_tz":360,"elapsed":1004,"user":{"displayName":"David W.","userId":"14633192575819064045"}}},"outputs":[],"source":["# Adjust these two for YOUR repo\n","REPO_OWNER = \"ywanglab\"\n","REPO_NAME  = \"STAT4160\"   # e.g., unified-stocks-team1\n","BASE_DIR   = \"/content/drive/MyDrive/dspt25\"\n","CLONE_DIR  = f\"{BASE_DIR}/{REPO_NAME}\"\n","REPO_URL   = f\"https://github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n","\n","# if on my office computer\n","\n","# REPO_NAME  = \"lectureNotes\"   # e.g., on my office computer\n","# BASE_DIR = r\"E:\\OneDrive - Auburn University Montgomery\\teaching\\AUM\\STAT 4160 Productivity Tools\" # on my office computer\n","# CLONE_DIR  = f\"{BASE_DIR}\\{REPO_NAME}\"\n","\n","import os, pathlib\n","pathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"36277e0a","outputId":"78152086-1a82-41be-b900-23001dd4a030","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763314436817,"user_tz":360,"elapsed":516,"user":{"displayName":"David W.","userId":"14633192575819064045"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Working dir: /content/drive/MyDrive/dspt25/STAT4160\n"]}],"source":["import os, subprocess, shutil, pathlib\n","\n","if not pathlib.Path(CLONE_DIR).exists():\n","    !git clone {REPO_URL} {CLONE_DIR}\n","else:\n","    # If the folder exists, just ensure it's a git repo and pull latest\n","    os.chdir(CLONE_DIR)\n","    # !git status\n","    # !git pull --rebase # !git pull --ff-only\n","os.chdir(CLONE_DIR)\n","print(\"Working dir:\", os.getcwd())"]},{"cell_type":"markdown","source":["## Session 19 — Tensors, Datasets, Training Loop\n","\n","### Learning goals\n","\n","By the end of class, students can:\n","\n","1.  Create a **windowed sequence dataset** across **multiple tickers** with shape `(B, T, F)` → predict `r_1d` at time `t+1`.\n","2.  Use `Dataset`/`DataLoader` correctly: **pin memory**, worker seeding, and efficient slicing.\n","3.  Write a **minimal training loop** with **early stopping**, **AMP** (mixed precision on GPU), and **checkpoint save/load**.\n","4.  Produce a tidy **validation metrics CSV** to compare later models.\n","\n","------------------------------------------------------------------------\n","\n","## Agenda\n","\n","-   tensors & batching; `Dataset`/`DataLoader`; pinning memory; reproducibility with seeds\n","\n","-   training loop anatomy; early stopping; AMP; checkpoints\n","\n","-   **In‑class lab**: build `WindowedDataset` → DataLoaders → tiny GRU regressor → train w/ early stopping → save best checkpoint; evaluate\n","\n","-   Wrap‑up + homework brief\n","\n","------------------------------------------------------------------------\n","\n","# ⭐Tensors & batching\n","\n","-   Tensors are N‑D arrays on **CPU** or **GPU** (`.to(device)`); keep everything `float32` unless using AMP (Automatic mixed precision: Pytorch tool for faster training using FP16 where safe).\n","\n","AMP is a PyTorch tool that:\n","\n","* makes training **faster**\n","* uses **less GPU memory**\n","* and does this automatically by switching between FP16 and FP32 internally\n","\n","All you do is wrap your forward pass with:\n","\n","```python\n","with torch.autocast(\"cuda\"):\n","    y = model(x)\n","    loss = criterion(y, target)\n","```\n","\n","and use a **GradScaler** for safe gradients.\n","\n","---\n","\n","\n","\n","-   For sequences: **batch** × **time** × **features** ⇒ `(B, T, F)`. Predict a scalar per sequence end (`r_1d` at time `t+1`).\n","\n","### `Dataset`/`DataLoader` patterns\n","\n","-   **Precompute an index** of windows: for each ticker, sliding windows end at `i` with context `T`; target is `r_1d[i]`.\n","\n","-   `DataLoader` tips:\n","\n","    -   `pin_memory=True` and `non_blocking=True` on `.to(device)` speed H2D copies (when using GPU).\n","\n","   \n","\n","# ⭐In GPU computing:\n","\n","* **Host (H)** = your **CPU** and system memory (RAM)\n","* **Device (D)** = your **GPU** and GPU memory (VRAM)\n","\n","So whenever data needs to be used by the GPU, it must be **copied from the host to the device**.\n","\n","\n","**H2D** stands for:\n","\n","**H → D = Host-to-Device**\n","\n","So an **H2D copy** is:\n","\n","> “Copying data from CPU memory into GPU memory.”\n","\n","Example in PyTorch:\n","\n","```python\n","x = x.to(\"cuda\")     # this triggers an H2D copy\n","```\n","\n","This takes time — often **more time than the actual GPU computation** — which is why  minimizing H2D copies.\n","\n","---\n","\n","Every time you do:\n","\n","* `.to(\"cuda\")`\n","* `tensor.cuda()`\n","* `tensor.to(device)`\n","\n","you are performing an **H2D copy**.\n","\n","If you do this inside your training loop for every batch, you slow things down a lot.\n","\n","---\n","\n","### ❓ If you have a DataLoader that outputs CPU tensors, and you call `.to(\"cuda\")` on each batch inside your training loop, is that an H2D copy?\n","\n","<details>\n","  <summary>Click to show answer</summary>\n","\n"," **yes.**\n","\n","</details>\n","\n","\n","\n","And that leads to the core idea from Lecture 19:\n","\n","> **One H2D copy per batch is fine. Many H2D copies per batch will kill your GPU speed.**\n","\n","\n","## ⭐ Mini-question\n","\n","Suppose you accidentally write this inside your training loop:\n","\n","```python\n","x = x.to(\"cuda\")\n","y = y.to(\"cuda\")\n","loss = loss.to(\"cuda\")\n","model = model.to(\"cuda\")\n","```\n","\n","### ❓ How many of these should *actually* be on the GPU each iteration?\n","\n","(Just pick one of these options)\n","\n","**A.** All of them\n","\n","**B.** Only `x` and `y`\n","\n","**C.** Only the model\n","\n","**D.** The model once (outside the loop) + `x` and `y` each batch\n","\n","**E.** Everything should stay on CPU\n","\n","Which one feels right?\n","\n","<details>\n","  <summary>Click to show answer</summary>\n","\n","Correct answer: **D**\n","###  The **model** should be moved to GPU **once**, before the training loop:\n","\n","```python\n","model = model.to(\"cuda\")\n","```\n","\n","(If you do this inside the loop, you pay the H2D copy cost **every batch**, which is extremely slow.)\n","\n","  Each batch’s **x** and **y** need to be copied H→D:\n","\n","```python\n","x = x.to(\"cuda\")\n","y = y.to(\"cuda\")\n","```\n","\n"," The **loss** should *not* be moved; it will already live on the GPU because `y_pred` and `y` are on GPU.\n","\n","---\n","\n"," Why this matters (1-sentence intuition)\n","\n","> **Good GPU training = copy as little as possible.**\n","> You want GPU data to *stay* on the GPU.\n","\n","\n","</details>\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    -   Seed workers for reproducibility; keep `num_workers=2` (Colab stable).\n","\n","### ⭐Seeds & determinism\n","\n","-   Set `python`, `numpy`, `torch` seeds; disable CuDNN benchmarking for reproducibility; prefer small batch sizes that fit CPU/GPU.\n","\n","### ⭐Training loop w/ early stopping\n","\n","-   Loop: `train_step` (model in `train()`), `val_step` (model in `eval()` + `no_grad()`).\n","-   Track **best val loss**; stop after `patience` epochs without improvement.\n","-   Save **checkpoint**: `state_dict`, optimizer state, epoch, metrics. Load with `load_state_dict`.\n","\n","### ⭐AMP & checkpoints\n","\n","-   On CUDA, wrap forward in `torch.cuda.amp.autocast()` and use `GradScaler` to scale loss.\n","-   Save the best checkpoint to `models/…pt`; log a CSV under `reports/`.\n","\n","------------------------------------------------------------------------\n","\n","## In‑class lab (Colab‑friendly)\n","\n","> Run each block as its own **separate cell** in Colab. Replace `REPO_NAME` as needed.\n","\n","### 0) Setup, mount, and check device\n"],"metadata":{"id":"-ZnucKiTUe78"}},{"cell_type":"code","source":["import os, pathlib, sys\n","\n","for p in [\"data/processed\",\"models\",\"reports\",\"scripts\",\"tests\"]:\n","    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\n","print(\"Working dir:\", os.getcwd())\n","\n","import torch, platform\n","print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available(), \"| Python:\", sys.version.split()[0], \"| OS:\", platform.system())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qf54hat2b0Ia","executionInfo":{"status":"ok","timestamp":1763314492919,"user_tz":360,"elapsed":5046,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"d2b7b828-855b-4e67-e7f9-f5f8a33bace6"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Working dir: /content/drive/MyDrive/dspt25/STAT4160\n","Torch: 2.8.0+cu126 | CUDA available: False | Python: 3.12.12 | OS: Linux\n"]}]},{"cell_type":"markdown","source":["⭐\n","```python\n","sys.version.split()[0]\n","```\n","\n","`sys.version`\n","\n","This is a **string** describing your Python version, e.g.:\n","\n","```\n","'3.10.12 (main, Jul  6 2023, 12:00:00) [GCC 11.2.0]'\n","```\n","\n","`.split()`\n","\n","Splitting that string (default = split on spaces) gives a list:\n","\n","```\n","['3.10.12',  \n"," '(main,',  \n"," 'Jul',  \n"," '6',  \n"," '2023,',  \n"," '12:00:00)',  \n"," '[GCC',  \n"," '11.2.0]']\n","```\n","\n"," `[0]`\n","\n","This takes the **first element**:\n","\n","```\n","'3.10.12'\n","```\n","\n","So the entire expression:\n","\n","```python\n","sys.version.split()[0]\n","```\n","\n","returns **just the Python version number**, without any extra metadata.\n","\n","\n","### ❓ If `sys.version` were the string\n","\n","```\n","\"3.9.6 something something\"\n","```\n","\n","what would `sys.version.split()[1]` return?\n","\n","<details>\n","  <summary>Click to show answer</summary>\n","\n","\"something\"\n","\n","</details>\n","\n"],"metadata":{"id":"WLGHoaUOWtRA"}},{"cell_type":"markdown","source":["# ⭐ What `platform.system()` does\n","\n","It returns a **string** describing the **operating system** your Python interpreter is running on.\n","\n","Typical outputs:\n","\n","* `\"Windows\"`\n","* `\"Linux\"`\n","* `\"Darwin\"` (this is macOS)\n","\n","\n","\n","# ⭐ Mini-check (your turn)\n","\n","If you run `platform.system()` inside Google Colab, what do you expect it prints?\n","\n","Just pick one:\n","\n","A. `\"Windows\"`\n","B. `\"Linux\"`\n","C. `\"Darwin\"`\n","<details>\n","  <summary>Click to show answer</summary>\n","\n","B\n","\n","</details>\n"],"metadata":{"id":"Z_n9urumX1TX"}},{"cell_type":"markdown","source":["# 1) Load features and pick columns (with fallbacks)"],"metadata":{"id":"eOKR5fi6YcQZ"}},{"cell_type":"code","source":["import pandas as pd, numpy as np\n","from pathlib import Path\n","\n","# Prefer static universe file if present (from Session 17)\n","f_static = Path(\"data/processed/features_v1_static.parquet\")\n","f_base   = Path(\"data/processed/features_v1.parquet\")\n","\n","if f_static.exists():\n","    df = pd.read_parquet(f_static)\n","elif f_base.exists():\n","    df = pd.read_parquet(f_base)\n","else:\n","    # Minimal fallback from returns\n","    rpath = Path(\"data/processed/returns.parquet\")\n","    if not rpath.exists():\n","        # synthesize small dataset\n","        rng = np.random.default_rng(0)\n","        dates = pd.bdate_range(\"2022-01-03\", periods=320)\n","        frames=[]\n","        for t in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\"]:\n","            eps = rng.normal(0, 0.012, size=len(dates)).astype(\"float32\")\n","            adj = 100*np.exp(np.cumsum(eps))\n","            g = pd.DataFrame({\n","                \"date\": dates, \"ticker\": t,\n","                \"adj_close\": adj.astype(\"float32\"),\n","                \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n","            })\n","            g[\"r_1d\"] = g[\"log_return\"].shift(-1)\n","            frames.append(g)\n","        df = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)\n","        df[\"ticker\"] = df[\"ticker\"].astype(\"category\")\n","    else:\n","        df = pd.read_parquet(rpath)\n","        df = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","        # add minimal lags\n","        for k in [1,2,3]:\n","            df[f\"lag{k}\"] = df.groupby(\"ticker\")[\"log_return\"].shift(k)\n","        df = df.dropna().reset_index(drop=True)\n","\n","# Ensure minimal features exist\n","cand_feats = [\"log_return\",\"lag1\",\"lag2\",\"lag3\",\"zscore_20\",\"roll_std_20\"]\n","FEATS = [c for c in cand_feats if c in df.columns]\n","assert \"r_1d\" in df.columns, \"Label r_1d missing; rebuild returns/features pipeline.\"\n","assert \"log_return\" in df.columns, \"log_return missing.\"\n","\n","# Keep a small subset of tickers for speed (5–10 tickers)\n","subset = df[\"ticker\"].astype(str).unique().tolist()[:8]\n","df = df[df[\"ticker\"].astype(str).isin(subset)].copy()\n","\n","# Harmonize types & sort\n","df[\"date\"] = pd.to_datetime(df[\"date\"])\n","df[\"ticker\"] = df[\"ticker\"].astype(\"category\")\n","df = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","\n","print(\"Using features:\", FEATS, \"| tickers:\", df[\"ticker\"].nunique(), \"| rows:\", len(df))\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"NeoAKWmhZgsa","executionInfo":{"status":"ok","timestamp":1763314495015,"user_tz":360,"elapsed":525,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"10214dc5-d89b-4435-a3fa-062f236f30e2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Using features: ['log_return', 'lag1', 'lag2', 'lag3', 'zscore_20', 'roll_std_20'] | tickers: 8 | rows: 1272\n"]},{"output_type":"execute_result","data":{"text/plain":["        date ticker  log_return      r_1d  weekday  month      lag1      lag2  \\\n","0 2020-01-29   AAPL   -0.018417 -0.002351        2      1 -0.012895 -0.019012   \n","1 2020-01-30   AAPL   -0.002351 -0.012675        3      1 -0.018417 -0.012895   \n","2 2020-01-31   AAPL   -0.012675  0.002713        4      1 -0.002351 -0.018417   \n","3 2020-02-03   AAPL    0.002713  0.001568        0      2 -0.012675 -0.002351   \n","4 2020-02-04   AAPL    0.001568 -0.001869        1      2  0.002713 -0.012675   \n","\n","       lag3  roll_mean_20  roll_std_20  zscore_20  ewm_mean_20  ewm_std_20  \\\n","0 -0.004576     -0.004086     0.008476  -1.690830    -0.005252    0.009304   \n","1 -0.019012     -0.004353     0.008324   0.240455    -0.004976    0.008875   \n","2 -0.012895     -0.004849     0.008517  -0.918756    -0.005709    0.008745   \n","3 -0.018417     -0.004268     0.008622   0.809695    -0.004907    0.008690   \n","4 -0.002351     -0.003963     0.008719   0.634254    -0.004291    0.008486   \n","\n","   exp_mean   exp_std  adj_close   volume  \n","0 -0.004086  0.008476  92.154846  1598707  \n","1 -0.004003  0.008270  91.938454  2992900  \n","2 -0.004397  0.008280  90.780533   634335  \n","3 -0.004088  0.008224  91.027122   913454  \n","4 -0.003852  0.008126  91.169922   662663  "],"text/html":["\n","  <div id=\"df-aa0199e9-30a3-4ac9-a9eb-218b0f0b1954\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>ticker</th>\n","      <th>log_return</th>\n","      <th>r_1d</th>\n","      <th>weekday</th>\n","      <th>month</th>\n","      <th>lag1</th>\n","      <th>lag2</th>\n","      <th>lag3</th>\n","      <th>roll_mean_20</th>\n","      <th>roll_std_20</th>\n","      <th>zscore_20</th>\n","      <th>ewm_mean_20</th>\n","      <th>ewm_std_20</th>\n","      <th>exp_mean</th>\n","      <th>exp_std</th>\n","      <th>adj_close</th>\n","      <th>volume</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2020-01-29</td>\n","      <td>AAPL</td>\n","      <td>-0.018417</td>\n","      <td>-0.002351</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>-0.012895</td>\n","      <td>-0.019012</td>\n","      <td>-0.004576</td>\n","      <td>-0.004086</td>\n","      <td>0.008476</td>\n","      <td>-1.690830</td>\n","      <td>-0.005252</td>\n","      <td>0.009304</td>\n","      <td>-0.004086</td>\n","      <td>0.008476</td>\n","      <td>92.154846</td>\n","      <td>1598707</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2020-01-30</td>\n","      <td>AAPL</td>\n","      <td>-0.002351</td>\n","      <td>-0.012675</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>-0.018417</td>\n","      <td>-0.012895</td>\n","      <td>-0.019012</td>\n","      <td>-0.004353</td>\n","      <td>0.008324</td>\n","      <td>0.240455</td>\n","      <td>-0.004976</td>\n","      <td>0.008875</td>\n","      <td>-0.004003</td>\n","      <td>0.008270</td>\n","      <td>91.938454</td>\n","      <td>2992900</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2020-01-31</td>\n","      <td>AAPL</td>\n","      <td>-0.012675</td>\n","      <td>0.002713</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>-0.002351</td>\n","      <td>-0.018417</td>\n","      <td>-0.012895</td>\n","      <td>-0.004849</td>\n","      <td>0.008517</td>\n","      <td>-0.918756</td>\n","      <td>-0.005709</td>\n","      <td>0.008745</td>\n","      <td>-0.004397</td>\n","      <td>0.008280</td>\n","      <td>90.780533</td>\n","      <td>634335</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2020-02-03</td>\n","      <td>AAPL</td>\n","      <td>0.002713</td>\n","      <td>0.001568</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>-0.012675</td>\n","      <td>-0.002351</td>\n","      <td>-0.018417</td>\n","      <td>-0.004268</td>\n","      <td>0.008622</td>\n","      <td>0.809695</td>\n","      <td>-0.004907</td>\n","      <td>0.008690</td>\n","      <td>-0.004088</td>\n","      <td>0.008224</td>\n","      <td>91.027122</td>\n","      <td>913454</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2020-02-04</td>\n","      <td>AAPL</td>\n","      <td>0.001568</td>\n","      <td>-0.001869</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0.002713</td>\n","      <td>-0.012675</td>\n","      <td>-0.002351</td>\n","      <td>-0.003963</td>\n","      <td>0.008719</td>\n","      <td>0.634254</td>\n","      <td>-0.004291</td>\n","      <td>0.008486</td>\n","      <td>-0.003852</td>\n","      <td>0.008126</td>\n","      <td>91.169922</td>\n","      <td>662663</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa0199e9-30a3-4ac9-a9eb-218b0f0b1954')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-aa0199e9-30a3-4ac9-a9eb-218b0f0b1954 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-aa0199e9-30a3-4ac9-a9eb-218b0f0b1954');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-69619361-c83a-4c0a-a41f-de0f56cde80c\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-69619361-c83a-4c0a-a41f-de0f56cde80c')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-69619361-c83a-4c0a-a41f-de0f56cde80c button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 1272,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2020-01-29 00:00:00\",\n        \"max\": \"2020-09-07 00:00:00\",\n        \"num_unique_values\": 159,\n        \"samples\": [\n          \"2020-05-18 00:00:00\",\n          \"2020-09-02 00:00:00\",\n          \"2020-07-27 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ticker\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"AMZN\",\n          \"DIS\",\n          \"AAPL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"log_return\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1270,\n        \"samples\": [\n          0.004358198028057814,\n          -0.015741758048534393,\n          0.00032276666024699807\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"r_1d\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1271,\n        \"samples\": [\n          0.011009661480784416,\n          -0.0014200751902535558,\n          -0.0010516807669773698\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weekday\",\n      \"properties\": {\n        \"dtype\": \"int8\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3,\n          1,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"int8\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          8,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1270,\n        \"samples\": [\n          0.011796596460044384,\n          -0.014038716442883015,\n          0.016896814107894897\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag2\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1270,\n        \"samples\": [\n          0.0063499826937913895,\n          0.008959803730249405,\n          -0.01760808378458023\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag3\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1270,\n        \"samples\": [\n          0.004759767558425665,\n          0.0008149401983246207,\n          0.0052503552287817\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"roll_mean_20\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1272,\n        \"samples\": [\n          -0.0007636475493200123,\n          -0.002152365166693926,\n          0.002984733087942004\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"roll_std_20\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1271,\n        \"samples\": [\n          0.012453265488147736,\n          0.010637170635163784,\n          0.007788216695189476\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"zscore_20\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1272,\n        \"samples\": [\n          -1.9369457960128784,\n          0.19622892141342163,\n          -0.48450079560279846\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ewm_mean_20\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1272,\n        \"samples\": [\n          -0.001551267458125949,\n          -0.0011013541370630264,\n          0.0023476879578083754\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ewm_std_20\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1272,\n        \"samples\": [\n          0.009351515211164951,\n          0.009463594295084476,\n          0.009540548548102379\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exp_mean\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1272,\n        \"samples\": [\n          -0.0014200815930962563,\n          -0.0008029865566641092,\n          -0.00041117225191555917\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exp_std\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1272,\n        \"samples\": [\n          0.008869551122188568,\n          0.009320287965238094,\n          0.010081321932375431\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"adj_close\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1272,\n        \"samples\": [\n          90.72705841064453,\n          97.56626892089844,\n          97.00519561767578\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"volume\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 1271,\n        \"samples\": [\n          4170878,\n          4597858,\n          2566075\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["# Time‑based split (first rolling‑origin split with embargo)"],"metadata":{"id":"LdpmVEe0Zmpg"}},{"cell_type":"code","source":["def make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n","    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n","    i = train_min - 1; out=[]\n","    while True:\n","        if i >= len(u): break\n","        a,b = u[0], u[i]\n","        vs = i + embargo + 1\n","        ve = vs + val_size - 1\n","        if ve >= len(u): break\n","        out.append((a,b,u[vs],u[ve]))\n","        i += step\n","    return out\n","\n","splits = make_rolling_origin_splits(df[\"date\"], 80, 21, 21, 5)\n","assert splits, \"Not enough history for a first split.\"\n","a,b,c,d = splits[0]\n","train_df = df[(df[\"date\"]>=a)&(df[\"date\"]<=b)].copy()\n","val_df   = df[(df[\"date\"]>=c)&(df[\"date\"]<=d)].copy()\n","print(\"Split 1 - Train:\", a.date(), \"→\", b.date(), \"| Val:\", c.date(), \"→\", d.date(),\n","      \"| train rows:\", len(train_df), \"| val rows:\", len(val_df))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QU78933nZofA","executionInfo":{"status":"ok","timestamp":1763314498357,"user_tz":360,"elapsed":30,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"21844a07-2fa6-4865-d987-a2159a2d7d07"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Split 1 - Train: 2020-01-29 → 2020-05-19 | Val: 2020-05-27 → 2020-06-24 | train rows: 640 | val rows: 168\n"]}]},{"cell_type":"markdown","source":["### 3) Reproducibility helpers, `FeatureScaler`, and `WindowedDataset`"],"metadata":{"id":"SO1Ze_5FZ0Z5"}},{"cell_type":"code","source":["import random, math, json\n","\n","def seed_everything(seed=1337):\n","    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.benchmark = False\n","        torch.backends.cudnn.deterministic = True\n","seed_everything(1337)\n","\n","class FeatureScaler: #customized version similar to StandardScaler of scikit-learn\n","    \"\"\"Train-only mean/std scaler for numpy arrays.\"\"\" # fitting on the val data woould leak information\n","    def __init__(self): self.mean_=None; self.std_=None\n","    def fit(self, X: np.ndarray):                    #addtributes endign in _ are learned from data, same convention used in scikit-learn.\n","        self.mean_ = X.mean(axis=0, dtype=np.float64) #X.shape=(T,F), mean(axis=0): feature-wise means->shape(F,)\n","        self.std_  = X.std(axis=0, dtype=np.float64) + 1e-8\n","        return self\n","    def transform(self, X: np.ndarray) -> np.ndarray:\n","        return (X - self.mean_) / self.std_\n","    def state_dict(self):\n","        return {\"mean\": self.mean_.tolist(), \"std\": self.std_.tolist()} #np array-> Python lists->JSON friendly\n","    def load_state_dict(self, d):   #restore the stats from d (for example, read from disk)\n","        self.mean_ = np.array(d[\"mean\"], dtype=np.float64)\n","        self.std_  = np.array(d[\"std\"],  dtype=np.float64)\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","class WindowedDataset(Dataset):\n","    \"\"\"\n","    Sliding windows over time per ticker (multi-ticker, fixed context_len).\n","    Each item: X in shape (T, F), y scalar: r_1d at window end.\n","    \"\"\"\n","    def __init__(self, frame: pd.DataFrame, feature_cols, context_len=64, scaler: FeatureScaler|None=None):\n","        assert \"ticker\" in frame and \"date\" in frame and \"r_1d\" in frame\n","        self.feature_cols = feature_cols\n","        self.T = int(context_len)\n","        self.groups = {}   # ticker -> dict('X': np.ndarray [N,F], 'y': np.ndarray [N])\n","        self.index  = []   # list of (ticker, end_idx)\n","        # Build groups (per ticker)\n","        for tkr, g in frame.groupby(\"ticker\"):\n","            g = g.sort_values(\"date\").reset_index(drop=True)\n","            X = g[feature_cols].to_numpy(dtype=np.float32)\n","            y = g[\"r_1d\"].to_numpy(dtype=np.float32)\n","            # valid windows end where we have T steps and y is finite\n","            for end in range(self.T-1, len(g)):\n","                if not np.isfinite(y[end]): # isfinite(value) returns true if value is not NaN, +inf, -inf. ONly windows whose target is a valid finite float are included.\n","                    continue\n","                self.index.append((tkr, end))\n","            self.groups[tkr] = {\"X\": X, \"y\": y}\n","        # Fit scaler on all TRAIN rows (only when building train dataset)\n","        self.scaler = scaler or FeatureScaler().fit( # use scaler or creae and fit a new FeatureScaler on this frame\n","            np.concatenate([self.groups[t][\"X\"] for t in self.groups], axis=0)\n","        )\n","    def __len__(self): return len(self.index)\n","    def __getitem__(self, i):\n","        tkr, end = self.index[i]\n","        g = self.groups[tkr]\n","        xs = g[\"X\"][end-self.T+1:end+1]        # (T, F) context\n","        xs = self.scaler.transform(xs)         # scale using train stats\n","        y  = g[\"y\"][end]                       # scalar target\n","        return torch.from_numpy(xs), torch.tensor(y), str(tkr)\n","\n","def make_loaders(train_df, val_df, feature_cols, context_len=64, batch_size=256, num_workers=2):\n","    # Train dataset fits scaler; Val shares it\n","    train_ds = WindowedDataset(train_df, feature_cols, context_len=context_len, scaler=None) # scaler=None: the data set fits a new scaler on the trianing data\n","    val_ds   = WindowedDataset(val_df,   feature_cols, context_len=context_len, scaler=train_ds.scaler)\n","    # Persist scaler for reuse\n","    Path(\"models\").mkdir(exist_ok=True)\n","    Path(\"models/scaler_split1.json\").write_text(json.dumps(train_ds.scaler.state_dict())) # save the scaler for future inference.\n","    pin = torch.cuda.is_available()\n","    g = torch.Generator()\n","    g.manual_seed(42)\n","    def _seed_worker(_):\n","        worker_seed = torch.initial_seed() % (2**32)\n","        np.random.seed(worker_seed); random.seed(worker_seed)\n","    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True,\n","                              num_workers=num_workers, pin_memory=pin, persistent_workers=(num_workers>0),\n","                              worker_init_fn=_seed_worker, generator=g)\n","    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False,\n","                              num_workers=num_workers, pin_memory=pin, persistent_workers=(num_workers>0),\n","                              worker_init_fn=_seed_worker)\n","    return train_ds, val_ds, train_loader, val_loader\n","\n","train_ds, val_ds, train_loader, val_loader = make_loaders(train_df, val_df, FEATS, context_len=32, batch_size=32)\n","\n","len(train_ds), len(val_ds), len(train_loader), len(val_loader), next(iter(train_loader))[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pm_ngafCZpyI","executionInfo":{"status":"ok","timestamp":1763315748505,"user_tz":360,"elapsed":240,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"4a4e9963-cb63-4bdb-fc35-26dc784a85e2"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-142154883.py:40: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  for tkr, g in frame.groupby(\"ticker\"):\n","/tmp/ipython-input-142154883.py:40: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  for tkr, g in frame.groupby(\"ticker\"):\n"]},{"output_type":"execute_result","data":{"text/plain":["(392, 0, 12, 0, torch.Size([32, 32, 6]))"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["# ⭐ What `seed_everything()` does (big picture)\n","\n","It ensures **reproducibility**.\n","\n","When you train a neural network, many parts involve randomness:\n","\n","* shuffling data\n","* initializing weights\n","* dropout masks\n","* GPU kernels picking different execution paths\n","* multi-worker DataLoader randomness\n","\n","If you **seed** everything, you make each run produce the **same results** every time — same batches, same weights, same loss curve.\n","\n","---\n","\n","# ⭐ Line-by-line explanation\n","\n","## 1. `random.seed(seed)`\n","\n","This seeds Python’s built-in random number generator.\n","\n","Affects things like:\n","\n","```python\n","random.random()\n","random.shuffle()\n","random.randint()\n","```\n","\n","So your **shuffling** or **random splits** won’t change each run.\n","\n","---\n","\n","## 2. `np.random.seed(seed)`\n","\n","Seeds NumPy's random generator.\n","\n","Affects:\n","\n","* random number generation in NumPy\n","* bootstraps\n","* sampling\n","* any NumPy-based augmentation\n","\n","This ensures all NumPy randomness is reproducible.\n","\n","---\n","\n","## 3. `torch.manual_seed(seed)`\n","\n","Seeds PyTorch’s random generator **on CPU**.\n","\n","Controls:\n","\n","* weight initialization\n","* dropout masks\n","* any CPU tensor randomness\n","\n","---\n","\n","## 4. `torch.cuda.manual_seed_all(seed)`\n","\n","If a GPU is available, this seeds **ALL GPUs** (in case of multi-GPU training).\n","\n","Controls:\n","\n","* GPU tensor randomness\n","* CUDA RNG kernels\n","* dropout on GPU\n","\n","---\n","\n","## 5. `torch.backends.cudnn.benchmark = False`\n","\n","cuDNN wants to “auto-tune” convolution algorithms for faster speed.\n","\n","This auto-tuning introduces **non-determinism**.\n","\n","Setting `benchmark=False` disables that tuning → more consistent behavior.\n","\n","---\n","\n","## 6. `torch.backends.cudnn.deterministic = True`\n","\n","Forces cuDNN to use **deterministic algorithms** only.\n","\n","Some GPU ops have fast algorithms that are nondeterministic.\n","This makes them deterministic — but sometimes a bit slower.\n","\n","\n","---\n","\n","# ⭐ Quick check\n","\n","❓ If you **don’t** call `seed_everything()`, which of these will vary between runs?\n","\n","A. Weight initialization\n","\n","B. Dropout masks\n","\n","C. Shuffle order of DataLoader batches\n","\n","D. GPU random operations\n","\n","E. All of the above\n","\n","<details>\n","  <summary>Click to show answer</summary>\n","\n","E\n","\n","</details>\n","\n","\n","## ⭐ Tiny reinforcement check\n","\n","Just to solidify your understanding:\n","\n","### ❓ Why does setting\n","\n","```python\n","torch.backends.cudnn.deterministic = True\n","```\n","\n","make training slightly slower?\n","\n","Pick one:\n","\n","A. It forces cuDNN to use deterministic algorithms that might not be the fastest.\n","B. It keeps the GPU in CPU mode.\n","C. It disables the optimizer.\n","D. It runs all operations twice.\n","\n","Which one feels right?\n","\n","\n","<details>\n","  <summary>Click to show answer</summary>\n","\n"," **A**\n","\n","Deterministic ≠ fastest.\n","cuDNN often has multiple algorithms for convolution, pooling, etc.:\n","\n","* Some are fast but nondeterministic\n","* Some are slower but deterministic\n","\n","When you set:\n","\n","```python\n","torch.backends.cudnn.deterministic = True\n","```\n","\n","PyTorch forces cuDNN to pick the **deterministic-but-slower** ones.\n","\n","That’s the tradeoff:\n","\n","> **Perfect reproducibility → slight speed loss.**\n","\n","\n","</details>"],"metadata":{"id":"AIPubW8clUyq"}},{"cell_type":"markdown","source":["\n","# ⭐ What `FeatureScaler` does (big picture)\n","\n","It is a **very small custom version** of scikit-learn’s `StandardScaler`:\n","\n","* It computes **mean** and **std** from the **training** set.\n","* It stores them.\n","* It applies the same normalization to **train** and **val** windows.\n","* It can save/load the scaling stats (`state_dict`).\n","\n","We do NOT fit the scaler on the **validation** data — that would leak information.\n","\n","\n","Because the model uses a **sliding window** per ticker:\n","\n","* If we normalize per window → we leak future information\n","* If we normalize per ticker → different tickers have mismatched scales\n","* If we normalize per split → val is contaminated by train\n","\n","Thus:\n","\n","> **We compute mean/std on ALL training rows across ALL tickers — only once.**\n","\n","Then reuse these stats for val/test/inference.\n","\n","\n","\n","# ⭐ Small check\n","\n","Look at this line:\n","\n","```python\n","self.std_ = X.std(axis=0, dtype=np.float64) + 1e-8\n","```\n","\n","### ❓ Why do you think we add `1e-8` to the std?\n","\n","A. To make training faster\n","\n","B. To avoid dividing by zero\n","\n","C. To improve GPU performance\n","\n","D. To increase randomness\n","\n","\n","Which one feels right?\n","<details>\n","  <summary>Click to show answer</summary>\n","\n"," B\n","</details>\n"],"metadata":{"id":"4YUTQnMjewm7"}},{"cell_type":"markdown","source":["⭐ ## Tiny check\n","\n","❓ Why do we **reuse** the same `scaler` for the validation dataset instead of fitting a new one on `val_df`?\n","\n","Try to say it in your own words (even a short sentence is fine).\n","<details>\n","  <summary>Click to show answer</summary>\n","\n","We must **not leak information from the validation set** into the training process.\n","\n","If you fit the scaler on validation data:\n","\n","* the mean/std of validation features influence the model\n","* so the model “peeks” at future data\n","* this destroys the purpose of validation\n","* and gives overly optimistic evaluation results\n","\n","So the rule is:\n","\n","> **Fit normalization only on training data.\n","> Apply (reuse) that normalization on val/test/inference data.**\n","\n","</details>\n","\n","\n","# ⭐ Quick reinforcement (small question)\n","\n","Look at this line again:\n","\n","```python\n","np.concatenate([self.groups[t][\"X\"] for t in self.groups], axis=0)\n","```\n","\n","### ❓ Why do we concatenate all tickers’ `X` values before fitting the scaler,\n","\n","instead of fitting a separate scaler **per ticker**?\n","\n","Choose the best reason:\n","\n","A. Separate scalers per ticker would leak information.\n","\n","B. Tick-by-tick scaling would make features inconsistent across tickers.\n","\n","C. Concatenation makes training faster.\n","\n","D. PyTorch requires it.\n","\n","\n","Which one do you think is correct?\n","\n","<details>\n","  <summary> click to see the answer. </summary>\n","   **B **\n","\n","If we used **one scaler per ticker**, then:\n","\n","* AAPL features would be normalized using AAPL’s own mean/std\n","* MSFT features using MSFT’s mean/std\n","* NVDA using NVDA’s mean/std\n","* etc.\n","\n","That means:\n","\n","> The same model input feature (e.g., volume, return) would be on **different scales** depending on which ticker it came from.\n","\n","The neural network would then struggle because:\n","\n","* A “0.1” feature from AAPL means something **different** from a “0.1” feature from MSFT.\n","* Feature ranges vary ticker-to-ticker.\n","* The model must learn two things at once:\n","\n","  * the pattern\n","  * the rescaling needed for each ticker\n","\n","This makes training harder and slower.\n","\n","So we instead compute **one global mean** and **one global std** across all tickers.\n","\n","This ensures:\n","\n","* All tickers sit in the **same feature space**\n","* Data is comparable across tickers\n","* The model doesn't accidentally learn scaling instead of patterns\n","\n","</details>\n","\n","\n","\n","⭐❓ Suppose MSFT has 10× larger dollar volume than AAPL.\n","After global scaling, will the model still “see” that difference?\n","\n","A. Yes\n","\n","B. No\n","\n","C. Only during training\n","\n","D. Only if we add a special feature\n","\n","What do you think?\n","\n","<details>\n","<summary> click to see the answer </summary>\n","\n"," **A — Yes**, the model still sees the difference.\n","\n","\n","\n","### ✔ What scaling *removes*\n","\n","Global standardization removes:\n","\n","* absolute magnitude\n","* raw units\n","* arbitrary differences like “MSFT volume is expressed in millions, AAPL in thousands”\n","\n","### ✔ What scaling *keeps*\n","\n","Scaling **does not remove the pattern** that MSFT tends to have higher volume than AAPL.\n","\n","Why?\n","\n","Because standardization (z-scoring):\n","\n","[\n","x' = \\frac{x - \\mu}{\\sigma}\n","]\n","\n","preserves **relative differences**:\n","\n","* If MSFT volume is frequently above the global mean → its z-scores stay **positive**.\n","* If AAPL volume is usually lower → its z-scores stay **negative or smaller**.\n","\n","So the *ranking* and *relative position* are preserved, even though the raw numbers are not.\n","\n","---\n","\n","#  Quick intuition example\n","\n","Suppose global mean volume = 5M shares.\n","\n","| Ticker | Actual Volume | After Scaling |\n","| ------ | ------------- | ------------- |\n","| MSFT   | 12M           | high positive |\n","| AAPL   | 3M            | negative      |\n","\n","Even after scaling:\n","\n","* MSFT still looks “bigger”\n","* AAPL still looks “smaller”\n","\n","The neural network sees this difference clearly.\n","\n","\n","</details>\n","# ⭐ Mini-check (one quick question)\n","\n","If two features are perfectly correlated before scaling (one is exactly 2× the other),\n","after scaling they will be:\n","\n","A. still perfectly correlated\\\n","B. only partly correlated\\\n","C. uncorrelated\\\n","D. random relative to each other\n","\n","What do you think?\n","<details>\n","<summary> Click for answer </summary>\n"," **A is correct.**\n","\n","\n","\n","If two features satisfy:\n","\n","$$\n","x_2 = 2x_1\n","$$\n","\n","then when you standardize each:\n","\n","$$\n","x_1' = \\frac{x_1 - \\mu_1}{\\sigma_1},\\quad\n","x_2' = \\frac{x_2 - \\mu_2}{\\sigma_2}\n","$$\n","\n","the relationship becomes:\n","\n","$$\n","x_2' = \\frac{2x_1 - \\mu_2}{\\sigma_2}\n","$$\n","\n","But since:\n","\n","* $\\mu_2 = 2\\mu_1$\n","* $\\sigma_2 = 2\\sigma_1$\n","\n","we get:\n","\n","$$\n","x_2' = \\frac{2(x_1 - \\mu_1)}{2\\sigma_1}\n","= \\frac{x_1 - \\mu_1}{\\sigma_1}\n","= x_1'\n","$$\n","\n","So after scaling:\n","\n","$$\n","x_2' = x_1'\n","$$\n","\n","\n","</details>\n","\n","## ⭐ Mini-Check To Lock It In\n","\n","Suppose we have:\n","\n","* AAPL’s 10-day average volume tends to be slightly below the global mean.\n","* MSFT’s tends to be above the global mean.\n","\n","After global scaling…\n","\n","### ❓ Which will happen?\n","\n","A. AAPL’s normalized volume tends to be negative; MSFT’s tends to be positive\n","B. The model cannot tell them apart\n","C. All tickers will have zero-mean volume on every individual day\n","D. Scaling destroys all volume information\n","\n","Which one feels right **now that you understand the mechanism**?\n","<details>\n","<summary> click for answer </summary>\n","**A**\n","</detials>\n","\n","\n","\n","\n"],"metadata":{"id":"X5l6s0aSobQ6"}},{"cell_type":"markdown","source":["# ⭐ Build PyTorch datasets and Dataloaders\n","\n","# 1. Build the two datasets\n","\n","```python\n","train_ds = WindowedDataset(..., scaler=None)\n","val_ds   = WindowedDataset(..., scaler=train_ds.scaler)\n","```\n","\n","###  Train dataset\n","\n","*  `scaler=None`, which means the dataset **fits a new scaler** on the training features.\n","* This scaler is saved as `train_ds.scaler`.\n","\n","###  Validation dataset\n","\n","*  `scaler=train_ds.scaler`\n","* So validation windows use **the same mean/std** as the train set.\n","* This avoids data leakage\n","\n","---\n","\n","# ⭐ 2. Save the scaler to disk\n","\n","```python\n","Path(\"models\").mkdir(exist_ok=True)\n","Path(\"models/scaler_split1.json\").write_text(json.dumps(train_ds.scaler.state_dict()))\n","```\n","\n","Because later, when you restore your model for inference on new data,\n","you must use **the same scaling statistics**.\n","\n","This saves the scaler as a JSON file like:\n","\n","```json\n","{\"mean\": [...], \"std\": [...]}\n","```\n","\n","\n","# ⭐ 3. Setup for DataLoader\n","\n","### `pin_memory = torch.cuda.is_available()`\n","\n","If CUDA is available:\n","\n","* DataLoader uses **page-locked host memory**,\n","* which makes **H→D copies faster**.\n","\n","\n","---\n","\n","# ⭐ 4. Create a manual random generator\n","\n","```python\n","g = torch.Generator()\n","g.manual_seed(42)\n","```\n","\n","This ensures **DataLoader shuffling is reproducible** across runs.\n","\n","Without this, even if you `seed_everything()`,\n","**DataLoader workers** may shuffle batches differently.\n","\n","---\n","\n","# ⭐ 5. Worker seeding\n","\n","```python\n","def _seed_worker(_):\n","    worker_seed = torch.initial_seed() % (2**32)\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","```\n","\n","When using `num_workers > 0`, each worker process must be seeded,\n","otherwise:\n","\n","* batch shuffling becomes nondeterministic\n","\n","* dropout masks produced on CPU become inconsistent\n","\n","This function ensures **each worker gets its own deterministic seed**.\n","\n","---\n","\n","# ⭐ 6. Build train_loader\n","\n","```python\n","train_loader = DataLoader(\n","    train_ds,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    drop_last=True,\n","    num_workers=num_workers,\n","    pin_memory=pin,\n","    persistent_workers=(num_workers>0),\n","    worker_init_fn=_seed_worker,\n","    generator=g\n",")\n","```\n","\n","Key points:\n","\n","###  `shuffle=True`\n","\n","Randomizes order of batches every epoch.\n","\n","###  `drop_last=True`\n","\n","Drops the last partial batch.\n","\n","* `len(train_ds) < batch_size` → **no batches at all**\n","* this is why you saw `StopIteration`\n","\n","###  `persistent_workers=True`\n","\n","Keeps workers alive between epochs → faster reload.\n","\n","###  `pin_memory`\n","\n","Speeds up CPU→GPU transfers.\n","\n","###  `worker_init_fn` + `generator`\n","\n","Makes shuffling + worker randomness reproducible.\n","\n","---\n","\n","# ⭐ 7. Build val_loader\n","\n","```python\n","val_loader = DataLoader(\n","    val_ds,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    drop_last=False,\n","    num_workers=num_workers,\n","    pin_memory=pin,\n","    persistent_workers=(num_workers>0),\n","    worker_init_fn=_seed_worker\n",")\n","```\n","\n","Differences from train_loader:\n","\n","* No shuffling (`shuffle=False`)\n","* No dropping incomplete batch (`drop_last=False`)\n","\n","Validation must be deterministic and evaluate **all** data.\n","\n","---\n","\n","\n","\n","# ⭐ Mini-check\n","\n","### ❓ Why do we use\n","\n","```python\n","shuffle=True\n","```\n","\n","for the **training** DataLoader but\n","\n","```python\n","shuffle=False\n","```\n","\n","for the **validation** DataLoader?\n","\n","<details>\n","<summary> click for answer </summary>\n","\n","Why `shuffle=True` for training\n","\n","During training, we want batches to be:\n","\n","* independent\n","* randomly mixed\n","* not in chronological/ticker order\n","* not grouped by similar values\n","\n","Why?\n","\n","Because shuffling:\n","\n","* prevents the model from overfitting to ordering patterns\n","* improves gradient estimates\n","* reduces variance\n","* helps SGD converge smoothly\n","* avoids “bad luck” batches (e.g., all rising stocks in one batch)\n","\n","\n","> **Training = shuffle to help the model generalize.**\n","\n","\n","\n"," Why `shuffle=False` for validation\n","\n","Validation must be:\n","\n","* deterministic\n","* repeatable\n","* stable from epoch to epoch\n","* in the same order so metrics can be compared\n","\n","If validation were shuffled:\n","\n","* the order changes, so evaluation would vary\n","* metrics like MAE might change slightly run-to-run\n","* debugging becomes harder\n","* reproducibility is lost\n","\n","\n","> **Validation = NO shuffle, because we want a consistent measurement.**\n","\n","---\n","\n"," summary in one sentence\n","\n","> **Shuffle training to improve learning; keep validation ordered to measure performance consistently.**\n","\n","\n","</details>\n"],"metadata":{"id":"DdtbXluB1mfL"}},{"cell_type":"markdown","source":["# ⭐ 1. What is a **worker** (in DataLoader)?\n","\n","**A worker = a separate background process created by DataLoader**\n","when you set:\n","\n","```python\n","num_workers > 0\n","```\n","\n","Each worker:\n","\n","* loads data\n","* runs your `Dataset.__getitem__`\n","* prepares batches\n","* feeds batches to the main process\n","\n","This makes loading **much faster**, because workers run **in parallel**.\n","\n","### Example\n","\n","If you set:\n","\n","```python\n","DataLoader(..., num_workers=4)\n","```\n","\n","Then the DataLoader creates **4 separate processes**.\n","These 4 workers load batches at the same time.\n","\n","### Why we need worker a seed?\n","\n","Because each worker has its own random state.\n","Without seeding, each worker would shuffle, augment, or sample **differently every run**.\n","\n","\n","\n","# ⭐ 2. What is `torch.Generator`?\n","\n","A **torch.Generator** is an object that *stores a random-number generator state*.\n","You can think of it as “a private RNG for PyTorch.”\n","\n","Example:\n","\n","```python\n","g = torch.Generator()\n","g.manual_seed(42)\n","```\n","\n","This creates a **separate, independent RNG** that DataLoader can use for shuffling.\n","\n","If we did not use our own generator:\n","\n","* DataLoader shuffling can vary between runs\n","* Even if you call `torch.manual_seed`, DataLoader spawns new worker processes → new seeds → different order\n","\n","So this is the **correct and safe** way to force DataLoader to shuffle deterministically.\n","\n","---\n","\n","# ⭐ 3. Difference between\n","\n","### `torch.Generator().manual_seed(seed)`\n","\n","### `torch.initial_seed()`\n","\n","### `torch.manual_seed(seed)`\n","\n","\n","---\n","\n","##  **A. `torch.manual_seed(seed)`**\n","\n","Seeds **the global PyTorch RNG on the main process**.\n","\n","Affects:\n","\n","* weight initialization\n","* dropout masks\n","* CPU ops with randomness\n","* GPU ops with randomness (only for the current device)\n","\n","You usually call:\n","\n","```python\n","torch.manual_seed(1337)\n","```\n","\n","But DataLoader workers do **not** inherit this seed predictably — they fork from the parent process and get modified seeds.\n","\n","---\n","\n","##  **B. `torch.Generator().manual_seed(seed)`**\n","\n","Creates a **separate RNG object**, with its own seed.\n","\n","Example:\n","\n","```python\n","g = torch.Generator()\n","g.manual_seed(42)\n","```\n","\n","This is mainly used when you want:\n","\n","* reproducible DataLoader shuffling\n","* reproducible random sampling inside a function\n","* an RNG that is separate from the global one\n","\n","When you pass it into DataLoader:\n","\n","```python\n","DataLoader(..., generator=g)\n","```\n","\n","you get **deterministic shuffling**.\n","\n","\n","\n","##  **C. `torch.initial_seed()`**\n","\n","This is called **inside a worker process**.\n","\n","It returns:\n","\n","> the seed assigned to this worker by PyTorch\n","\n","This is why worker seeding functions use:\n","\n","```python\n","worker_seed = torch.initial_seed() % (2**32)\n","```\n","\n","Each worker gets a *different*, *deterministic* seed, created from the global seed and worker ID.\n","\n","This seed is used to seed:\n","\n","* NumPy\n","* Python’s `random`\n","\n","inside that worker.\n","\n","---\n","\n","# ⭐ Summary Table (super simple)\n","\n","| Function                           | Where it applies | What it seeds                          |\n","| ---------------------------------- | ---------------- | -------------------------------------- |\n","| `torch.manual_seed(s)`             | main process     | global PyTorch RNG                     |\n","| `torch.Generator().manual_seed(s)` | private RNG      | DataLoader shuffle or custom RNG state |\n","| `torch.initial_seed()`             | worker process   | returns worker’s assigned seed         |\n","\n","---\n","\n","# ⭐ One-sentence summary\n","\n","> **`torch.manual_seed` seeds PyTorch globally,\n","> `torch.Generator().manual_seed` seeds a private RNG (often for DataLoader),\n","> and `torch.initial_seed()` tells each worker its assigned seed.**\n","\n","---\n","\n","# ⭐ Quick check (your turn)\n","\n","### ❓ Why do we need both\n","\n","`generator=g` **and** `worker_init_fn=_seed_worker`\n","in the DataLoader?\n","\n","Pick the best explanation:\n","\n","A. Generator controls global randomness; worker_init_fn adds speed.\\\n","B. Generator seeds shuffling; worker_init_fn seeds each worker process.\\\n","C. One is for CPU, the other is for GPU.\\\n","D. Both do the same thing; one is redundant.\\\n","\n","What do you think?\n","<details>\n","<summary> Click for answer </summary>\n","\n"," **B is correct.**\n","\n","\n","---\n","\n","\n","### ✔ `generator=g`\n","\n","Controls **the shuffling order** of the DataLoader.\n","\n","* Ensures batch order is identical each run\n","* Applies before worker processes are created\n","* Guarantees reproducible sampling\n","\n","Think of it as:\n","\n","> “What order should batches come in?”\n","\n","---\n","\n","###  `worker_init_fn=_seed_worker`\n","\n","Seeds **each worker process** (Python + NumPy RNG).\n","\n","* Ensures each worker samples deterministically\n","* Makes random transforms / sampling within the worker reproducible\n","* Prevents workers from generating different data each run\n","\n","Think of it as:\n","\n","> “Inside each worker, how should random numbers behave?”\n","\n","---\n","\n","#  Together, they guarantee FULL reproducibility\n","\n","You need **both** pieces:\n","\n","1. **`generator`** → controls *global* behavior (shuffling)\n","2. **`worker_init_fn`** → controls *per-worker* behavior (random ops in workers)\n","\n","Without `generator`:\n","You cannot guarantee the same shuffle order.\n","\n","Without `worker_init_fn`:\n","Each worker will produce different per-item randomness every run.\n","\n","With both:\n","Your entire data pipeline becomes **completely deterministic**.\n","\n","\n","\n","</details>\n"],"metadata":{"id":"4kTKJAxA49AI"}},{"cell_type":"markdown","source":[" # ⭐ why both `val_ds` and `val_loader` have length zero?\n"],"metadata":{"id":"PEpjPXVn-FBp"}},{"cell_type":"code","source":["len(train_df), len(val_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hiTCPAh8oawU","executionInfo":{"status":"ok","timestamp":1763318554880,"user_tz":360,"elapsed":41,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"6a98671e-0833-4465-8a86-c53c2a0a57e2"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(640, 168)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["val_df.groupby(\"ticker\").size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":954},"id":"vBREh6Ie_Eel","executionInfo":{"status":"ok","timestamp":1763318723379,"user_tz":360,"elapsed":21,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"81f3b13a-5ade-4874-d703-bd9c289b0b07"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-691067041.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  val_df.groupby(\"ticker\").size()\n"]},{"output_type":"execute_result","data":{"text/plain":["ticker\n","AAPL     21\n","AMZN     21\n","BAC      21\n","CSCO     21\n","CVX      21\n","DIS      21\n","GOOGL    21\n","HD       21\n","INTC      0\n","JNJ       0\n","JPM       0\n","KO        0\n","META      0\n","MSFT      0\n","NFLX      0\n","NVDA      0\n","ORCL      0\n","PFE       0\n","PG        0\n","T         0\n","TSLA      0\n","V         0\n","VZ        0\n","WMT       0\n","XOM       0\n","dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","    <tr>\n","      <th>ticker</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>AAPL</th>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>AMZN</th>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>BAC</th>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>CSCO</th>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>CVX</th>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>DIS</th>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>GOOGL</th>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>HD</th>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>INTC</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>JNJ</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>JPM</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>KO</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>META</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>MSFT</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>NFLX</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>NVDA</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>ORCL</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>PFE</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>PG</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>T</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>TSLA</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>V</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>VZ</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>WMT</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>XOM</th>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["val_df.groupby(\"ticker\").size().describe()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"rAXiPqkE-ivE","executionInfo":{"status":"ok","timestamp":1763318579033,"user_tz":360,"elapsed":51,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"f3a59f56-a895-46a9-9bc9-230b809457af"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3518275911.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  val_df.groupby(\"ticker\").size().describe()\n"]},{"output_type":"execute_result","data":{"text/plain":["count    25.000\n","mean      6.720\n","std       9.998\n","min       0.000\n","25%       0.000\n","50%       0.000\n","75%      21.000\n","max      21.000\n","dtype: float64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>25.000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>6.720</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>9.998</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>21.000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>21.000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> float64</label>"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# Fix\n","# A ticker contributes a window (T) only when 1. there are at least T=context_len rows in g; 2. And y[end]=r_1d is finite.\n","# Adjust the context size.\n","train_ds, val_ds, train_loader, val_loader = make_loaders(train_df, val_df, FEATS, context_len=16, batch_size=32)\n","\n","len(train_ds), len(val_ds), len(train_loader), len(val_loader), next(iter(train_loader))[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xV9drusE_s56","executionInfo":{"status":"ok","timestamp":1763318940020,"user_tz":360,"elapsed":192,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"2ae23be3-7231-4e92-d894-66a51ef4e21b"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-142154883.py:40: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  for tkr, g in frame.groupby(\"ticker\"):\n","/tmp/ipython-input-142154883.py:40: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  for tkr, g in frame.groupby(\"ticker\"):\n"]},{"output_type":"execute_result","data":{"text/plain":["(520, 48, 16, 2, torch.Size([32, 16, 6]))"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["# ⭐ 4) Define a tiny GRU regressor"],"metadata":{"id":"l-tszd2gZ_ko"}},{"cell_type":"markdown","source":["A GRU (Gated Recurrent Unit) model is a type of recurrent neural network (RNN) designed for processing sequential data, like text or time series. It addresses the limitations of standard RNNs by using a gating mechanism to manage the flow of information, making it more effective at capturing long-term dependencies. Compared to LSTM, the GRU is a simpler and more computationally efficient architecture, with fewer parameters, making it faster to train.  [1, 2, 3, 4, 5, 6]  \n","You can watch this video to learn about the working of GRU in detail: https://www.youtube.com/watch?v=IBs8D8PWMc8 (https://www.youtube.com/watch?v=IBs8D8PWMc8)\n","Key features of a GRU model:\n","\n","• Gating mechanism: GRUs use two gates—an update gate and a reset gate—to control what information is remembered, forgotten, or passed on. [2, 5, 6]  \n","\t• Update gate: Determines how much of the previous hidden state to keep and how much of the new candidate state to add, balancing past and present information. [2, 7]  \n","\t• Reset gate: Decides how much of the previous information to forget, allowing the network to disregard irrelevant past information. [2, 6, 8]  \n","\n","• Simplified architecture: It merges the forget and input gates into a single update gate and does not have a separate cell state like LSTMs, resulting in fewer parameters. [2, 4, 5]  \n","• Efficiency: Due to its simpler structure, the GRU model is often faster to train than an LSTM while performing similarly on many tasks. [4, 5, 9]  \n","• Sequential data processing: It excels at handling sequential data by maintaining a hidden state that summarizes information from previous steps. [3, 10]  \n","\n","Common applications:\n","\n","• Natural Language Processing (NLP): Used for tasks like text classification, sentiment analysis, and machine translation. [3, 11, 12]  \n","• Time series forecasting: Effective for predicting future values in a sequence, such as stock prices or weather patterns. [3, 13]  \n","• Speech recognition: Models the temporal dependencies in speech signals to improve accuracy. [1, 3]  \n","\n","\n","[1] https://en.wikipedia.org/wiki/Gated_recurrent_unit\n","[2] https://medium.com/@maizi5469/5-0-gated-recurrent-unit-gru-4bcd3065734f\n","[3] https://ravjot03.medium.com/gru-explained-the-simplified-rnn-solution-for-sequential-data-c706d0d149c5\n","[4] https://medium.com/@anishnama20/understanding-gated-recurrent-unit-gru-in-deep-learning-2e54923f3e2\n","[5] https://medium.com/@deepanshusnpt/day-10-gated-recurrent-unit-gru-networks-a-simplified-rnn-56aedfa3faf4\n","[6] https://www.analyticsvidhya.com/blog/2021/03/introduction-to-gated-recurrent-unit-gru/\n","[7] https://www.activeloop.ai/resources/glossary/gated-recurrent-units-gru/\n","[8] https://towardsdatascience.com/the-math-behind-gated-recurrent-units-854d88aded65/\n","[9] https://stackoverflow.com/questions/59932978/which-one-is-faster-either-gru-or-lstm\n","[10] https://www.mdpi.com/2073-4441/17/21/3039\n","[11] https://schneppat.com/gated-recurrent-unit-gru.html\n","[12] https://www.pickl.ai/blog/gated-recurrent-unit-in-deep-learning/\n","[13] https://medium.com/@chandramouliarun/gated-recurrent-units-grus-a-deep-dive-into-modern-sequence-modeling-4a90086101e9\n","\n","\n"],"metadata":{"id":"D_4v3G1DChBX"}},{"cell_type":"code","source":["import torch.nn as nn, torch\n","\n","class GRURegressor(nn.Module):\n","    def __init__(self, in_features: int, hidden: int = 64, num_layers: int = 2, dropout: float = 0.1):\n","        super().__init__()\n","        self.gru = nn.GRU(input_size=in_features, hidden_size=hidden, num_layers=num_layers,\n","                          batch_first=True, dropout=dropout if num_layers>1 else 0.0)\n","        self.head = nn.Sequential(\n","            nn.Linear(hidden, hidden),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden, 1)\n","        )\n","    def forward(self, x):        # x: (B, T, F)\n","        _, hN = self.gru(x)      # hN: (num_layers, B, H); take last layer\n","        h = hN[-1]               # (B, H)\n","        return self.head(h).squeeze(-1)  # (B,)\n","\n","def make_model():\n","    return GRURegressor(in_features=len(FEATS), hidden=64, num_layers=2, dropout=0.1)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = make_model().to(device)\n","sum(p.numel() for p in model.parameters()), device"],"metadata":{"id":"-XIFIh7NZ3en","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763319237353,"user_tz":360,"elapsed":21,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"5f2e3fc5-5284-4671-e226-78588d9f15c5"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(43009, device(type='cpu'))"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["## ⭐ 1. The GRURegressor class\n","\n","```python\n","class GRURegressor(nn.Module):\n","    def __init__(self, in_features: int, hidden: int = 64, num_layers: int = 2, dropout: float = 0.1):\n","        super().__init__()\n","```\n","\n","* Subclass of `nn.Module` → standard PyTorch model.\n","* `in_features` = number of input features per time step (this will be `len(FEATS)`).\n","* `hidden` = hidden size of the GRU.\n","* `num_layers` = how many GRU layers stacked.\n","* `dropout` = regularization strength.\n","\n","---\n","\n","### 1a. The GRU layer\n","\n","```python\n","self.gru = nn.GRU(\n","    input_size=in_features,\n","    hidden_size=hidden,\n","    num_layers=num_layers,\n","    batch_first=True,\n","    dropout=dropout if num_layers>1 else 0.0\n",")\n","```\n","\n","* `input_size=in_features` → each time step has that many features.\n","* `hidden_size=hidden` → GRU hidden dimension = H.\n","* `num_layers=2` → stacked GRU with 2 layers.\n","* `batch_first=True` → input shape is `(B, T, F)` instead of `(T, B, F)`.\n","* Dropout between GRU layers only applies if `num_layers > 1` (PyTorch requirement).\n","\n","So this GRU consumes a sequence and gives you:\n","\n","* `output`: `(B, T, H)`\n","* `hN`: `(num_layers, B, H)` → final hidden state for each layer.\n","\n","---\n","\n","### 1b. The head (regression on the final state)\n","\n","```python\n","self.head = nn.Sequential(\n","    nn.Linear(hidden, hidden),\n","    nn.ReLU(),\n","    nn.Dropout(dropout),\n","    nn.Linear(hidden, 1)\n",")\n","```\n","\n","This is a small MLP that maps from the final GRU hidden state (size H) to a scalar prediction:\n","\n","* `Linear(hidden, hidden)` → dense layer.\n","* `ReLU()` → nonlinearity.\n","* `Dropout` → regularization.\n","* `Linear(hidden, 1)` → final scalar output per example.\n","\n","---\n","\n","## 2. The forward method\n","\n","```python\n","def forward(self, x):        # x: (B, T, F)\n","    _, hN = self.gru(x)      # hN: (num_layers, B, H);\n","    h = hN[-1]               # take last layer->(B, H)\n","    return self.head(h).squeeze(-1)  # (B,)\n","```\n","\n","Step-by-step:\n","\n","1. Input `x` shape: `(B, T, F)`\n","\n","   * B = batch size\n","   * T = context_len\n","   * F = number of features (`len(FEATS)`)\n","\n","2. `self.gru(x)` returns `(output, hN)`:\n","\n","   * `output`: `(B, T, H)` (unused here)\n","   * `hN`: `(num_layers, B, H)` = final hidden per layer\n","\n","3. `h = hN[-1]` picks the last layer’s final hidden state: shape `(B, H)`.\n","\n","4. `self.head(h)` → shape `(B, 1)`.\n","\n","5. `.squeeze(-1)` → removes the last dimension → `(B,)`, a 1D tensor of predictions.\n","\n","So each batch gives you **one scalar prediction per sequence** (per window).\n","\n","---\n","\n","## 3. Building and moving the model to device\n","\n","```python\n","def make_model():\n","    return GRURegressor(in_features=len(FEATS), hidden=64, num_layers=2, dropout=0.1)\n","```\n","\n","* Uses the feature list `FEATS` to set correct `in_features`.\n","\n","```python\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = make_model().to(device)\n","```\n","\n","* Picks `\"cuda\"` if a GPU is available, else `\"cpu\"`.\n","* Moves all model parameters to that device.\n","\n","---\n","\n","## 4. Counting parameters\n","\n","```python\n","sum(p.numel() for p in model.parameters()), device\n","```\n","\n","* `model.parameters()` → iterator over all tensors in the model.\n","* `p.numel()` → number of elements in that tensor.\n","* `sum(...)` → total number of trainable parameters.\n","\n","The expression returns a tuple: `(n_params, device)`.\n","\n","---\n","\n","## ⭐ Tiny check\n","\n","In `forward`, we used `hN[-1]`.\n","\n","### ❓ Why are we taking `hN[-1]` instead of `hN[0]`?\n","\n","Pick the best answer:\n","\n","A. `hN[-1]` gives the final time step of the sequence.\\\n","B. `hN[-1]` gives the last layer’s hidden state, which is usually the most processed representation.\\\n","C. `hN[-1]` saves memory.\\\n","D. It doesn’t matter; any index would work.\\\n","\n","What do you think?\n","<details>\n","<summary> click for answer </summary>\n"," **B is the correct and important reason.**\n","\n","\n","\n","#  Why we use `hN[-1]`\n","\n","For an `nn.GRU` with `num_layers=2`,\n","the shape of the hidden state is:\n","\n","```\n","hN: (num_layers, B, H)\n","```\n","\n","That means:\n","\n","* `hN[0]` = final hidden state of **layer 1**\n","* `hN[1]` = final hidden state of **layer 2** (the *top* layer)\n","\n","And in general:\n","\n","> **`hN[-1]` is the deepest representation the model has computed.\n","> It's the most expressive summary of the entire sequence.**\n","\n","So this is the correct choice for downstream regression.\n","\n","If you used `hN[0]`, you'd be using the shallowest layer's representation — less abstract, usually worse.\n","\n","\n","\n","</details>\n"],"metadata":{"id":"vlejfJeX-Dva"}},{"cell_type":"markdown","source":["# 5) Training loop with AMP, early stopping, checkpointing"],"metadata":{"id":"TO6I1nBIaCcI"}},{"cell_type":"code","source":["from torch.optim import AdamW\n","from torch.cuda.amp import autocast, GradScaler\n","import math, time\n","\n","def mae_t(y_true, y_pred): return torch.mean(torch.abs(y_true - y_pred))\n","def smape_t(y_true, y_pred, eps=1e-8):\n","    return torch.mean(2.0*torch.abs(y_true - y_pred)/(torch.abs(y_true)+torch.abs(y_pred)+eps))\n","\n","def train_one_epoch(model, loader, optimizer, scaler, device, use_amp=True):  # scaler will be set as GradScaler\n","    model.train(); total=0.0; n=0\n","    for xb, yb, _ in loader:\n","        xb = xb.to(device, non_blocking=True).float()\n","        yb = yb.to(device, non_blocking=True).float()\n","        optimizer.zero_grad(set_to_none=True)\n","        if use_amp and device.type==\"cuda\":\n","            with autocast(dtype=torch.float16):\n","                pred = model(xb)\n","                loss = mae_t(yb, pred)  # train with MAE (robust)\n","            scaler.scale(loss).backward()  #scaler =GradScaler() see below in function fit()\n","            scaler.step(optimizer); scaler.update()\n","        else:\n","            pred = model(xb); loss = mae_t(yb, pred)\n","            loss.backward(); optimizer.step()\n","        bs = xb.size(0); total += loss.item()*bs; n += bs\n","    return total/max(n,1) #avg loss over all samples\n","\n","@torch.no_grad() # turn off gradient tracking, see below for in-depth\n","def evaluate(model, loader, device):\n","    model.eval(); tot_mae=tot_smape=0.0; n=0\n","    for xb, yb, _ in loader:\n","        xb = xb.to(device, non_blocking=True).float()\n","        yb = yb.to(device, non_blocking=True).float()\n","        pred = model(xb)\n","        bs = xb.size(0)\n","        tot_mae   += mae_t(yb, pred).item()*bs\n","        tot_smape += smape_t(yb, pred).item()*bs\n","        n += bs\n","    return {\"mae\": tot_mae/max(n,1), \"smape\": tot_smape/max(n,1)}\n","\n","def fit(model, train_loader, val_loader, epochs=12, lr=1e-3, wd=1e-5, patience=3, use_amp=True):\n","    opt = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n","    scaler = GradScaler(enabled=use_amp and (device.type==\"cuda\"))\n","    best = math.inf; best_metrics=None; best_epoch=-1\n","    ckpt_path = Path(\"models/gru_split1.pt\")\n","    history=[]\n","    for epoch in range(1, epochs+1):\n","        t0=time.time()\n","        tr_loss = train_one_epoch(model, train_loader, opt, scaler, device, use_amp)\n","        val = evaluate(model, val_loader, device)\n","        dt=time.time()-t0 # record training time for this epoch\n","        history.append({\"epoch\":epoch,\"train_mae\":tr_loss,\"val_mae\":val[\"mae\"],\"val_smape\":val[\"smape\"],\"seconds\":dt})\n","        print(f\"Epoch {epoch:02d}  train_mae={tr_loss:.5f}  val_mae={val['mae']:.5f}  val_sMAPE={val['smape']:.5f}  ({dt:.1f}s)\")\n","        # early stopping on val mae\n","        if val[\"mae\"] < best - 1e-6:\n","            best = val[\"mae\"]; best_metrics=val; best_epoch=epoch\n","            torch.save({\n","                \"model_state\": model.state_dict(),\n","                \"optimizer_state\": opt.state_dict(),\n","                \"epoch\": epoch,\n","                \"val\": val,\n","                \"config\": {\"lr\":lr,\"wd\":wd,\"epochs\":epochs,\"context_len\":train_loader.dataset.T,\"feats\":FEATS}\n","            }, ckpt_path)\n","        elif epoch - best_epoch >= patience:\n","            print(f\"Early stopping at epoch {epoch} (best {best:.5f} @ {best_epoch})\")\n","            break\n","    return history, best, best_epoch, ckpt_path\n","\n","history, best, best_epoch, ckpt_path = fit(model, train_loader, val_loader,\n","                                           epochs=10, lr=1e-3, wd=1e-5, patience=3, use_amp=True)\n","print(\"Best val_mae:\", best, \"at epoch\", best_epoch, \"| saved:\", ckpt_path.exists())"],"metadata":{"id":"zrdzjCORaENY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763321318551,"user_tz":360,"elapsed":10175,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"c1c878d8-0a40-4623-ebd5-fbf40496f87c"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-253071626.py:42: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler(enabled=use_amp and (device.type==\"cuda\"))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 01  train_mae=0.02655  val_mae=0.01088  val_sMAPE=1.15312  (0.5s)\n","Epoch 02  train_mae=0.01527  val_mae=0.00896  val_sMAPE=1.62450  (0.3s)\n","Epoch 03  train_mae=0.01282  val_mae=0.00781  val_sMAPE=1.46157  (0.3s)\n","Epoch 04  train_mae=0.01155  val_mae=0.00904  val_sMAPE=1.51492  (0.3s)\n","Epoch 05  train_mae=0.01127  val_mae=0.00890  val_sMAPE=1.52386  (0.3s)\n","Epoch 06  train_mae=0.01055  val_mae=0.01012  val_sMAPE=1.60047  (0.3s)\n","Early stopping at epoch 6 (best 0.00781 @ 3)\n","Best val_mae: 0.007809180611123641 at epoch 3 | saved: True\n"]}]},{"cell_type":"markdown","source":["# ⭐ Taining function\n","\n","---\n","\n","## Big picture\n","\n","```python\n","def train_one_epoch(model, loader, optimizer, scaler, device, use_amp=True):\n","    model.train(); total=0.0; n=0\n","    ...\n","    return total/max(n,1)\n","```\n","\n","* Runs **one full pass** over `loader` (all training batches).\n","* Uses **MAE loss**.\n","* Optionally uses **AMP** (automatic mixed precision) when on CUDA.\n","* Returns the **average loss over all samples**.\n","\n","---\n","\n","## 1. Start of epoch\n","\n","```python\n","model.train(); total=0.0; n=0\n","```\n","\n","* `model.train()` → tells PyTorch we are in **training mode**\n","  (enables dropout, uses batchnorm in training mode, etc.)\n","* `total` will accumulate **sum of losses × batch_size**\n","* `n` will accumulate **number of samples**\n","\n","So `total / n` at the end = mean MAE.\n","\n","---\n","\n","## 2. Loop over batches\n","\n","```python\n","for xb, yb, _ in loader:\n","```\n","\n","* `xb`: input features, shape `(B, T, F)`\n","* `yb`: targets, shape `(B,)`\n","* `_`: tickers (string labels), not used here\n","\n","---\n","\n","## 3. Move batch to device\n","\n","```python\n","xb = xb.to(device, non_blocking=True).float()\n","yb = yb.to(device, non_blocking=True).float()\n","```\n","\n","* Copies tensors to GPU (or CPU) — this is an **H2D copy** if on CUDA.\n","* `.float()` ensures `float32` (important before AMP casting).\n","* `non_blocking=True` can make H2D copies overlap with compute when using pinned memory. (see below for in-depth)\n","\n","---\n","\n","## 4. Zero the gradients\n","\n","```python\n","optimizer.zero_grad(set_to_none=True)\n","```\n","\n","* Clears old gradients before the backward pass.\n","* `set_to_none=True` is a small performance optimization vs setting grads to 0.\n","\n","---\n","\n","## 5. AMP vs non-AMP branch\n","\n","### With AMP (GPU only)\n","\n","```python\n","if use_amp and device.type==\"cuda\":\n","    with autocast(dtype=torch.float16):\n","        pred = model(xb)\n","        loss = mae_t(yb, pred)  # train with MAE (robust)\n","    scaler.scale(loss).backward() # scaler will be set as GradScaler()\n","    scaler.step(optimizer); scaler.update()\n","```\n","\n","* `autocast(dtype=torch.float16)`:\n","\n","  * Runs many ops in **fp16** internally where safe\n","  * Some critical ops stay in fp32 automatically\n","* `loss = mae_t(yb, pred)` → MAE loss on GPU.\n","* `scaler.scale(loss).backward()`:\n","\n","  * multiplies loss by a scaling factor to avoid underflow in fp16\n","  * then backprop on scaled loss\n","* `scaler.step(optimizer)`:\n","\n","  * steps optimizer with unscaled gradients (internally)\n","* `scaler.update()`:\n","\n","  * adjusts scaling factor for next iteration if needed\n","\n","This is the standard **GradScaler** pattern for mixed precision training.\n","\n","---\n","\n","### Without AMP (CPU or disabled)\n","\n","```python\n","else:\n","    pred = model(xb); loss = mae_t(yb, pred)\n","    loss.backward(); optimizer.step()\n","```\n","\n","Plain 32-bit training:\n","\n","* Forward → loss → backward → optimizer step.\n","\n","---\n","\n","## 6. Accumulate epoch loss\n","\n","```python\n","bs = xb.size(0)\n","total += loss.item() * bs\n","n += bs\n","```\n","\n","* `bs` = batch size for this batch (may be smaller on last batch).\n","* `loss.item()` → scalar Python float (average loss over this batch).\n","* Multiply by `bs` → total loss contribution from all samples in batch.\n","* `total` = sum of loss over **all samples**.\n","* `n` = total number of samples seen.\n","\n","---\n","\n","## 7. Return mean loss\n","\n","```python\n","return total/max(n,1)\n","```\n","\n","* If `n > 0`, returns `total / n` = **mean MAE over epoch**.\n","* `max(n,1)` avoids division by 0 if loader is empty.\n","\n","---\n","\n","## One-sentence summary\n","\n","> `train_one_epoch` iterates over training batches, optionally uses AMP on GPU, does forward + backward + optimizer step, and returns the mean MAE over all samples.\n","\n","---\n","\n","\n","\n","# ⭐ Quick check\n","In the AMP branch, why don’t we call `loss.backward()` directly, but instead:\n","\n","```python\n","scaler.scale(loss).backward()\n","```\n","\n","Pick the best reason:\n","\n","A. To make gradients bigger so the optimizer converges faster.\\\n","B. To avoid gradient underflow in fp16 by scaling the loss before backward.\\\n","C. To make training deterministic.\\\n","D. To change the learning rate each step.\\\n","\n","What do you think?\n","<details>\n","<summary> Click for answer </summary>\n"," **B is correct.**\n","\n","\n","\n","---\n","\n","#  Why we use `scaler.scale(loss).backward()` in AMP\n","\n","In mixed-precision (fp16) training:\n","\n","* fp16 has a **much smaller dynamic range**\n","* gradients can easily become **very tiny**\n","* tiny gradients may **underflow to zero** (become exactly 0), especially in deep networks\n","\n","This destroys learning.\n","\n","So PyTorch’s `GradScaler` solves it by:\n","\n","1. **Scaling the loss upward**, e.g. multiplying by a large number like 1024.\n","2. Calling `.backward()` on the *scaled* loss.\n","3. Unscaling the gradients before the optimizer step.\n","4. Detecting overflows and adjusting scaling automatically.\n","\n","Hence:\n","\n","```python\n","scaler.scale(loss).backward()\n","```\n","\n","prevents gradients from disappearing when using fp16.\n","\n","---\n","\n","# ⭐ Quick visual intuition\n","\n","Without scaling:\n","\n","```\n","true gradient = 0.00000018   # fits fine in FP32\n","fp16 → becomes 0.0           # underflow\n","```\n","\n","With scaling:\n","\n","```\n","scaled loss = loss * 1000\n","scaled gradient = 0.00018\n","fp16 → still representable\n","unscale later → correct gradient recovered\n","```\n","\n","\n","\n","</details>\n"],"metadata":{"id":"86OQpTs9KJ8V"}},{"cell_type":"markdown","source":["# ⭐ 1. What `non_blocking=True` actually means\n","\n","When you do:\n","\n","```python\n","xb = xb.to(device, non_blocking=True)\n","```\n","\n","you are telling PyTorch:\n","\n","> “If it is safe, please copy this tensor **asynchronously** from CPU → GPU.”\n","\n","Meaning:\n","\n","* The CPU **does not wait** for the copy to finish.\n","* The GPU can start the next operation sooner.\n","* Copies can overlap with GPU computation.\n","\n","This can speed things up — *but only when certain conditions are met*.\n","\n","---\n","\n","# ⭐ 2. Why it matters\n","\n","A normal H2D copy (`non_blocking=False`):\n","\n","* blocks the CPU\n","* must finish before forward pass begins\n","* wastes time\n","\n","With `non_blocking=True`:\n","\n","* CPU starts the copy\n","* **immediately continues** to execute Python code\n","* GPU *may* overlap compute and data copy\n","\n","It’s like saying:\n","\n","> “Start the transfer and don’t freeze everything while it finishes.”\n","\n","---\n","\n","# ⭐ 3. When it actually works (important!)\n","\n","`non_blocking=True` **only provides real overlap** if BOTH:\n","\n","###  Condition 1: DataLoader uses **pin_memory=True**\n","\n","Pinned memory = page-locked memory that CUDA can DMA (direct memory access, see below for in-depth)  directly:\n","\n","```python\n","DataLoader(..., pin_memory=True)\n","```\n","\n","Without pinned memory, CUDA cannot safely copy asynchronously.\n","\n","###  Condition 2: You're copying from **CPU → GPU**\n","\n","Async copy only applies to H2D transfers.\n","GPU→GPU transfers are already async.\n","\n","###  Condition 3: The code after the `.to(device)` does not force synchronization\n","\n","Some operations force a sync (e.g. printing a CUDA tensor), which removes the benefit.\n","\n","---\n","\n","# ⭐ 4. What “asynchronous” really means (simple mental model)\n","\n","## Blocking copy\n","\n","```\n","[CPU copies data] ----> wait ----> continue\n","```\n","\n","## Non-blocking copy\n","\n","```\n","[CPU starts copy] -> continue running Python\n","        \\---- [GPU copy happens in background]\n","```\n","\n","You get more overlap, like pipelining:\n","\n","* worker loads next batch\n","* GPU computes on current batch\n","  -.cuda streams copy next batch while GPU is training\n","\n","This speeds up training with medium/large models.\n","\n","---\n","\n","# ⭐ 5. Why deep learning frameworks use it\n","\n","Because the faster your GPU is, the more dangerous the bottleneck becomes:\n","\n","> The GPU often finishes compute **before** the next batch is ready.\n","\n","Non-blocking transfer + pinned memory + multiple workers\n","helps eliminate that bottleneck.\n","\n","---\n","\n","# ⭐ 6. Very short explanation\n","\n","\n","> **`non_blocking=True` allows H2D copies to run asynchronously, overlapping with GPU compute, but only if the input tensors come from pinned CPU memory.**\n","\n","---\n","\n","# ⭐ Quick check\n","\n","Which of the following is **required** for `non_blocking=True` to actually improve training speed?\n","\n","A. GPU runtime must be slow \\\n","B. Pinned memory must be enabled in DataLoader\\\n","C. Model must be FP16\\\n","D. Batch size must be at least 1024\\\n","\n","What do you think?\n","\n","<details>\n","<summary> Click for answer </summary>\n","B\n","</details>\n"],"metadata":{"id":"WzKE1QbejihG"}},{"cell_type":"markdown","source":["# ⭐ What is DMA?\n","\n","**DMA = Direct Memory Access.**\n","\n","It is a hardware feature that allows data to be moved:\n","\n","* **directly** between system RAM ↔ GPU memory\n","* **without involving the CPU** in the actual transfer\n","\n","This makes data movement:\n","\n","* **faster**\n","* **more efficient**\n","* **non-blocking** (CPU stays free and can continue doing other work)\n","\n","You can think of DMA like a “dedicated data mover” that copies memory independently.\n","\n","---\n","\n","# ⭐ Why do GPUs need DMA?\n","\n","When you do:\n","\n","```python\n","xb = xb.to(\"cuda\", non_blocking=True)\n","```\n","\n","this triggers a **host → device memory copy**.\n","\n","Two scenarios:\n","\n","---\n","\n","## ❌ Without DMA:\n","\n","* CPU must **read and push** each chunk of memory manually.\n","* CPU is **blocked** until the copy finishes.\n","* Slow and wasteful, especially when copying lots of batches.\n","\n","---\n","\n","##  With DMA:\n","\n","The CPU says:\n","\n","> “Hey DMA engine, copy this memory block from RAM into GPU memory —\n","> let me know when you’re done.”\n","\n","And then the CPU **immediately continues** doing other work\n","(e.g., preparing the next batch, running Python code).\n","\n","Meanwhile:\n","\n","* DMA hardware moves the data **in the background**\n","* GPU can start computation as soon as the data arrives\n","* CPU and GPU are both used efficiently\n","\n","---\n","\n","# ⭐ How PyTorch uses DMA\n","\n","DMA is used **automatically** when:\n","\n","### ✔ 1. `pin_memory=True` in the DataLoader\n","\n","Pinned (page-locked) memory is required for DMA.\n","\n","###  2. Input tensors come from that pinned memory\n","\n","DataLoader returns tensors allocated in pinned RAM.\n","\n","###  3. You call `.to(device, non_blocking=True)`\n","\n","This allows CUDA to perform a **DMA-based async copy**.\n","\n","---\n","\n","# ⭐ Why pinned memory is required for DMA?\n","\n","Because:\n","\n","* DMA can only read from **page-locked memory**\n","* Regular pageable RAM may be moved/swapped by OS\n","* GPU cannot DMA safely from pageable memory\n","\n","So:\n","\n","> **Pinned memory + non_blocking = true asynchronous DMA copy.**\n","\n","Without pinned memory:\n","\n","> `.to(device, non_blocking=True)` silently falls back to a blocking copy.\n","\n","---\n","\n","# ⭐ Visual intuition\n","\n","### Without DMA\n","\n","(Blocking, CPU copies bytes manually)\n","\n","```\n","CPU: [copying 10 MB] ---- waiting ---- waiting ---- continue\n","GPU: idle\n","```\n","\n","### With DMA\n","\n","(Asynchronous copy)\n","\n","```\n","CPU: Start DMA and continue → next batch, next Python steps\n","GPU: computing on previous batch\n","DMA engine: transferring new data in background\n","```\n","\n","Everything overlaps — much faster.\n","\n","---\n","\n","# ⭐ One-sentence definition\n","\n","> **DMA is hardware that copies memory directly between RAM and GPU without using the CPU, enabling fast asynchronous data transfer when used with pinned memory.**\n","\n","---\n","\n","# ⭐ Quick check (your turn)\n","\n","Why does PyTorch need *pinned memory* for async H2D copies?\n","\n","A. Because pinned memory never changes address, so DMA can safely read it\\\n","B. Because pinned memory is faster to allocate\\\n","C. Because pinned memory stores tensors on the GPU\\\n","D. Because pinning memory makes tensors fp16\n","\n","Which answer feels correct?\n","\n","<details>\n","<summary> Click for answer </summary>\n","A\n","</details>\n","\n"],"metadata":{"id":"DPvVRcjelI4z"}},{"cell_type":"markdown","source":["# ⭐ 1. `@torch.no_grad()`\n","\n","This decorator tells PyTorch:\n","\n","> \"During this entire function, **do not track gradients**.\" (do not build computation graph)\n","\n","Why?\n","\n","* Evaluation/inference doesn’t need gradients\n","* Saves memory\n","* Speeds up computation\n","* Ensures no accidental `.backward()` occurs\n","\n","So this whole function is **pure inference** — very efficient.\n","\n","---\n","\n","# ⭐ 2. `model.eval()`\n","\n","This switches the model to **evaluation mode**:\n","\n","* Disables dropout\n","* Turns batchnorm into \"use running stats\" mode\n","* Ensures deterministic, consistent behavior\n","\n","Training mode: `model.train()`\n","Eval mode: `model.eval()`\n","\n","These two change how some layers behave.\n","\n","\n","# ⭐ 4. Loop over batches\n","\n","```python\n","for xb, yb, _ in loader:\n","```\n","\n","Each batch:\n","\n","* `xb`: inputs `(B, T, F)`\n","* `yb`: targets `(B,)`\n","* `_`: ticker names (ignored)\n","\n","---\n","\n","# ⭐ 5. Move to device\n","\n","```python\n","xb = xb.to(device, non_blocking=True).float()\n","yb = yb.to(device, non_blocking=True).float()\n","```\n","\n","Same idea as in training:\n","\n","* H2D copy (async if pinned memory)\n","* Convert to float32\n","* No gradients tracked\n","\n","---\n","\n","\n","# ⭐ 7. Compute metrics per batch\n","\n","```python\n","bs = xb.size(0)\n","tot_mae   += mae_t(yb, pred).item() * bs\n","tot_smape += smape_t(yb, pred).item() * bs\n","n += bs\n","```\n","\n","Important points:\n","\n","* `mae_t(yb,pred).item()` → average MAE over the batch\n","* multiply by batch size → sum of MAE over samples\n","* accumulate over all validation batches\n","* keep track of total number of samples (`n`)\n","\n","At the end, we divide:\n","\n","```\n","tot_mae / n\n","```\n","\n","to get the **true** dataset-wide MAE.\n","\n","This is correct because not all batches may be the same size (especially last batch).\n","\n","\n","---\n","\n","\n","\n","# ⭐ Quick comprehension check (your turn)\n","\n","### ❓ Why do we wrap the entire function with `@torch.no_grad()`\n","\n","instead of just using it inside the loop?\n","\n","Pick the best reason:\n","\n","A. It makes the code look nicer.\\\n","B. It ensures *every* tensor operation inside this function is gradient-free, simplifying logic and ensuring correctness.\\\n","C. It changes the learning rate.\\\n","D. It forces the model to use FP16.\n","\n","What do you think?\n","\n","<details>\n","<summary> Click for answer </summary>\n","B\n","</details>\n","\n"],"metadata":{"id":"f8VcKKu0n5Yq"}},{"cell_type":"markdown","source":["# ⭐ Fit funciton\n","```python\n","def fit(model, train_loader, val_loader, epochs=12, lr=1e-3, wd=1e-5, patience=3, use_amp=True):\n","    opt = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n","    scaler = GradScaler(enabled=use_amp and (device.type==\"cuda\"))\n","    best = math.inf; best_metrics=None; best_epoch=-1\n","    ckpt_path = Path(\"models/gru_split1.pt\")\n","    history=[]\n","    for epoch in range(1, epochs+1):\n","        t0=time.time()\n","        tr_loss = train_one_epoch(model, train_loader, opt, scaler, device, use_amp)\n","        val = evaluate(model, val_loader, device)\n","        dt=time.time()-t0\n","        history.append({\"epoch\":epoch,\"train_mae\":tr_loss,\"val_mae\":val[\"mae\"],\"val_smape\":val[\"smape\"],\"seconds\":dt})\n","        print(f\"Epoch {epoch:02d}  train_mae={tr_loss:.5f}  val_mae={val['mae']:.5f}  val_sMAPE={val['smape']:.5f}  ({dt:.1f}s)\")\n","        # early stopping on val mae\n","        if val[\"mae\"] < best - 1e-6:\n","            best = val[\"mae\"]; best_metrics=val; best_epoch=epoch\n","            torch.save({\n","                \"model_state\": model.state_dict(),\n","                \"optimizer_state\": opt.state_dict(),\n","                \"epoch\": epoch,\n","                \"val\": val,\n","                \"config\": {\"lr\":lr,\"wd\":wd,\"epochs\":epochs,\"context_len\":train_loader.dataset.T,\"feats\":FEATS}\n","            }, ckpt_path)\n","        elif epoch - best_epoch >= patience:\n","            print(f\"Early stopping at epoch {epoch} (best {best:.5f} @ {best_epoch})\")\n","            break\n","    return history, best, best_epoch, ckpt_path\n","```\n","\n","---\n","\n","## 1. Optimizer + GradScaler\n","\n","```python\n","opt = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n","```\n","\n","* Uses **AdamW** (Adam with decoupled weight decay).\n","* `weight_decay=wd` is your L2-style regularization.\n","\n","```python\n","scaler = GradScaler(enabled=use_amp and (device.type==\"cuda\"))\n","```\n","\n","* `GradScaler` handles **loss scaling** for AMP.\n","* It’s only enabled if `use_amp=True` **and** you’re on CUDA.\n","* On CPU, it silently does nothing (no AMP).\n","\n","---\n","\n","## 2. Tracking the best model\n","\n","```python\n","best = math.inf\n","best_metrics = None\n","best_epoch = -1\n","ckpt_path = Path(\"models/gru_split1.pt\")\n","history = []\n","```\n","\n","* `best` = best validation MAE seen so far (initialized to ∞).\n","* `best_metrics` = dict with MAE/SMAPE at best epoch.\n","* `best_epoch` = epoch number of best model.\n","* `history` = list of per-epoch logs (for plotting later).\n","\n","---\n","\n","## 3. Epoch loop\n","\n","```python\n","for epoch in range(1, epochs+1):\n","    t0 = time.time()\n","    tr_loss = train_one_epoch(model, train_loader, opt, scaler, device, use_amp)\n","    val = evaluate(model, val_loader, device)\n","    dt = time.time() - t0\n","```\n","\n","Per epoch:\n","\n","* Call `train_one_epoch` → returns **train MAE**.\n","* Call `evaluate` → returns dict with `{\"mae\": ..., \"smape\": ...}`.\n","* `dt` = wall-clock time for this epoch.\n","\n","---\n","\n","## 4. Save to history + print\n","\n","```python\n","history.append({\n","    \"epoch\": epoch,\n","    \"train_mae\": tr_loss,\n","    \"val_mae\": val[\"mae\"],\n","    \"val_smape\": val[\"smape\"],\n","    \"seconds\": dt\n","})\n","print(f\"Epoch {epoch:02d}  train_mae={tr_loss:.5f}  val_mae={val['mae']:.5f}  val_sMAPE={val['smape']:.5f}  ({dt:.1f}s)\")\n","```\n","\n","* `history` keeps a full record you can later turn into a DataFrame or plot.\n","* The print gives a compact training log.\n","\n","---\n","\n","## 5. Early stopping + checkpoint saving\n","\n","```python\n","# early stopping on val mae\n","if val[\"mae\"] < best - 1e-6:\n","    best = val[\"mae\"]; best_metrics = val; best_epoch = epoch\n","    torch.save({\n","        \"model_state\": model.state_dict(),\n","        \"optimizer_state\": opt.state_dict(),\n","        \"epoch\": epoch,\n","        \"val\": val,\n","        \"config\": {\"lr\":lr, \"wd\":wd, \"epochs\":epochs,\n","                   \"context_len\":train_loader.dataset.T, \"feats\":FEATS}\n","    }, ckpt_path)\n","elif epoch - best_epoch >= patience:\n","    print(f\"Early stopping at epoch {epoch} (best {best:.5f} @ {best_epoch})\")\n","    break\n","```\n","\n","Two main ideas:\n","\n","###  5.1. Model improvement\n","\n","```python\n","if val[\"mae\"] < best - 1e-6:\n","```\n","\n","* If validation MAE **improves** by at least `1e-6`:\n","\n","  * Update `best`\n","  * Record `best_metrics`, `best_epoch`\n","  * Save a checkpoint with:\n","\n","    * `model_state` (weights)\n","    * `optimizer_state` (momentum, etc.)\n","    * `epoch` and `val` metrics\n","    * `config` (hyperparameters, feature config)\n","\n","This way, even if later epochs get worse, you have the **best model** saved.\n","\n","###  5.2. Patience-based early stopping\n","\n","```python\n","elif epoch - best_epoch >= patience:\n","    print(f\"Early stopping at epoch {epoch} (best {best:.5f} @ {best_epoch})\")\n","    break\n","```\n","\n","* If **no improvement** for `patience` epochs in a row:\n","\n","  * Stop training early\n","  * Avoid overfitting and wasted compute\n","\n","Example: `patience=3`\n","\n","* If best epoch is 4 and you’re now at epoch 7:\n","\n","  * 7 - 4 = 3 → stop at 7\n","\n","---\n","\n","## 6. Return values\n","\n","```python\n","return history, best, best_epoch, ckpt_path\n","```\n","\n","You get:\n","\n","* `history` → full training curve\n","* `best` → best validation MAE\n","* `best_epoch` → where that happened\n","* `ckpt_path` → where the best model was saved\n","\n","---\n","\n","## Tiny check for you\n","\n","The early stopping condition is:\n","\n","```python\n","elif epoch - best_epoch >= patience:\n","```\n","\n","### ❓ Why do we use `epoch - best_epoch` here?\n","\n","A. To stop after a fixed number of total epochs.\\\n","B. To stop after \"patience\" epochs **without any improvement** in validation MAE.\\\n","C. To slow down the optimizer.\\\n","D. To change the learning rate schedule.\n","\n","Which one feels correct?\n","<details>\n","<summary> Click for answer </summary>\n","B\n","</details>\n"],"metadata":{"id":"wSWfxKVCq-yE"}},{"cell_type":"markdown","source":["# 6) Evaluate best checkpoint & write a small report"],"metadata":{"id":"X7hjcC9daHnI"}},{"cell_type":"code","source":["# Reload best checkpoint and compute final validation metrics + save CSV\n","ckpt = torch.load(\"models/gru_split1.pt\", map_location=\"cpu\")\n","model.load_state_dict(ckpt[\"model_state\"])\n","model.to(device)\n","final = evaluate(model, val_loader, device)\n","import pandas as pd\n","rep = pd.DataFrame([{\n","    \"split\": 1,\n","    \"context_len\": train_loader.dataset.T,\n","    \"feats\": \",\".join(FEATS),\n","    \"val_mae\": final[\"mae\"],\n","    \"val_smape\": final[\"smape\"],\n","    \"best_epoch\": ckpt.get(\"epoch\", None),\n","    \"params_M\": round(sum(p.numel() for p in model.parameters())/1e6, 3)\n","}])\n","rep.to_csv(\"reports/gru_split1_metrics.csv\", index=False)\n","rep"],"metadata":{"id":"iULAOb25aJdv","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1763330699097,"user_tz":360,"elapsed":1485,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"d134c9c3-3801-4822-9a1d-c5851c908f00"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   split  context_len                                            feats  \\\n","0      1           16  log_return,lag1,lag2,lag3,zscore_20,roll_std_20   \n","\n","    val_mae  val_smape  best_epoch  params_M  \n","0  0.007809   1.461566           3     0.043  "],"text/html":["\n","  <div id=\"df-6802cc40-3be6-4853-b711-596fde169654\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>split</th>\n","      <th>context_len</th>\n","      <th>feats</th>\n","      <th>val_mae</th>\n","      <th>val_smape</th>\n","      <th>best_epoch</th>\n","      <th>params_M</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>16</td>\n","      <td>log_return,lag1,lag2,lag3,zscore_20,roll_std_20</td>\n","      <td>0.007809</td>\n","      <td>1.461566</td>\n","      <td>3</td>\n","      <td>0.043</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6802cc40-3be6-4853-b711-596fde169654')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-6802cc40-3be6-4853-b711-596fde169654 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-6802cc40-3be6-4853-b711-596fde169654');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_b3d4a6ae-9fa7-4711-9ee1-e310e880df53\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('rep')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_b3d4a6ae-9fa7-4711-9ee1-e310e880df53 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('rep');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"rep","summary":"{\n  \"name\": \"rep\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 16,\n        \"max\": 16,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feats\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"log_return,lag1,lag2,lag3,zscore_20,roll_std_20\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_mae\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.007809180611123641,\n        \"max\": 0.007809180611123641,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.007809180611123641\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_smape\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.4615657329559326,\n        \"max\": 1.4615657329559326,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.4615657329559326\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_epoch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 3,\n        \"max\": 3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"params_M\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.043,\n        \"max\": 0.043,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.043\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["# ⭐ 1. Load checkpoint from disk\n","\n","```python\n","ckpt = torch.load(\"models/gru_split1.pt\", map_location=\"cpu\")\n","```\n","\n","###  What it does:\n","\n","* Loads the dictionary you saved earlier inside `fit()`.\n","* `map_location=\"cpu\"` forces all tensors to CPU (safe and portable).\n","\n","  * You later move the model to GPU if needed.\n","* The checkpoint contains:\n","\n","  * `\"model_state\"` → a state_dict of weights\n","  * `\"optimizer_state\"` → optimizer buffers (you may or may not use this)\n","  * `\"epoch\"` → best epoch\n","  * `\"val\"` → metrics at that epoch\n","  * `\"config\"` → hyperparameters, feature config, etc.\n","\n","---\n","\n","# ⭐ 2. Load the model weights\n","\n","```python\n","model.load_state_dict(ckpt[\"model_state\"])\n","model.to(device)\n","```\n","\n","###  What it does:\n","\n","1. Loads the saved weights into your model architecture.\n","2. Moves the model to the appropriate device (`cuda` or `cpu`).\n","\n","⚠️ Important:\n","The **model architecture must match** the architecture used during training\n","(same hidden size, layers, features, etc.), or this will error.\n","\n","---\n","\n","# ⭐ 3. Evaluate the model\n","\n","```python\n","final = evaluate(model, val_loader, device)\n","```\n","\n","This computes:\n","\n","* final MAE\n","* final SMAPE\n","* using the **whole validation set**\n","* without gradients (thanks to `@torch.no_grad()` inside `evaluate`)\n","\n","This is your unbiased, final validation score.\n","\n","---\n","\n","# ⭐ 4. Create a Pandas report row\n","\n","```python\n","import pandas as pd\n","rep = pd.DataFrame([{\n","    \"split\": 1,\n","    \"context_len\": train_loader.dataset.T,\n","    \"feats\": \",\".join(FEATS),\n","    \"val_mae\": final[\"mae\"],\n","    \"val_smape\": final[\"smape\"],\n","    \"best_epoch\": ckpt.get(\"epoch\", None),\n","    \"params_M\": round(sum(p.numel() for p in model.parameters())/1e6, 3)\n","}])\n","```\n","\n","### ✔ Column-by-column explanation:\n","\n","* `\"split\": 1`\n","  If you're running multiple data splits, this tracks which one.\n","\n","* `\"context_len\": train_loader.dataset.T`\n","  The window size used (e.g., 32 or 64).\n","\n","* `\"feats\": \",\".join(FEATS)`\n","  Stores the feature list as a single string.\n","\n","* `\"val_mae\": final[\"mae\"]`\n","  The final MAE (after restoring best checkpoint).\n","\n","* `\"val_smape\": final[\"smape\"]`\n","  SMAPE metric.\n","\n","* `\"best_epoch\": ckpt.get(\"epoch\", None)`\n","  Epoch number where early stopping found the best model.\n","\n","* `\"params_M\": ...`\n","  Number of parameters in megabytes:\n","\n","  ```python\n","  sum(p.numel() for p in model.parameters()) / 1e6\n","  ```\n","\n","This is a clean, reproducible experiment summary row.\n","\n","---\n","\n","# ⭐ 5. Save to CSV\n","\n","```python\n","rep.to_csv(\"reports/gru_split1_metrics.csv\", index=False)\n","rep\n","```\n","\n","* Saves the metrics for this run into a report file.\n","* Returning `rep` prints the DataFrame nicely in the notebook.\n","\n","This is nice because:\n","\n","* All results across models, features, window lengths, etc., become easy to compare.\n","* You can aggregate multiple split results later into one summary.\n","\n","---\n","\n","# ⭐ One-Sentence Summary\n","\n","> You load the best checkpoint, restore the model, evaluate it, record its final performance, and save a clean experiment summary.\n","\n","---\n","\n","# ⭐ Quick comprehension check (your turn)\n","\n","Why do we use `map_location=\"cpu\"` when loading the checkpoint,\n","even if we later want to evaluate on the GPU?\n","\n","Pick the best answer:\n","\n","A. CPU loading is faster.\\\n","B. The checkpoint always stores CPU weights.\\\n","C. It makes loading safe even if the checkpoint was saved on a different machine or GPU configuration.\\\n","D. It automatically improves MAE.\n","\n","Which one feels correct?\n","\n","<details>\n","<summary> Click for answer </summary>\n","C\n","</details>\n"],"metadata":{"id":"VcX-iC8CtMN_"}},{"cell_type":"markdown","source":["> **Time check:** With \\~5–8 tickers, `T=64`, and 10 epochs, this should finish in a couple of minutes on Colab CPU; faster on GPU.\n","\n","------------------------------------------------------------------------\n","\n","## Wrap‑up\n","\n","-   **WindowedDataset** emits causal windows `(≤ t)` and targets `r_1d[t]` (i.e., `t+1` return).\n","-   Use **train‑fit scaler** and **reuse it** on validation to avoid leakage.\n","-   Keep the training loop **simple**: MAE training objective, AMP on CUDA, **early stopping** on validation MAE, and save a **checkpoint**.\n","-   Produce a **CSV** with validation metrics to track progress and compare future models.\n","\n","------------------------------------------------------------------------\n","\n","## Homework (due before Session 20)\n","\n","**Goal:** Train a stronger **sequence baseline** (choose **LSTM** or **TCN**) on a subset (5–10 tickers). Log metrics and push your checkpoint + report.\n","\n","### Part A — Script `scripts/train_seq.py` (LSTM or TCN)\n"],"metadata":{"id":"w0QgNVxXaU9D"}},{"cell_type":"code","source":["#!/usr/bin/env python\n","from __future__ import annotations\n","import argparse, json, math\n","from pathlib import Path\n","import numpy as np, pandas as pd, torch, torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from torch.cuda.amp import autocast, GradScaler\n","\n","# --- (reuse minimal dataset/scaler from class; compact copy here) ---\n","class FeatureScaler:\n","    def __init__(self): self.mean_=None; self.std_=None\n","    def fit(self, X): self.mean_=X.mean(0); self.std_=X.std(0)+1e-8; return self\n","    def transform(self, X): return (X-self.mean_)/self.std_\n","    def state_dict(self): return {\"mean\": self.mean_.tolist(), \"std\": self.std_.tolist()}\n","    def load_state_dict(self, d): import numpy as np; self.mean_=np.array(d[\"mean\"]); self.std_=np.array(d[\"std\"])\n","class WindowedDataset(Dataset):\n","    def __init__(self, df, feats, T=64, scaler=None):\n","        self.feats=feats; self.T=T; self.idx=[]; self.g={}\n","        for tkr,g in df.groupby(\"ticker\"):\n","            g=g.sort_values(\"date\").reset_index(drop=True)\n","            X=g[feats].to_numpy(\"float32\"); y=g[\"r_1d\"].to_numpy(\"float32\")\n","            for end in range(T-1,len(g)):\n","                if np.isfinite(y[end]): self.idx.append((tkr,end))\n","            self.g[tkr]={\"X\":X,\"y\":y}\n","        self.scaler = scaler or FeatureScaler().fit(np.concatenate([self.g[t][\"X\"] for t in self.g],0))\n","    def __len__(self): return len(self.idx)\n","    def __getitem__(self,i):\n","        tkr,end=self.idx[i]; g=self.g[tkr]\n","        X=g[\"X\"][end-self.T+1:end+1]; X=self.scaler.transform(X)\n","        y=g[\"y\"][end]\n","        return torch.from_numpy(X), torch.tensor(y)\n","\n","def make_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n","    u=np.array(sorted(pd.to_datetime(pd.Series(dates).unique()))); i=train_min-1; out=[]\n","    while True:\n","        if i>=len(u): break\n","        a,b=u[0],u[i]; vs=i+embargo+1; ve=vs+val_size-1\n","        if ve>=len(u): break\n","        out.append((a,b,u[vs],u[ve])); i+=step\n","    return out\n","\n","# --- Models ---\n","class LSTMReg(nn.Module):\n","    def __init__(self, in_f, hidden=64, layers=2, dropout=0.1):\n","        super().__init__()\n","        self.lstm = nn.LSTM(in_f, hidden, num_layers=layers, batch_first=True, dropout=dropout if layers>1 else 0.)\n","        self.head = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Dropout(dropout), nn.Linear(hidden,1))\n","    def forward(self, x):\n","        out,_ = self.lstm(x)\n","        h = out[:,-1,:]\n","        return self.head(h).squeeze(-1)\n","\n","class TCNBlock(nn.Module):\n","    def __init__(self, in_c, out_c, k=3, d=1, dropout=0.1):\n","        super().__init__()\n","        pad = (k-1)*d\n","        self.net = nn.Sequential(\n","            nn.Conv1d(in_c, out_c, k, padding=pad, dilation=d),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Conv1d(out_c, out_c, k, padding=pad, dilation=d),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","        )\n","        self.down = nn.Conv1d(in_c, out_c, 1) if in_c!=out_c else nn.Identity()\n","    def forward(self, x):        # x: (B, F, T)\n","        y = self.net(x)\n","        # Causal crop to ensure output aligns with last time step\n","        crop = y.shape[-1]-x.shape[-1]\n","        if crop>0: y = y[..., :-crop]\n","        return y + self.down(x)\n","\n","class TCNReg(nn.Module):\n","    def __init__(self, in_f, ch=64, blocks=3, k=3, dropout=0.1):\n","        super().__init__()\n","        layers=[]; c=in_f\n","        for b in range(blocks):\n","            layers.append(TCNBlock(c, ch, k=k, d=2**b, dropout=dropout)); c=ch\n","        self.tcn = nn.Sequential(*layers)\n","        self.head = nn.Sequential(nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Linear(ch,1))\n","    def forward(self, x):\n","        # x: (B,T,F) -> (B,F,T) for Conv1d\n","        x = x.transpose(1,2)\n","        y = self.tcn(x)              # (B, C, T)\n","        return self.head(y).squeeze(-1)\n","\n","def mae_t(y,yhat): return torch.mean(torch.abs(y - yhat))\n","def smape_t(y,yhat,eps=1e-8): return torch.mean(2*torch.abs(y-yhat)/(torch.abs(y)+torch.abs(yhat)+eps))\n","\n","def main():\n","    ap=argparse.ArgumentParser()\n","    ap.add_argument(\"--features\", default=\"data/processed/features_v1.parquet\")\n","    ap.add_argument(\"--context\", type=int, default=64)\n","    ap.add_argument(\"--model\", choices=[\"lstm\",\"tcn\"], default=\"lstm\")\n","    ap.add_argument(\"--epochs\", type=int, default=12)\n","    ap.add_argument(\"--batch\", type=int, default=256)\n","    ap.add_argument(\"--lr\", type=float, default=1e-3)\n","    ap.add_argument(\"--patience\", type=int, default=3)\n","    ap.add_argument(\"--tickers\", type=int, default=8)\n","    args=ap.parse_args()\n","\n","    df = pd.read_parquet(\"data/processed/features_v1_static.parquet\") if Path(\"data/processed/features_v1_static.parquet\").exists() else pd.read_parquet(args.features)\n","    df = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","    cand = [\"log_return\",\"lag1\",\"lag2\",\"lag3\",\"zscore_20\",\"roll_std_20\"]\n","    feats = [c for c in cand if c in df.columns]\n","    assert \"r_1d\" in df.columns\n","    # subset tickers\n","    keep = df[\"ticker\"].astype(str).unique().tolist()[:args.tickers]\n","    df = df[df[\"ticker\"].astype(str).isin(keep)].copy()\n","\n","    splits = make_splits(df[\"date\"])\n","    a,b,c,d = splits[0]\n","    tr = df[(df[\"date\"]>=a)&(df[\"date\"]<=b)]\n","    va = df[(df[\"date\"]>=c)&(df[\"date\"]<=d)]\n","\n","    train_ds = WindowedDataset(tr, feats, T=args.context, scaler=None)\n","    val_ds   = WindowedDataset(va, feats, T=args.context, scaler=train_ds.scaler)\n","\n","    pin = torch.cuda.is_available()\n","    g = torch.Generator(); g.manual_seed(42)\n","    def _seed_worker(_): import numpy as np, random, torch; ws=torch.initial_seed()%2**32; np.random.seed(ws); random.seed(ws)\n","    train_ld = DataLoader(train_ds, batch_size=args.batch, shuffle=True, drop_last=True,\n","                          num_workers=2, pin_memory=pin, worker_init_fn=_seed_worker, generator=g)\n","    val_ld   = DataLoader(val_ds, batch_size=args.batch, shuffle=False, drop_last=False,\n","                          num_workers=2, pin_memory=pin, worker_init_fn=_seed_worker)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    net = (LSTMReg(len(feats)) if args.model==\"lstm\" else TCNReg(len(feats))).to(device)\n","    opt = torch.optim.AdamW(net.parameters(), lr=args.lr, weight_decay=1e-5)\n","    scaler = GradScaler(enabled=(device.type==\"cuda\"))\n","    best = 1e9; best_epoch=0\n","    ckpt = Path(f\"models/{args.model}_split1.pt\")\n","\n","    for epoch in range(1, args.epochs+1):\n","        net.train(); tmae=0; n=0\n","        for xb,yb in train_ld:\n","            xb=xb.to(device).float(); yb=yb.to(device).float()\n","            opt.zero_grad(set_to_none=True)\n","            with autocast(enabled=(device.type==\"cuda\"), dtype=torch.float16):\n","                yhat = net(xb)\n","                loss = mae_t(yb, yhat)\n","            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n","            bs=xb.size(0); tmae += loss.item()*bs; n+=bs\n","        tr_mae=tmae/n\n","        # val\n","        net.eval(); vmae=vsm=0; n=0\n","        with torch.no_grad():\n","            for xb,yb in val_ld:\n","                xb=xb.to(device).float(); yb=yb.to(device).float()\n","                yhat = net(xb)\n","                bs=xb.size(0); vmae += mae_t(yb,yhat).item()*bs; vsm += smape_t(yb,yhat).item()*bs; n+=bs\n","        vmae/=n; vsm/=n\n","        print(f\"Epoch {epoch:02d}  tr_mae={tr_mae:.5f}  val_mae={vmae:.5f}  val_sMAPE={vsm:.5f}\")\n","        if vmae < best-1e-6:\n","            best=vmae; best_epoch=epoch\n","            torch.save({\"model\": net.state_dict(), \"epoch\": epoch, \"feats\": feats, \"context\": args.context}, ckpt)\n","        elif epoch - best_epoch >= args.patience:\n","            print(\"Early stopping.\")\n","            break\n","\n","    Path(\"reports\").mkdir(exist_ok=True)\n","    pd.DataFrame([{\"model\":args.model,\"context\":args.context,\"val_mae\":best,\"best_epoch\":best_epoch,\"feats\":\",\".join(feats)}]).to_csv(\n","        f\"reports/{args.model}_split1_metrics.csv\", index=False)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"70RmJg8VaYJb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%bash\n","chmod +x scripts/train_seq.py\n","python scripts/train_seq.py --model lstm --context 64 --tickers 8 --epochs 12"],"metadata":{"id":"okymVLVFabcI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Part B — Add a quick **Makefile** target and a tiny test\n","\n","**Append to `Makefile`:**\n","\n","``` make\n",".PHONY: train-lstm\n","train-lstm: ## Train LSTM baseline on split 1 (subset of tickers)\n","\\tpython scripts/train_seq.py --model lstm --context 64 --tickers 8 --epochs 12\n","```\n","\n","\n"],"metadata":{"id":"q784JCDDak5I"}},{"cell_type":"markdown","source":["**Basic shape test for dataset windows:**"],"metadata":{"id":"9-71D7DTamnL"}},{"cell_type":"code","source":["# tests/test_windowed_dataset.py\n","import pandas as pd, numpy as np, os\n","def test_window_shapes():\n","    import scripts.train_seq as T\n","    df = pd.read_parquet(\"data/processed/features_v1.parquet\").sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","    feats = [c for c in [\"log_return\",\"lag1\",\"lag2\",\"lag3\"] if c in df.columns]\n","    splits = T.make_splits(df[\"date\"])\n","    a,b,c,d = splits[0]\n","    ds = T.WindowedDataset(df[(df[\"date\"]>=a)&(df[\"date\"]<=b)], feats, T=32, scaler=None)\n","    X,y = ds[0]\n","    assert X.shape == (32, len(feats))\n","    assert np.isfinite(y.item())"],"metadata":{"id":"wxCg0_8Ua41Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%bash\n","pytest -q -k windowed_dataset"],"metadata":{"id":"UpZlZZuxauKJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Part C — Report\n","\n","Add to your Quarto report (e.g., `reports/eda.qmd`):\n","\n","````markdown \\## PyTorch Baselines\n","\n","```{python}\n","import pandas as pd\n","print(pd.read_csv(\"reports/gru_split1_metrics.csv\"))\n","try:\n","    print(pd.read_csv(\"reports/lstm_split1_metrics.csv\"))\n","except Exception as e:\n","    print(\"lstm metrics not found yet\")\n","\n","```\n","````"],"metadata":{"id":"OhRD3thqbFGi"}}]}