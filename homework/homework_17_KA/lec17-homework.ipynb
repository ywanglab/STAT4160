{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# from google.colab import drive\n","# drive.flush_and_unmount()           # ignore errors if already unmounted\n","\n","#If cannot remount, simply delete the mounted drive and then remount\n","# rm -rf /content/drive\n"],"metadata":{"id":"H-1K7L9NJHMB","executionInfo":{"status":"ok","timestamp":1763255387991,"user_tz":360,"elapsed":1069,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"3b0cacda","outputId":"f8061cab-040c-4949-e227-6f7f8eb41278","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763255389608,"user_tz":360,"elapsed":1615,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Colab cell\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"01f28b78","executionInfo":{"status":"ok","timestamp":1763255389784,"user_tz":360,"elapsed":173,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}}},"outputs":[],"source":["# Adjust these two for YOUR repo\n","REPO_OWNER = \"kadkins3880\"\n","REPO_NAME  = \"STAT4160\"   # e.g., unified-stocks-team1\n","BASE_DIR   = \"/content/drive/MyDrive/dspt25\"\n","CLONE_DIR  = f\"{BASE_DIR}/{REPO_NAME}\"\n","REPO_URL   = f\"https://github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n","\n","# if on my office computer\n","\n","# REPO_NAME  = \"lectureNotes\"   # e.g., on my office computer\n","# BASE_DIR = r\"E:\\OneDrive - Auburn University Montgomery\\teaching\\AUM\\STAT 4160 Productivity Tools\" # on my office computer\n","# CLONE_DIR  = f\"{BASE_DIR}\\{REPO_NAME}\"\n","\n","import os, pathlib\n","pathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"36277e0a","outputId":"760d1035-e988-4721-b051-7265c923421c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763255390045,"user_tz":360,"elapsed":258,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Working dir: /content/drive/MyDrive/dspt25/STAT4160\n"]}],"source":["import os, subprocess, shutil, pathlib\n","\n","if not pathlib.Path(CLONE_DIR).exists():\n","    !git clone {REPO_URL} {CLONE_DIR}\n","else:\n","    # If the folder exists, just ensure it's a git repo and pull latest\n","    os.chdir(CLONE_DIR)\n","    # !git status\n","    # !git pull --rebase # !git pull --ff-only\n","os.chdir(CLONE_DIR)\n","print(\"Working dir:\", os.getcwd())"]},{"cell_type":"markdown","source":["## Session 17 ‚Äî Feature Timing, Biases & Leakage\n","\n","### Learning goals\n","\n","By the end of class, students can:\n","\n","1.  Explain and **avoid look‚Äëahead** and **survivorship** biases.\n","2.  Freeze and use a **static ticker universe** chosen from the **train window** (not the whole history).\n","3.  Define labels correctly (e.g., **t+1** and **t+5**) and verify them with tests.\n","4.  Add **leakage tests** that recompute trusted features and fail on any future‚Äëpeek.\n","\n","------------------------------------------------------------------------\n","\n","## Agenda\n","\n","-   what leakage looks like; examples; how it sneaks in\n","\n","-   survivorship bias (today‚Äôs constituents ‚â† past reality); freezing a universe\n","\n","-    label definitions (t+1, multi‚Äëstep) and alignment rules\n","\n","-   **In‚Äëclass lab**:\n","\n","    1.  Freeze a static universe from the first split‚Äôs train window\n","    2.  Add leakage tests that recompute known‚Äëgood features\n","    3.  Add multi‚Äëstep labels (e.g., t+5) with tests\n","\n","-   Wrap‚Äëup & homework brief\n","\n","------------------------------------------------------------------------\n","\n","### What is data leakage?\n","\n","-   **Look‚Äëahead leakage:** using any info from *t+1* or later to compute features at *t* or to scale/normalize train and validation together.\n","-   **Common culprits:** `shift(-1)` in features, global scaling fit on full data, forward‚Äëfill across split boundaries, using today‚Äôs close to predict today‚Äôs close.\n","\n","### Survivorship bias\n","\n","-   Using **today‚Äôs index membership** to pick tickers for the past ‚áí drops delisted/removed names ‚áí **optimistically biased** results.\n","-   **Cure:** freeze a **static universe** from the **training window** (e.g., all tickers with ‚â• 252 observations by the end of the first train window). Save it and **filter by it** for all future experiments.\n","\n","### Label definitions (be explicit)\n","\n","-   **t+1 log return**: `r_1d = log_return.shift(-1)` per ticker (your Session‚Äë9 label).\n","-   **t+5 log return** (multi‚Äëstep): `r_5d = log_return.shift(-1) + ‚Ä¶ + log_return.shift(-5)` per ticker.\n","-   Rules: labels come from **future**; features come from **‚â§ t**. Splits with **embargo** reduce adjacency leakage.\n","\n","------------------------------------------------------------------------\n","\n","## In‚Äëclass lab (Colab‚Äëfriendly)\n","\n","> Run each block as its own cell. Update `REPO_NAME` as needed.\n","\n","### 0) Setup & load data (with fallbacks)\n"],"metadata":{"id":"pfC1a7q_vg83"}},{"cell_type":"code","source":["import os, pathlib, numpy as np, pandas as pd\n","from pathlib import Path\n","\n","for p in [\"data/raw\",\"data/processed\",\"data/static\",\"reports\",\"scripts\",\"tests\"]:\n","    Path(p).mkdir(parents=True, exist_ok=True)\n","print(\"Working dir:\", os.getcwd())\n","\n","# Load returns or synthesize a small fallback\n","rpath = Path(\"data/processed/returns.parquet\")\n","if rpath.exists():\n","    returns = pd.read_parquet(rpath)\n","else:\n","    rng = np.random.default_rng(0)\n","    dates = pd.bdate_range(\"2022-01-03\", periods=360)\n","    rows=[]\n","    for t in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\",\"TSLA\",\"META\",\"NFLX\"]:\n","        eps = rng.normal(0,0.012,size=len(dates)).astype(\"float32\")\n","        adj = 100*np.exp(np.cumsum(eps))\n","        df = pd.DataFrame({\n","            \"date\": dates, \"ticker\": t,\n","            \"adj_close\": adj.astype(\"float32\"),\n","            \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n","        })\n","        df[\"r_1d\"] = df[\"log_return\"].shift(-1) # negative shift (backward) as the target\n","        df[\"weekday\"] = df[\"date\"].dt.weekday.astype(\"int8\")\n","        df[\"month\"]   = df[\"date\"].dt.month.astype(\"int8\")\n","        rows.append(df)\n","    returns = pd.concat(rows, ignore_index=True).dropna().reset_index(drop=True)\n","    returns[\"ticker\"] = returns[\"ticker\"].astype(\"category\")\n","    returns.to_parquet(rpath, index=False)\n","\n","# Load features_v1 or construct minimal lags for tests\n","fpath = Path(\"data/processed/features_v1.parquet\")\n","if fpath.exists():\n","    feats = pd.read_parquet(fpath).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","else:\n","    feats = returns.sort_values([\"ticker\",\"date\"]).copy()\n","    for k in [1,2,3]:\n","        feats[f\"lag{k}\"] = feats.groupby(\"ticker\")[\"log_return\"].shift(k) # postiive shift (forward) as the \"lags\"\n","    feats[\"roll_mean_20\"] = feats.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).mean().reset_index(level=0, drop=True) # to see how groupby affect index, see below.\n","    feats[\"roll_std_20\"]  = feats.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)\n","    feats[\"zscore_20\"]    = (feats[\"log_return\"] - feats[\"roll_mean_20\"]) / (feats[\"roll_std_20\"] + 1e-8)\n","    feats = feats.dropna().reset_index(drop=True)\n","\n","# Harmonize types\n","returns[\"date\"] = pd.to_datetime(returns[\"date\"])\n","feats[\"date\"]   = pd.to_datetime(feats[\"date\"])\n","returns[\"ticker\"] = returns[\"ticker\"].astype(\"category\") #category type is faster\n","feats[\"ticker\"]   = feats[\"ticker\"].astype(\"category\")\n","returns = returns.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","feats   = feats.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","returns.head(3), feats.head(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0EdxQpZtxb0Z","executionInfo":{"status":"ok","timestamp":1763255392574,"user_tz":360,"elapsed":2528,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"9ae813ba-8128-4ce5-90ba-adbfd400287d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Working dir: /content/drive/MyDrive/dspt25/STAT4160\n"]},{"output_type":"execute_result","data":{"text/plain":["(        date ticker  log_return      r_1d  weekday  month\n"," 0 2020-01-01   AAPL         NaN  0.002987        2      1\n"," 1 2020-01-02   AAPL    0.002987 -0.002741        3      1\n"," 2 2020-01-03   AAPL   -0.002741 -0.008906        4      1,\n","         date ticker  log_return      r_1d  weekday  month      lag1      lag2  \\\n"," 0 2020-01-29   AAPL   -0.018417 -0.002351        2      1 -0.012895 -0.019012   \n"," 1 2020-01-30   AAPL   -0.002351 -0.012675        3      1 -0.018417 -0.012895   \n"," 2 2020-01-31   AAPL   -0.012675  0.002713        4      1 -0.002351 -0.018417   \n"," \n","        lag3  roll_mean_20  roll_std_20  zscore_20  ewm_mean_20  ewm_std_20  \\\n"," 0 -0.004576     -0.004086     0.008476  -1.690830    -0.005252    0.009304   \n"," 1 -0.019012     -0.004353     0.008324   0.240455    -0.004976    0.008875   \n"," 2 -0.012895     -0.004849     0.008517  -0.918756    -0.005709    0.008745   \n"," \n","    exp_mean   exp_std  adj_close   volume  \n"," 0 -0.004086  0.008476  92.154846  1598707  \n"," 1 -0.004003  0.008270  91.938454  2992900  \n"," 2 -0.004397  0.008280  90.780533   634335  )"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["\n","### What happens to the index after `groupby`\n","\n","1. **`groupby(\"ticker\")[\"log_return\"]`**\n","   This creates groups by ticker but doesn‚Äôt change the DataFrame yet‚Äîit‚Äôs a grouped Series.\n","\n","2. **`.rolling(20, min_periods=20).mean()`**\n","   The rolling mean is computed **within each ticker**. The result is a **Series whose index is a MultiIndex** with two levels:\n","\n","   * **Level 0** = the group key(s) (here, `'ticker'`)\n","   * **Level 1** = the *original index* of `feats` (e.g., your DateTimeIndex or RangeIndex)\n","\n","   So at this point the result is indexed like `(ticker, original_index)`.\n","\n","3. **`.reset_index(level=0, drop=True)`**\n","   This removes (drops) **level 0** of the MultiIndex (i.e., the `'ticker'` level), leaving you with just the **original index**.\n","   Dropping that level ensures the resulting Series lines up 1:1 with `feats`‚Äôs index, so you can safely assign it as a new column.\n","\n","> **‚ÄúWhat is level 0?‚Äù**\n","> In a pandas **MultiIndex**, *level 0* is simply the **outermost** index level. In this case, it‚Äôs the `'ticker'` level that was introduced by `groupby`.\n","\n","---\n","\n","## Minimal example (with tiny window to make it easy to see)\n","\n","```python\n","import pandas as pd\n","\n","# Example data: 2 tickers over 6 days\n","feats = pd.DataFrame(\n","    {\n","        \"ticker\": [\"A\",\"A\",\"A\",\"B\",\"B\",\"B\"],\n","        \"log_return\": [0.01, -0.02, 0.03, 0.10, 0.05, -0.02],\n","    },\n","    index=pd.date_range(\"2024-01-01\", periods=6, freq=\"D\"),\n",")\n","feats.index.name = \"date\"\n","```\n","\n","### Rolling mean within each ticker (window=3 here just for display)\n","\n","```python\n","s = feats.groupby(\"ticker\")[\"log_return\"].rolling(3, min_periods=1).mean()\n","```\n","\n","**Index after groupby+rolling:** (note the two levels: `ticker` then `date`)\n","\n","```\n","ticker  date\n","A       2024-01-01    0.010000\n","        2024-01-02   -0.005000\n","        2024-01-03    0.006667\n","B       2024-01-04    0.100000\n","        2024-01-05    0.075000\n","        2024-01-06    0.043333\n","```\n","\n","Here, **level 0** is `'ticker'`. **Level 1** is the original index (`'date'`).\n","\n","### Drop level 0 so it aligns with `feats`‚Äô index\n","\n","```python\n","s_aligned = s.reset_index(level=0, drop=True)\n","```\n","\n","Now the index is **just the original** (`date`):\n","\n","```\n","date\n","2024-01-01    0.010000\n","2024-01-02   -0.005000\n","2024-01-03    0.006667\n","2024-01-04    0.100000\n","2024-01-05    0.075000\n","2024-01-06    0.043333\n","```\n","\n","### Assign back\n","\n","```python\n","feats[\"roll_mean_3\"] = s_aligned\n","```\n","\n","Result:\n","\n","```\n","             ticker  log_return  roll_mean_3\n","date\n","2024-01-01      A        0.01       0.010000\n","2024-01-02      A       -0.02      -0.005000\n","2024-01-03      A        0.03       0.006667\n","2024-01-04      B        0.10       0.100000\n","2024-01-05      B        0.05       0.075000\n","2024-01-06      B       -0.02       0.043333\n","```\n","\n","---\n","\n","### Two small gotchas\n","\n","* **Order vs. labels:** after `groupby(...).rolling(...).mean()`, the MultiIndex groups values by ticker. After you `reset_index(level=0, drop=True)`, the Series‚Äôs **index labels** match `feats`‚Äô index, but the **order** may be grouped by ticker. That‚Äôs okay‚Äîpandas aligns on labels when assigning. If you *also* want the original row order in that Series, you can do:\n","\n","  ```python\n","  s_aligned = s.reset_index(level=0, drop=True).sort_index()\n","  ```\n","\n","* **A simpler alternative:** you can avoid the MultiIndex entirely by using `transform`, which preserves the original index:\n","\n","  ```python\n","  feats[\"roll_mean_20\"] = (\n","      feats.groupby(\"ticker\")[\"log_return\"]\n","           .transform(lambda s: s.rolling(20, min_periods=20).mean())\n","  )\n","  ```\n","\n","\n"],"metadata":{"id":"rYAQ1S6l3uns"}},{"cell_type":"markdown","source":["# 1) Freeze a **static universe** from the **first split‚Äôs train window**"],"metadata":{"id":"a_XJ2vh90yOT"}},{"cell_type":"code","source":["import numpy as np, pandas as pd\n","\n","def make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n","    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n","    i = train_min - 1; splits=[]\n","    while True:\n","        if i >= len(u): break\n","        a,b = u[0], u[i]  # a is fixed to u[0], the start date\n","        vs = i + embargo + 1\n","        ve = vs + val_size - 1\n","        if ve >= len(u): break\n","        splits.append((a,b,u[vs],u[ve]))\n","        i += step  # step is the size increased for a expanding window\n","    return splits\n","\n","TRAIN_MIN =80 # adjust this constant\n","splits = make_rolling_origin_splits(returns[\"date\"], train_min=TRAIN_MIN, val_size=21, step=21, embargo=5)\n","assert len(splits) >= 1, \"Not enough history for a first split.\"\n","a,b,c,d = splits[0]\n","print(\"First train window:\", a.date(), \"‚Üí\", b.date())\n","\n","# Eligible = tickers with at least train_min rows by train_end (b)\n","train_slice = returns[(returns[\"date\"]>=a) & (returns[\"date\"]<=b)]\n","counts = train_slice.groupby(\"ticker\").size()\n","eligible = counts[counts >= TRAIN_MIN].index.sort_values()  # extract just index (ticker name) and sort, see more explanaiton below\n","universe = pd.DataFrame({\"ticker\": eligible})\n","univ_name = f\"data/static/universe_{b.date()}.csv\"\n","universe.to_csv(univ_name, index=False)\n","print(\"Saved static universe:\", univ_name, \"| tickers:\", len(universe))\n","universe.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"qIcVaMLy06Z4","executionInfo":{"status":"ok","timestamp":1763255393294,"user_tz":360,"elapsed":719,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"05ee8d6f-e620-4871-e9c5-bbada3f6b35c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["First train window: 2020-01-01 ‚Üí 2020-04-21\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1339305557.py:24: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  counts = train_slice.groupby(\"ticker\").size()\n"]},{"output_type":"stream","name":"stdout","text":["Saved static universe: data/static/universe_2020-04-21.csv | tickers: 25\n"]},{"output_type":"execute_result","data":{"text/plain":["  ticker\n","0   AAPL\n","1   AMZN\n","2    BAC\n","3   CSCO\n","4    CVX"],"text/html":["\n","  <div id=\"df-6eb68b17-bcad-4543-a1c1-d37a5e9a1536\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ticker</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>AAPL</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>AMZN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>BAC</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>CSCO</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>CVX</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6eb68b17-bcad-4543-a1c1-d37a5e9a1536')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-6eb68b17-bcad-4543-a1c1-d37a5e9a1536 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-6eb68b17-bcad-4543-a1c1-d37a5e9a1536');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-8d96def1-9d3c-4b12-b1e8-85fa98059ad8\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8d96def1-9d3c-4b12-b1e8-85fa98059ad8')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-8d96def1-9d3c-4b12-b1e8-85fa98059ad8 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"universe","summary":"{\n  \"name\": \"universe\",\n  \"rows\": 25,\n  \"fields\": [\n    {\n      \"column\": \"ticker\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"INTC\",\n          \"ORCL\",\n          \"AAPL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["### `counts = train_slice.groupby(\"ticker\").size()`\n","\n","* `train_slice.groupby(\"ticker\")` splits the DataFrame into groups by ticker symbol.\n","* `.size()` returns the **number of rows** in each group ‚Äî that is, how many data points each ticker has.\n","* The result is a **Series** indexed by ticker:\n","\n","Example:\n","\n","```python\n","import pandas as pd\n","\n","train_slice = pd.DataFrame({\n","    \"ticker\": [\"AAPL\",\"AAPL\",\"MSFT\",\"MSFT\",\"MSFT\",\"NVDA\"],\n","    \"date\": pd.date_range(\"2024-01-01\", periods=6),\n","    \"value\": [1,2,3,4,5,6]\n","})\n","\n","TRAIN_MIN = 3\n","\n","counts = train_slice.groupby(\"ticker\").size()\n","print(counts)\n","```\n","\n","Output:\n","\n","```\n","ticker\n","AAPL    2\n","MSFT    3\n","NVDA    1\n","dtype: int64\n","```\n","\n","So:\n","\n","* `'AAPL'` appears 2 times,\n","* `'MSFT'` 3 times,\n","* `'NVDA'` 1 time.\n","\n","---\n","\n","### `eligible = counts[counts >= TRAIN_MIN].index.sort_values()`\n","\n","* `counts >= TRAIN_MIN` produces a Boolean mask:\n","\n","  ```\n","  AAPL    False\n","  MSFT     True\n","  NVDA    False\n","  dtype: bool\n","  ```\n","* `counts[counts >= TRAIN_MIN]` filters the Series to include only the tickers satisfying the condition (here, at least 3 data points):\n","\n","  ```\n","  ticker\n","  MSFT    3\n","  dtype: int64\n","  ```\n","* `.index` extracts just the tickers (`Index([\"MSFT\"], dtype=object)`).\n","* `.sort_values()` sorts that index alphabetically.\n","\n","Final result:\n","\n","```python\n","eligible\n","# Index(['MSFT'], dtype='object')\n","```\n","\n"],"metadata":{"id":"BUC5ar3ObYs1"}},{"cell_type":"markdown","source":["> From now on, **filter** your data to `universe` before modeling/evaluation.\n","\n","# 2) Apply the static universe to your features"],"metadata":{"id":"Ub9TLHfD23KW"}},{"cell_type":"code","source":["feats_static = feats[feats[\"ticker\"].isin(set(universe[\"ticker\"]))].copy()\n","feats_static.to_parquet(\"data/processed/features_v1_static.parquet\", compression=\"zstd\", index=False)\n","print(\"Wrote data/processed/features_v1_static.parquet\", feats_static.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bD9Pbd3i2rGP","executionInfo":{"status":"ok","timestamp":1763255393675,"user_tz":360,"elapsed":380,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"6d2f57e7-cf8d-4373-c059-e58e50c46dbd"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote data/processed/features_v1_static.parquet (3975, 18)\n"]}]},{"cell_type":"markdown","source":["# 3) Add **leakage tests** that recompute trusted features & compare\n","\n","Create a high‚Äëvalue test file that **fails** if any feature depends on future rows."],"metadata":{"id":"Wv3b0sE34WS8"}},{"cell_type":"code","source":["# tests/test_leakage_features.py\n","from __future__ import annotations # stop evaluating type annotations before Python 3.14, see more details below\n","import numpy as np, pandas as pd\n","import pytest\n","\n","SAFE_ROLL = 20\n","\n","@pytest.fixture(scope=\"session\") # scope=\"session\": per test session, for all tests. see below for more dtails.\n","def df():\n","    import pandas as pd\n","    import pathlib\n","    p = pathlib.Path(\"data/processed/features_v1_static.parquet\")\n","    if not p.exists():\n","        p = pathlib.Path(\"data/processed/features_v1.parquet\")\n","    df = pd.read_parquet(p).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","    df[\"date\"] = pd.to_datetime(df[\"date\"])\n","    return df\n","\n","def test_label_definition_r1d(df):\n","    for tkr, g in df.groupby(\"ticker\"):\n","        assert g[\"r_1d\"].iloc[:-1].equals(g[\"log_return\"].iloc[1:]), f\"r_1d mismatch for {tkr}\"\n","\n","def _recompute_safe(g: pd.DataFrame) -> pd.DataFrame:\n","    # Recompute causal features using only <= t information\n","    out = pd.DataFrame(index=g.index)\n","    s = g[\"log_return\"]\n","    out[\"lag1\"] = s.shift(1)\n","    out[\"lag2\"] = s.shift(2)\n","    out[\"lag3\"] = s.shift(3)\n","    rm = s.rolling(SAFE_ROLL, min_periods=SAFE_ROLL).mean()\n","    rs = s.rolling(SAFE_ROLL, min_periods=SAFE_ROLL).std()\n","    out[\"roll_mean_20\"] = rm\n","    out[\"roll_std_20\"]  = rs\n","    out[\"zscore_20\"]    = (s - rm) / (rs + 1e-8)\n","    # EWM & expanding if present\n","    out[\"exp_mean\"] = s.expanding(min_periods=SAFE_ROLL).mean() # window expanded from the beginning (with a min_periods window)\n","    out[\"exp_std\"]  = s.expanding(min_periods=SAFE_ROLL).std()\n","    out[\"ewm_mean_20\"] = s.ewm(span=20, adjust=False).mean() # see below for more detail. adjust=False, use recursive formula\n","    out[\"ewm_std_20\"]  = s.ewm(span=20, adjust=False).std()\n","    # RSI(14) if adj_close present\n","    if \"adj_close\" in g:\n","        delta = g[\"adj_close\"].diff()\n","        up = delta.clip(lower=0).ewm(alpha=1/14, adjust=False).mean()\n","        dn = (-delta.clip(upper=0)).ewm(alpha=1/14, adjust=False).mean()\n","        rs = up / (dn + 1e-12)\n","        out[\"rsi_14\"] = 100 - (100/(1+rs))\n","    return out\n","\n","@pytest.mark.parametrize(\"col\", [\"lag1\",\"lag2\",\"lag3\",\"roll_mean_20\",\"roll_std_20\",\"zscore_20\",\"exp_mean\",\"exp_std\",\"ewm_mean_20\",\"ewm_std_20\",\"rsi_14\"])\n","# run pytest for each value in the list of \"col\"\n","def test_features_match_causal_recompute(df, col):\n","    if col not in df.columns:\n","        pytest.skip(f\"{col} not present\")\n","    # Compare per ticker to avoid cross-group alignment issues\n","    for tkr, g in df.groupby(\"ticker\", sort=False):\n","        ref = _recompute_safe(g)\n","        if col not in ref.columns:\n","            continue\n","        a = g[col].to_numpy()\n","        b = ref[col].to_numpy()\n","        # Allow NaNs at the start; compare where both finite\n","        mask = np.isfinite(a) & np.isfinite(b)   # elementwise and\n","        if mask.sum() == 0:\n","            continue\n","        diff = np.nanmax(np.abs(a[mask] - b[mask]))\n","        assert float(diff) <= 1e-6, f\"{col} deviates from causal recompute for {tkr}: max |Œî|={diff}\"\n","\n","def test_no_feature_equals_target(df):\n","    y = df[\"r_1d\"].to_numpy()\n","    for col in df.select_dtypes(include=[\"float32\",\"float64\"]).columns:\n","        if col in {\"r_1d\",\"log_return\"}:\n","            continue\n","        x = df[col].to_numpy()\n","        # Proportion of exact equality (within tiny tol) should not be high\n","        eq = np.isfinite(x) & np.isfinite(y) & (np.abs(x - y) < 1e-12)\n","        assert eq.mean() < 0.8, f\"Suspicious: feature {col} equals target too often\""],"metadata":{"id":"eNsuwxqI4gf7","executionInfo":{"status":"ok","timestamp":1763255393823,"user_tz":360,"elapsed":144,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["`from __future__ import annotations` is (was) a per‚Äëmodule switch that **stops Python from immediately evaluating type annotations**. Instead, the raw text of each annotation is stored (as a string) and only interpreted later by tools like `typing.get_type_hints`. This was introduced by **PEP 563** and is known as *postponed (stringified) annotations*. ([Python Enhancement Proposals (PEPs)][1])\n","\n","---\n","\n","## Why it existed\n","\n","Before Python 3.14, annotations were normally evaluated *eagerly* at definition time. That caused headaches like:\n","\n","* **Forward references** failed unless you quoted them (`\"User\"`) or reordered code.\n","* **Import cycles** and **import‚Äëtime work** triggered by annotations.\n","\n","Opting into `from __future__ import annotations` solved this by storing `'User'` instead of looking up `User` immediately. ([Python documentation][2])\n","\n","**Example (pre‚Äë3.14 behavior):**\n","\n","```py\n","# without the future import (<=3.13 default)\n","def f(x: C) -> None: ...\n","class C: pass\n","# NameError at function definition time (C not yet defined)\n","\n","# with the future import (3.7+)\n","from __future__ import annotations\n","def f(x: C) -> None: ...\n","class C: pass\n","print(f.__annotations__)         # {'x': 'C', 'return': 'None'}\n","```\n","\n","(The same example and outcomes are described in the 3.14 docs' ‚ÄúAnnotation semantics‚Äù section.) ([Python documentation][2])\n","\n","---\n","\n","## What‚Äôs the status **today** (Python 3.14+)\n","\n","Starting with **Python 3.14**, the default model changed to **deferred evaluation** (PEP 649 / PEP 749): annotations are computed lazily *when you ask for them*, not at import time‚Äîso forward references work out of the box, and you don‚Äôt need this future import anymore. ([Python documentation][3])\n","\n","> **Important:** In 3.14+, if you *do* keep `from __future__ import annotations`, you force the older **stringified** behavior in that module (for backward compatibility). The docs note this legacy switch ‚Äúwill eventually be deprecated and removed.‚Äù ([Python documentation][2])\n","\n","To inspect annotations under the new model, Python 3.14 adds **`annotationlib`** with `get_annotations()` and formats to get evaluated values, forward‚Äërefs, or strings:\n","\n","```py\n","from annotationlib import get_annotations, Format\n","get_annotations(f, format=Format.VALUE)       # evaluated\n","get_annotations(f, format=Format.FORWARDREF)  # safe proxies\n","get_annotations(f, format=Format.STRING)      # strings\n","```\n","\n","See the new library docs for details. ([Python documentation][2])\n","\n","---\n","\n","## When should you use it?\n","\n","* **Targeting Python ‚â§ 3.13:**\n","  Use `from __future__ import annotations` if you want easy forward references and to avoid import‚Äëtime evaluation of annotations. Tools can evaluate them later via `typing.get_type_hints`. ([Python Enhancement Proposals (PEPs)][1])\n","* **Targeting Python ‚â• 3.14:**\n","  **Don‚Äôt use it.** The default already defers evaluation; keeping the future import reverts you to stringified annotations and is slated for deprecation/removal. ([Python documentation][3])\n","\n","---\n","\n","## Where it must appear\n","\n","Future statements are **per‚Äëmodule** and must be placed **at the very top** of the file (after the module docstring and other future statements, before any other code). Otherwise you‚Äôll get a `SyntaxError`. ([Python documentation][4])\n","\n","---\n","\n","## Quick cheat‚Äësheet\n","\n","* **Effect (3.7‚Äì3.13 with the future import):**\n","  `__annotations__` contains **strings**; evaluation is postponed. ([Python Enhancement Proposals (PEPs)][1])\n","* **Effect (3.14+ by default):**\n","  Annotations are **lazy** (computed on demand). Keep the future import only if you intentionally want strings‚Äîfor legacy reasons. ([Python documentation][3])\n","* **Introspecting:**\n","  Use `typing.get_type_hints(...)` on older versions; on 3.14+ you can also use `annotationlib.get_annotations(..., format=...)`. ([Python documentation][2])\n","\n","\n","\n","[1]: https://peps.python.org/pep-0563/ \"PEP 563 ‚Äì Postponed Evaluation of Annotations\"\n","[2]: https://docs.python.org/3/library/annotationlib.html \"annotationlib ‚Äî Functionality for introspecting annotations ‚Äî Python 3.14.0 documentation\"\n","[3]: https://docs.python.org/3/whatsnew/3.14.html \"What's new in Python 3.14\"\n","[4]: https://docs.python.org/3/reference/simple_stmts.html\"7. Simple statements\"\n"],"metadata":{"id":"P53t2WM0B7kc"}},{"cell_type":"markdown","source":["##  1. A decorator is just a function that **takes another function (or class)** and **returns a new one**.\n","\n","Think of it as:\n","\n","> ‚ÄúWrap a function with extra behavior ‚Äî before, after, or around its execution ‚Äî without changing its code.‚Äù\n","\n","---\n","\n","### Basic example\n","\n","```python\n","def my_decorator(func):\n","    def wrapper():\n","        print(\"Before the function runs\")\n","        func()\n","        print(\"After the function runs\")\n","    return wrapper\n","```\n","\n","Now decorate a function:\n","\n","```python\n","@my_decorator\n","def say_hello():\n","    print(\"Hello!\")\n","\n","say_hello()\n","```\n","\n","Output:\n","\n","```\n","Before the function runs\n","Hello!\n","After the function runs\n","```\n","\n","---\n","\n","### What actually happens under the hood\n","\n","The line:\n","\n","```python\n","@my_decorator\n","def say_hello():\n","    ...\n","```\n","\n","is **syntactic sugar** for:\n","\n","```python\n","def say_hello():\n","    ...\n","say_hello = my_decorator(say_hello)\n","```\n","\n","So the name `say_hello` now **refers to the `wrapper` function** returned by `my_decorator`.\n","\n","---\n","\n","##  2. Decorators can add *extra logic* ‚Äî logging, timing, validation, caching, etc.\n","\n","Example: measure how long a function takes\n","\n","```python\n","import time\n","\n","def timer(func):\n","    def wrapper(*args, **kwargs):\n","        start = time.time()\n","        result = func(*args, **kwargs)\n","        end = time.time()\n","        print(f\"{func.__name__} took {end - start:.3f}s\")\n","        return result\n","    return wrapper\n","\n","@timer\n","def slow_add(a, b):\n","    time.sleep(1)\n","    return a + b\n","\n","slow_add(3, 4)\n","```\n","\n","Output:\n","\n","```\n","slow_add took 1.001s\n","```\n","\n","---\n","\n","## 3. Decorators can accept arguments too\n","\n","If you want a decorator that itself takes arguments (e.g. a retry count, a logging level), you need an *extra layer of function nesting*:\n","\n","```python\n","def repeat(n_times):\n","    def decorator(func):\n","        def wrapper(*args, **kwargs):\n","            for _ in range(n_times):\n","                func(*args, **kwargs)\n","        return wrapper\n","    return decorator\n","\n","@repeat(3)\n","def greet():\n","    print(\"Hello!\")\n","\n","greet()\n","```\n","\n","Output:\n","\n","```\n","Hello!\n","Hello!\n","Hello!\n","```\n","\n","---\n","\n","## 4. Preserving function metadata (`functools.wraps`)\n","\n","Without help, the decorated function loses its original name and docstring:\n","\n","```python\n","print(slow_add.__name__)  # 'wrapper'  üò¨\n","```\n","\n","To fix that, use `functools.wraps(func)`:\n","\n","```python\n","from functools import wraps\n","\n","def timer(func):\n","    @wraps(func)\n","    def wrapper(*args, **kwargs):\n","        ...\n","    return wrapper\n","```\n","\n","Now:\n","\n","```python\n","print(slow_add.__name__)  # 'slow_add' ‚úÖ\n","```\n","\n","---\n","\n","##  5. Decorators can also be used on classes or methods\n","\n","For instance, applying `@classmethod`, `@staticmethod`, `@property`, or even custom decorators to modify class behavior.\n","\n","Example:\n","\n","```python\n","def uppercase(method):\n","    @wraps(method)\n","    def wrapper(*args, **kwargs):\n","        result = method(*args, **kwargs)\n","        return result.upper()\n","    return wrapper\n","\n","class Greeter:\n","    @uppercase\n","    def greet(self):\n","        return \"hello world\"\n","\n","print(Greeter().greet())  # 'HELLO WORLD'\n","```\n","\n","---\n","\n","##  6. Summary\n","\n","| Concept                   | Meaning                                                          |\n","| ------------------------- | ---------------------------------------------------------------- |\n","| **Decorator**             | Function that takes another function/class and returns a new one |\n","| **`@decorator` syntax**   | Equivalent to `func = decorator(func)`                           |\n","| **Used for**              | Logging, validation, timing, caching, permissions, etc.          |\n","| **Use `functools.wraps`** | To keep original function‚Äôs metadata                             |\n","| **Can be stacked**        | Multiple decorators can wrap each other                          |\n","\n","---\n","\n","\n"],"metadata":{"id":"uxmHs9QtF2YF"}},{"cell_type":"markdown","source":["In **pytest**, the `@pytest.fixture` decorator can take a `scope` argument that controls **how long the fixture object lives** ‚Äî that is, how often pytest creates and destroys it.\n","\n","\n","\n","###  Basic idea\n","\n","```python\n","@pytest.fixture(scope=\"session\")\n","def db_connection():\n","    print(\"Setting up database connection...\")\n","    yield connect_to_db()\n","    print(\"Tearing down database connection...\")\n","```\n","\n","Here, `scope=\"session\"` means:\n","\n","> The fixture is **created once per entire test session**, shared across **all tests**, and **destroyed at the end** of the run.\n","\n","So every test that depends on `db_connection` will reuse the same instance.\n","\n","---\n","\n","###  All possible scope levels\n","\n","| Scope        | Lifetime                                            | Typical Use                                               |\n","| ------------ | --------------------------------------------------- | --------------------------------------------------------- |\n","| `\"function\"` | Default ‚Äî new fixture per test function             | Isolated tests; avoids cross-test state                   |\n","| `\"class\"`    | One fixture per test class                          | Expensive setup reused by tests in same class             |\n","| `\"module\"`   | One fixture per Python module (file)                | Shared state across all tests in one file                 |\n","| `\"package\"`  | One fixture per package (folder with `__init__.py`) | Rarely used; persists for all tests in that package       |\n","| `\"session\"`  | One fixture for the entire pytest run               | Global resources (DB, API client, temporary folder, etc.) |\n","\n","---\n","\n","###  Lifetime & teardown sequence\n","\n","Fixtures are **created lazily** ‚Äî only when first requested ‚Äî and **torn down in reverse order** of creation at the end of their scope.\n","\n","For `scope=\"session\"`, the fixture:\n","\n","1. Is initialized once before any test needs it.\n","2. Is reused across all modules, classes, and functions.\n","3. Is finalized when pytest is exiting (after all tests complete).\n","\n","---\n","\n","###  Example of multiple scopes\n","\n","```python\n","import pytest\n","\n","@pytest.fixture(scope=\"session\")\n","def db():\n","    print(\"Connecting to database\")\n","    yield \"DB_CONNECTION\"\n","    print(\"Closing database\")\n","\n","@pytest.fixture(scope=\"module\")\n","def dataset(db):\n","    print(\"Loading dataset from\", db)\n","    yield \"DATASET\"\n","    print(\"Unloading dataset\")\n","\n","def test_a(dataset):\n","    print(\"Test A using\", dataset)\n","\n","def test_b(dataset):\n","    print(\"Test B using\", dataset)\n","```\n","\n","**Output (simplified):**\n","\n","```\n","Connecting to database      ‚Üê once per session\n","Loading dataset from DB_CONNECTION\n","Test A using DATASET\n","Test B using DATASET\n","Unloading dataset           ‚Üê once per module\n","Closing database            ‚Üê once per session\n","```\n","\n","---\n","\n","\n","## üîπ The code defines two fixtures\n","\n","### 1Ô∏è‚É£ `db` ‚Äî session-scoped\n","\n","```python\n","@pytest.fixture(scope=\"session\")\n","def db():\n","    print(\"Connecting to database\")\n","    yield \"DB_CONNECTION\"\n","    print(\"Closing database\")\n","```\n","\n","* `scope=\"session\"` ‚Üí created **once for the entire pytest run**.\n","* When first needed, pytest runs the setup part (before `yield`).\n","* The value `\"DB_CONNECTION\"` is passed to any test or fixture that depends on `db`.\n","* After *all* tests finish, pytest resumes after the `yield` ‚Üí teardown runs: `\"Closing database\"`.\n","\n","So think of it as a **global resource** ‚Äî e.g., one connection reused everywhere.\n","\n","---\n","\n","### 2Ô∏è‚É£ `dataset` ‚Äî module-scoped\n","\n","```python\n","@pytest.fixture(scope=\"module\")\n","def dataset(db):\n","    print(\"Loading dataset from\", db)\n","    yield \"DATASET\"\n","    print(\"Unloading dataset\")\n","```\n","\n","* `scope=\"module\"` ‚Üí created **once per test file (module)**.\n","* Depends on `db`, so pytest first ensures `db` is ready.\n","* After the `yield`, its value (`\"DATASET\"`) is passed to any test using `dataset`.\n","* When the module‚Äôs tests finish, it runs the teardown code after `yield`.\n","\n","So within a file, all tests share one `\"DATASET\"`; but in a different test file, pytest would call this fixture again.\n","\n","---\n","\n","## The two test functions\n","\n","```python\n","def test_a(dataset):\n","    print(\"Test A using\", dataset)\n","\n","def test_b(dataset):\n","    print(\"Test B using\", dataset)\n","```\n","\n","Both tests depend on the `dataset` fixture.\n","\n","---\n","\n","## üîπ Execution flow step-by-step\n","\n","1. **Pytest starts the session.**\n","\n","   * It sees that `test_a` and `test_b` both require `dataset`, which in turn requires `db`.\n","\n","2. **Before the first test, it resolves dependencies:**\n","\n","   * No `db` yet ‚Üí create it:\n","\n","     ```\n","     Connecting to database\n","     ```\n","   * No `dataset` yet (for this module) ‚Üí create it, using `db`:\n","\n","     ```\n","     Loading dataset from DB_CONNECTION\n","     ```\n","\n","3. **Run the tests (both share same `dataset`):**\n","\n","   ```\n","   Test A using DATASET\n","   Test B using DATASET\n","   ```\n","\n","4. **Module finishes ‚Üí teardown `dataset`:**\n","\n","   ```\n","   Unloading dataset\n","   ```\n","\n","5. **All tests finished ‚Üí teardown `db`:**\n","\n","   ```\n","   Closing database\n","   ```\n","\n","---\n","\n","##  Why it behaves that way\n","\n","* Pytest builds a **fixture dependency tree**.\n","* Fixtures with **larger scopes** live longer:\n","\n","  ```\n","  session > package > module > class > function\n","  ```\n","* When a fixture depends on another, pytest ensures the *dependency* has an equal or wider scope.\n","  (It‚Äôs invalid for a narrower fixture to depend on a wider one.)\n","\n","So here:\n","\n","* `dataset (module)` depends on `db (session)`\n","* `db` stays alive until the end of session.\n","* `dataset` stays alive until end of this module.\n","\n","---\n","\n","##  Summary of lifetime\n","\n","| Fixture            | Scope      | Created                       | Destroyed                           |\n","| ------------------ | ---------- | ----------------------------- | ----------------------------------- |\n","| `db`               | `session`  | Once at first use in any test | After all tests finish              |\n","| `dataset`          | `module`   | Once per test file            | After all tests in that file finish |\n","| `test_a`, `test_b` | `function` | Called per test               | End after each test                 |\n","\n","---\n","\n","### Final output order (as printed)\n","\n","```\n","Connecting to database      ‚Üê db setup (once)\n","Loading dataset from DB_CONNECTION   ‚Üê dataset setup (once per module)\n","Test A using DATASET                 ‚Üê test function\n","Test B using DATASET                 ‚Üê test function\n","Unloading dataset                    ‚Üê dataset teardown\n","Closing database                     ‚Üê db teardown (end of session)\n","```\n","\n","---\n","\n","Would you like me to expand this example to include a **function-scoped** fixture too (so you can see how its setup/teardown happens before and after *each* test)?\n","\n","\n","###  When to use `scope=\"session\"`\n","\n","Use it when:\n","\n","* Setup is **expensive** and can safely be **shared globally** (e.g. DB, Docker container, web server).\n","* Tests **don‚Äôt modify** shared state or are designed to handle concurrency.\n","* You want **fast test suites** that reuse a single resource.\n","\n","Avoid it when:\n","\n","* Tests rely on **isolation** or mutate the resource.\n","* The resource depends on per-test configuration.\n","\n","---\n","\n","### Summary\n","\n","* `scope=\"session\"` ‚Üí fixture lives for the whole pytest run.\n","* Created **once**, shared **everywhere**, destroyed **at the very end**.\n","* Great for global connections, caches, or initialization steps.\n","* Other scopes (`function`, `class`, `module`, `package`) define progressively larger lifetimes.\n","\n","---\n","\n","\n","\n"],"metadata":{"id":"j43PbrgHEF0Q"}},{"cell_type":"markdown","source":["## 1. `expanding(min_periods=SAFE_ROLL).std()`\n","\n","### Meaning\n","\n","This uses **expanding windows** ‚Äî cumulative statistics from the start up to the current point.\n","\n","```python\n","out[\"exp_std\"] = s.expanding(min_periods=SAFE_ROLL).std()\n","```\n","\n","* `s.expanding(...)` ‚Üí returns an *Expanding* object that represents\n","  ‚Äúall data from the beginning up to this index‚Äù.\n","* `min_periods=SAFE_ROLL` ‚Üí until you have at least `SAFE_ROLL` observations,\n","  it returns `NaN`.\n","\n","So at index *i*, this is:\n","\n","$$\n","\\text{exp_std}_i = \\operatorname{std}(s_0, s_1, \\dots, s_i)\n","\\quad \\text{(if } i+1 \\ge SAFE_ROLL)\n","$$\n","\n","### Example\n","\n","If `SAFE_ROLL = 3` and\n","`s = [1, 2, 4, 8]`, then:\n","\n","| index | values used  | std    |\n","| ----- | ------------ | ------ |\n","| 0     | (not enough) | NaN    |\n","| 1     | (not enough) | NaN    |\n","| 2     | [1, 2, 4]    | ‚âà 1.53 |\n","| 3     | [1, 2, 4, 8] | ‚âà 3.11 |\n","\n","It‚Äôs like a **growing window** ‚Äî the first value that meets the minimum, then keeps accumulating all past data.\n","\n","---\n","\n","## 2. `ewm(span=20, adjust=False).mean()`\n","\n","### Meaning\n","\n","This uses an **exponentially weighted moving average** (EWMA).\n","\n","```python\n","out[\"ewm_mean_20\"] = s.ewm(span=20, adjust=False).mean()\n","```\n","\n","* `span=20` controls the *decay rate* (analogous to a 20-period moving average).\n","* Recent values get **more weight**, older values **less weight**, with exponential decay.\n","* `adjust=False` ‚Üí makes it compute the recursive form:\n","\n","$$\n","  y_t = (1 - \\alpha) , y_{t-1} + \\alpha , x_t,\n","  \\quad \\text{where } \\alpha = \\frac{2}{span + 1}.\n"," $$\n","\n","If `span=20`, then `Œ± ‚âà 0.095`.\n","\n","This is smoother and more responsive than a simple rolling mean.\n","\n","---\n","\n","### Difference from rolling mean\n","\n","| Type                        | Uses fixed window?               | Weights           | Behavior                             |\n","| --------------------------- | -------------------------------- | ----------------- | ------------------------------------ |\n","| `rolling(window=20).mean()` | ‚úÖ Fixed last 20 points           | Equal             | Sharp edges (drops oldest instantly) |\n","| `ewm(span=20).mean()`       | ‚ùå Infinite window                | Exponential decay | Smooth adaptation                    |\n","| `expanding().mean()`        | ‚ùå Growing window (all past data) | Equal             | Becomes stable slowly                |\n","\n","---\n","\n","### Example\n","\n","If `s = [1, 2, 3, 4, 5]` and `span=2`:\n","\n","| index | formula                 | result |\n","| ----- | ----------------------- | ------ |\n","| 0     | y‚ÇÄ = 1                  | 1.00   |\n","| 1     | y‚ÇÅ = 0.33¬∑1 + 0.67¬∑2    | 1.67   |\n","| 2     | y‚ÇÇ = 0.33¬∑1.67 + 0.67¬∑3 | 2.44   |\n","| 3     | y‚ÇÉ = 0.33¬∑2.44 + 0.67¬∑4 | 3.48   |\n","| 4     | y‚ÇÑ = 0.33¬∑3.48 + 0.67¬∑5 | 4.49   |\n","\n","---\n","\n","##  Summary\n","\n","| Feature                | Window Type                     | Formula                   | Description                                   |\n","| ---------------------- | ------------------------------- | ------------------------- | --------------------------------------------- |\n","| `expanding(...).std()` | Growing (all data)              | Std. of all points so far | Measures long-term volatility as sample grows |\n","| `ewm(span=20).mean()`  | Infinite with exponential decay | Weighted mean, Œ±=2/(20+1) | Tracks recent trend smoothly                  |\n","\n","---\n","\n","\n"],"metadata":{"id":"pw9uAkzCKoKI"}},{"cell_type":"markdown","source":["The general form of the  weighted mean is:\n","\n","$$\n","\\text{WM}_t = \\frac{\\sum_{i=0}^{t} w_i x_i}{\\sum_{i=0}^{t} w_i},\n","$$\n","\n","where the weights $ w_i $ **decay exponentially** as observations get older.\n","\n","In Pandas:\n","\n","```python\n","s.ewm(span=20, adjust=...).mean()\n","```\n","\n","The key question is **how** those weights $ w_i $ are normalized and updated as new data arrive ‚Äî that‚Äôs what `adjust` controls.\n","\n","---\n","\n","##  When `adjust=True` (the default)\n","\n","This is the **textbook (exact)** exponentially weighted formula:\n","\n","$$\n","y_t = \\frac{\\sum_{i=0}^{t} (1 - \\alpha)^{t - i} \\, x_i}{\\sum_{i=0}^{t} (1 - \\alpha)^{t - i}},\n","$$\n","where\n","$$\n","\\alpha = \\frac{2}{\\text{span} + 1}.\n","$$\n","\n","Each point contributes explicitly with its weight.\n","Older points keep a geometric weight of $(1 - \\alpha)^k$.\n","\n","---\n","\n","###  Interpretation\n","\n","* It‚Äôs the **\"batch\"** version ‚Äî computes the weighted average over all points seen so far.\n","* The denominator (sum of weights) grows toward $1 / \\alpha$ asymptotically.\n","* You can view it as ‚Äúthe exact result you‚Äôd get if you recomputed the whole weighted average from scratch each time‚Äù.\n","\n","---\n","\n","### Example\n","\n","Let‚Äôs take a short sequence:\n","\n","```python\n","import pandas as pd\n","s = pd.Series([10, 20, 30])\n","```\n","\n","With `Œ± = 0.5` (say `span=2`):\n","\n","| t | x‚Çú | Weights (adjust=True)                              | Weighted mean               |\n","| - | -- | -------------------------------------------------- | --------------------------- |\n","| 0 | 10 | [1.0]                                              | 10.0                        |\n","| 1 | 20 | [0.5, 1.0] ‚Üí normalized ‚Üí [1/3, 2/3]               | (1/3)¬∑10 + (2/3)¬∑20 = 16.67 |\n","| 2 | 30 | [0.25, 0.5, 1.0] ‚Üí normalized ‚Üí [0.14, 0.29, 0.57] | ‚âà 23.33                     |\n","\n","---\n","\n","##  When `adjust=False`\n","\n","Now Pandas uses the **recursive (online)** formulation:\n","\n","$$\n","y_t = (1 - \\alpha) y_{t-1} + \\alpha x_t.\n","$$\n","\n","This does *not* renormalize all past weights each time ‚Äî instead, it uses the previous result as a smoothed state.\n","\n","It gives a **slightly different series**, especially at the beginning, because the normalization is implicit.\n","\n","---\n","\n","###  Interpretation\n","\n","* It‚Äôs the **\"recursive / efficient\"** form, commonly used in streaming or real-time settings.\n","\n","* You can think of it as:\n","\n","  > ‚ÄúUpdate the previous estimate by moving Œ± fraction toward the new observation.‚Äù\n","\n","* After many points, both `adjust=True` and `adjust=False` converge to almost the same value, but the *initial few points differ*.\n","\n","---\n","\n","### Example continued\n","\n","Same data, same Œ± = 0.5:\n","\n","| t | x‚Çú | Formula         | y‚Çú   |\n","| - | -- | --------------- | ---- |\n","| 0 | 10 | y‚ÇÄ = x‚ÇÄ         | 10.0 |\n","| 1 | 20 | 0.5¬∑10 + 0.5¬∑20 | 15.0 |\n","| 2 | 30 | 0.5¬∑15 + 0.5¬∑30 | 22.5 |\n","\n","Compare:\n","\n","* `adjust=True` gave `[10.0, 16.67, 23.33]`\n","* `adjust=False` gave `[10.0, 15.0, 22.5]`\n","\n","They‚Äôre close, but not identical ‚Äî the *recursive version* lags slightly more at the beginning.\n","\n","---\n","\n","##  Summary\n","\n","| Flag           | Formula                                                           | Type                                               | When to use                                                                              |\n","| -------------- | ----------------------------------------------------------------- | -------------------------------------------------- | ---------------------------------------------------------------------------------------- |\n","| `adjust=True`  | $$y_t = \\frac{\\sum (1-\\alpha)^{t-i} x_i}{\\sum (1-\\alpha)^{t-i}}$$ | *Exact weighted average* (re-normalized each time) | When you want mathematically correct EWM or when comparing to textbook definitions       |\n","| `adjust=False` | $$y_t = (1-\\alpha)y_{t-1} + \\alpha x_t$$                          | *Recursive (online) update*                        | When streaming, efficiency, or aligning with real-time filters (EMA, technical analysis) |\n","\n","---\n","\n","###  Practical takeaways\n","\n","* In finance or ML feature engineering, `adjust=False` is most common ‚Äî it mimics an *exponential moving average (EMA)* used in trading and real-time filters.\n","* In statistical modeling, `adjust=True` corresponds to the *pure exponential weighting formula* that reweights all history at each step.\n","* After a long time, both give nearly identical results; the difference is mainly in early periods.\n","\n","---\n"],"metadata":{"id":"opXyIsohLnvw"}},{"cell_type":"markdown","source":["\n","\n","##  1. `&` ‚Äî *bitwise AND operator*\n","\n","In most languages (including **Python**, **C**, **C++**, **Java**, **JavaScript**):\n","\n","* `&` performs a **bitwise AND** between two integers (or booleans, elementwise for arrays in NumPy/Pandas).\n","* It operates on **each bit individually**.\n","\n","### Example (bitwise)\n","\n","```python\n","a = 6  # binary: 110\n","b = 3  # binary: 011\n","print(a & b)   # 010  ‚Üí 2\n","```\n","\n","|           a          |  b  | a & b |\n","| :------------------: | :-: | :---: |\n","|           1          |  0  |   0   |\n","|           1          |  1  |   1   |\n","|           0          |  1  |   0   |\n","| ‚Üí result: `010‚ÇÇ = 2` |     |       |\n","\n","---\n","\n","## 2. `&&` ‚Äî *logical AND operator*\n","\n","* `&&` exists in **C, C++, Java, JavaScript**, etc.\n","* It performs **short-circuit logical AND**:\n","  evaluates the left-hand side first; only evaluates the right-hand side if needed.\n","\n","### Example (logical)\n","\n","```c\n","if (x > 0 && y > 0) {\n","    printf(\"Both positive\\n\");\n","}\n","```\n","\n","Here, if `x > 0` is false, the second condition `y > 0` is **not even checked**.\n","\n","---\n","\n","##  3. Python **does not have `&&`**\n","\n","In Python, logical operations use **English keywords**:\n","\n","| C / Java | Python equivalent |   |      |\n","| -------- | ----------------- | - | ---- |\n","| `&&`     | `and`             |   |      |\n","| `        \\|                   \\| ` | `or` |\n","| `!`      | `not`             |   |      |\n","\n","So:\n","\n","```python\n","if x > 0 and y > 0:\n","    print(\"Both positive\")\n","```\n","\n","Python‚Äôs `and` also **short-circuits**, just like `&&`.\n","\n","---\n","\n","##  4. Mixing up `&` and `and` in Python\n","\n","If you accidentally write:\n","\n","```python\n","(x > 0) & (y > 0)\n","```\n","\n","it *will* work, but **as a bitwise operation** ‚Äî not a logical short-circuit.\n","\n","* For **scalars**, it behaves similarly but with stricter typing rules.\n","* For **NumPy / Pandas**, it‚Äôs **elementwise logical AND**.\n","\n","### Example with Pandas/NumPy\n","\n","```python\n","import pandas as pd\n","df = pd.DataFrame({\"a\": [1, -2, 3], \"b\": [2, 3, -1]})\n","\n","mask = (df[\"a\"] > 0) & (df[\"b\"] > 0)\n","print(mask)\n","```\n","\n","Output:\n","\n","```\n","0     True\n","1    False\n","2    False\n","dtype: bool\n","```\n","\n","Works ‚Äî because Pandas overloads `&` for elementwise logical AND.\n","But if you used `and`, it would error:\n","\n","```python\n","(df[\"a\"] > 0 and df[\"b\"] > 0)\n","# ValueError: The truth value of a Series is ambiguous\n","```\n","\n","---\n","\n","##  5. Summary\n","\n","| Operator | Language         | Meaning                       | Short-circuits? | Typical use                  |                     |   |   |\n","| -------- | ---------------- | ----------------------------- | --------------- | ---------------------------- | ------------------- | - | - |\n","| `&`      | All              | Bitwise AND / elementwise AND | ‚ùå No            | Integer or array bitwise ops |                     |   |   |\n","| `&&`     | C, C++, Java, JS | Logical AND                   | ‚úÖ Yes           | Boolean conditions           |                     |   |   |\n","| `and`    | Python           | Logical AND                   | ‚úÖ Yes           | Boolean conditions           |                     |   |   |\n","| `        \\| `, `             \\|                               \\| `, `or`         | Bitwise OR / logical OR      | Similar distinction |   |   |\n","\n","---\n","\n","### TL;DR\n","\n","* In **Python**, use:\n","\n","  * `and` / `or` for logic\n","  * `&` / `|` for bitwise or elementwise logic (NumPy, Pandas)\n","* In **C-like languages**, use:\n","\n","  * `&&` / `||` for logic\n","  * `&` / `|` for bitwise operations.\n","\n","\n"],"metadata":{"id":"Lz3yrki9R9qs"}},{"cell_type":"code","source":[],"metadata":{"id":"NBBgCDnYR9R0","executionInfo":{"status":"ok","timestamp":1763255393837,"user_tz":360,"elapsed":11,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["Run test now:"],"metadata":{"id":"g5GX-92B4mu5"}},{"cell_type":"code","source":["!pytest -q tests/test_leakage_features.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MaLfxCkq4lK_","executionInfo":{"status":"ok","timestamp":1763255400808,"user_tz":360,"elapsed":6970,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"e0ded8c7-d92b-49f0-8b88-5da15dc721ee"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                            [100%]\u001b[0m\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m__________________________ test_label_definition_r1d ___________________________\u001b[0m\n","\n","df =            date ticker  log_return  ...   exp_std  adj_close   volume\n","0    2020-01-29   AAPL   -0.018417  ...  0.00847...489  82.670891  1777769\n","3974 2020-09-07    XOM   -0.008826  ...  0.009480  81.944473  1272137\n","\n","[3975 rows x 18 columns]\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_label_definition_r1d\u001b[39;49;00m(df):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m tkr, g \u001b[95min\u001b[39;49;00m df.groupby(\u001b[33m\"\u001b[39;49;00m\u001b[33mticker\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",">           \u001b[94massert\u001b[39;49;00m g[\u001b[33m\"\u001b[39;49;00m\u001b[33mr_1d\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].iloc[:-\u001b[94m1\u001b[39;49;00m].equals(g[\u001b[33m\"\u001b[39;49;00m\u001b[33mlog_return\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].iloc[\u001b[94m1\u001b[39;49;00m:]), \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mr_1d mismatch for \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtkr\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: r_1d mismatch for AAPL\u001b[0m\n","\u001b[1m\u001b[31mE           assert False\u001b[0m\n","\u001b[1m\u001b[31mE            +  where False = equals(1     -0.002351\\n2     -0.012675\\n3      0.002713\\n4      0.001568\\n5     -0.001869\\n         ...   \\n154   -0.011083\\n155   -0.012161\\n156    0.013355\\n157   -0.005071\\n158    0.002917\\nName: log_return, Length: 158, dtype: float32)\u001b[0m\n","\u001b[1m\u001b[31mE            +    where equals = 0     -0.002351\\n1     -0.012675\\n2      0.002713\\n3      0.001568\\n4     -0.001869\\n         ...   \\n153   -0.011083\\n154   -0.012161\\n155    0.013355\\n156   -0.005071\\n157    0.002917\\nName: r_1d, Length: 158, dtype: float32.equals\u001b[0m\n","\n","\u001b[1m\u001b[31mtests/test_leakage_features.py\u001b[0m:21: AssertionError\n","\u001b[31m\u001b[1m________________ test_features_match_causal_recompute[exp_mean] ________________\u001b[0m\n","\n","df =            date ticker  log_return  ...   exp_std  adj_close   volume\n","0    2020-01-29   AAPL   -0.018417  ...  0.00847...489  82.670891  1777769\n","3974 2020-09-07    XOM   -0.008826  ...  0.009480  81.944473  1272137\n","\n","[3975 rows x 18 columns]\n","col = 'exp_mean'\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcol\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[33m\"\u001b[39;49;00m\u001b[33mlag1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mlag2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mlag3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mroll_mean_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mroll_std_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mzscore_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mexp_mean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mexp_std\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mewm_mean_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mewm_std_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mrsi_14\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_features_match_causal_recompute\u001b[39;49;00m(df, col):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m col \u001b[95mnot\u001b[39;49;00m \u001b[95min\u001b[39;49;00m df.columns:\u001b[90m\u001b[39;49;00m\n","            pytest.skip(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcol\u001b[33m}\u001b[39;49;00m\u001b[33m not present\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[90m# Compare per ticker to avoid cross-group alignment issues\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m tkr, g \u001b[95min\u001b[39;49;00m df.groupby(\u001b[33m\"\u001b[39;49;00m\u001b[33mticker\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, sort=\u001b[94mFalse\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            ref = _recompute_safe(g)\u001b[90m\u001b[39;49;00m\n","            \u001b[94mif\u001b[39;49;00m col \u001b[95mnot\u001b[39;49;00m \u001b[95min\u001b[39;49;00m ref.columns:\u001b[90m\u001b[39;49;00m\n","                \u001b[94mcontinue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            a = g[col].to_numpy()\u001b[90m\u001b[39;49;00m\n","            b = ref[col].to_numpy()\u001b[90m\u001b[39;49;00m\n","            \u001b[90m# Allow NaNs at the start; compare where both finite\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            mask = np.isfinite(a) & np.isfinite(b)\u001b[90m\u001b[39;49;00m\n","            \u001b[94mif\u001b[39;49;00m mask.sum() == \u001b[94m0\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                \u001b[94mcontinue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            diff = np.nanmax(np.abs(a[mask] - b[mask]))\u001b[90m\u001b[39;49;00m\n",">           \u001b[94massert\u001b[39;49;00m \u001b[96mfloat\u001b[39;49;00m(diff) <= \u001b[94m1e-6\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcol\u001b[33m}\u001b[39;49;00m\u001b[33m deviates from causal recompute for \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtkr\u001b[33m}\u001b[39;49;00m\u001b[33m: max |Œî|=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdiff\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: exp_mean deviates from causal recompute for AAPL: max |Œî|=0.0006860782421426849\u001b[0m\n","\u001b[1m\u001b[31mE           assert 0.0006860782421426849 <= 1e-06\u001b[0m\n","\u001b[1m\u001b[31mE            +  where 0.0006860782421426849 = float(np.float64(0.0006860782421426849))\u001b[0m\n","\n","\u001b[1m\u001b[31mtests/test_leakage_features.py\u001b[0m:65: AssertionError\n","\u001b[31m\u001b[1m________________ test_features_match_causal_recompute[exp_std] _________________\u001b[0m\n","\n","df =            date ticker  log_return  ...   exp_std  adj_close   volume\n","0    2020-01-29   AAPL   -0.018417  ...  0.00847...489  82.670891  1777769\n","3974 2020-09-07    XOM   -0.008826  ...  0.009480  81.944473  1272137\n","\n","[3975 rows x 18 columns]\n","col = 'exp_std'\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcol\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[33m\"\u001b[39;49;00m\u001b[33mlag1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mlag2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mlag3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mroll_mean_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mroll_std_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mzscore_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mexp_mean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mexp_std\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mewm_mean_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mewm_std_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mrsi_14\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_features_match_causal_recompute\u001b[39;49;00m(df, col):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m col \u001b[95mnot\u001b[39;49;00m \u001b[95min\u001b[39;49;00m df.columns:\u001b[90m\u001b[39;49;00m\n","            pytest.skip(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcol\u001b[33m}\u001b[39;49;00m\u001b[33m not present\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[90m# Compare per ticker to avoid cross-group alignment issues\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m tkr, g \u001b[95min\u001b[39;49;00m df.groupby(\u001b[33m\"\u001b[39;49;00m\u001b[33mticker\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, sort=\u001b[94mFalse\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            ref = _recompute_safe(g)\u001b[90m\u001b[39;49;00m\n","            \u001b[94mif\u001b[39;49;00m col \u001b[95mnot\u001b[39;49;00m \u001b[95min\u001b[39;49;00m ref.columns:\u001b[90m\u001b[39;49;00m\n","                \u001b[94mcontinue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            a = g[col].to_numpy()\u001b[90m\u001b[39;49;00m\n","            b = ref[col].to_numpy()\u001b[90m\u001b[39;49;00m\n","            \u001b[90m# Allow NaNs at the start; compare where both finite\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            mask = np.isfinite(a) & np.isfinite(b)\u001b[90m\u001b[39;49;00m\n","            \u001b[94mif\u001b[39;49;00m mask.sum() == \u001b[94m0\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                \u001b[94mcontinue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            diff = np.nanmax(np.abs(a[mask] - b[mask]))\u001b[90m\u001b[39;49;00m\n",">           \u001b[94massert\u001b[39;49;00m \u001b[96mfloat\u001b[39;49;00m(diff) <= \u001b[94m1e-6\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcol\u001b[33m}\u001b[39;49;00m\u001b[33m deviates from causal recompute for \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtkr\u001b[33m}\u001b[39;49;00m\u001b[33m: max |Œî|=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdiff\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: exp_std deviates from causal recompute for AAPL: max |Œî|=0.0007243017298223572\u001b[0m\n","\u001b[1m\u001b[31mE           assert 0.0007243017298223572 <= 1e-06\u001b[0m\n","\u001b[1m\u001b[31mE            +  where 0.0007243017298223572 = float(np.float64(0.0007243017298223572))\u001b[0m\n","\n","\u001b[1m\u001b[31mtests/test_leakage_features.py\u001b[0m:65: AssertionError\n","\u001b[31m\u001b[1m______________ test_features_match_causal_recompute[ewm_mean_20] _______________\u001b[0m\n","\n","df =            date ticker  log_return  ...   exp_std  adj_close   volume\n","0    2020-01-29   AAPL   -0.018417  ...  0.00847...489  82.670891  1777769\n","3974 2020-09-07    XOM   -0.008826  ...  0.009480  81.944473  1272137\n","\n","[3975 rows x 18 columns]\n","col = 'ewm_mean_20'\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcol\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[33m\"\u001b[39;49;00m\u001b[33mlag1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mlag2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mlag3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mroll_mean_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mroll_std_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mzscore_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mexp_mean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mexp_std\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mewm_mean_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mewm_std_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mrsi_14\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_features_match_causal_recompute\u001b[39;49;00m(df, col):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m col \u001b[95mnot\u001b[39;49;00m \u001b[95min\u001b[39;49;00m df.columns:\u001b[90m\u001b[39;49;00m\n","            pytest.skip(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcol\u001b[33m}\u001b[39;49;00m\u001b[33m not present\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[90m# Compare per ticker to avoid cross-group alignment issues\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m tkr, g \u001b[95min\u001b[39;49;00m df.groupby(\u001b[33m\"\u001b[39;49;00m\u001b[33mticker\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, sort=\u001b[94mFalse\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            ref = _recompute_safe(g)\u001b[90m\u001b[39;49;00m\n","            \u001b[94mif\u001b[39;49;00m col \u001b[95mnot\u001b[39;49;00m \u001b[95min\u001b[39;49;00m ref.columns:\u001b[90m\u001b[39;49;00m\n","                \u001b[94mcontinue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            a = g[col].to_numpy()\u001b[90m\u001b[39;49;00m\n","            b = ref[col].to_numpy()\u001b[90m\u001b[39;49;00m\n","            \u001b[90m# Allow NaNs at the start; compare where both finite\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            mask = np.isfinite(a) & np.isfinite(b)\u001b[90m\u001b[39;49;00m\n","            \u001b[94mif\u001b[39;49;00m mask.sum() == \u001b[94m0\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                \u001b[94mcontinue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            diff = np.nanmax(np.abs(a[mask] - b[mask]))\u001b[90m\u001b[39;49;00m\n",">           \u001b[94massert\u001b[39;49;00m \u001b[96mfloat\u001b[39;49;00m(diff) <= \u001b[94m1e-6\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcol\u001b[33m}\u001b[39;49;00m\u001b[33m deviates from causal recompute for \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtkr\u001b[33m}\u001b[39;49;00m\u001b[33m: max |Œî|=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdiff\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: ewm_mean_20 deviates from causal recompute for AAPL: max |Œî|=0.013164876028895378\u001b[0m\n","\u001b[1m\u001b[31mE           assert 0.013164876028895378 <= 1e-06\u001b[0m\n","\u001b[1m\u001b[31mE            +  where 0.013164876028895378 = float(np.float64(0.013164876028895378))\u001b[0m\n","\n","\u001b[1m\u001b[31mtests/test_leakage_features.py\u001b[0m:65: AssertionError\n","\u001b[31m\u001b[1m_______________ test_features_match_causal_recompute[ewm_std_20] _______________\u001b[0m\n","\n","df =            date ticker  log_return  ...   exp_std  adj_close   volume\n","0    2020-01-29   AAPL   -0.018417  ...  0.00847...489  82.670891  1777769\n","3974 2020-09-07    XOM   -0.008826  ...  0.009480  81.944473  1272137\n","\n","[3975 rows x 18 columns]\n","col = 'ewm_std_20'\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcol\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[33m\"\u001b[39;49;00m\u001b[33mlag1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mlag2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mlag3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mroll_mean_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mroll_std_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mzscore_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mexp_mean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mexp_std\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mewm_mean_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mewm_std_20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mrsi_14\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_features_match_causal_recompute\u001b[39;49;00m(df, col):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m col \u001b[95mnot\u001b[39;49;00m \u001b[95min\u001b[39;49;00m df.columns:\u001b[90m\u001b[39;49;00m\n","            pytest.skip(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcol\u001b[33m}\u001b[39;49;00m\u001b[33m not present\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[90m# Compare per ticker to avoid cross-group alignment issues\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m tkr, g \u001b[95min\u001b[39;49;00m df.groupby(\u001b[33m\"\u001b[39;49;00m\u001b[33mticker\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, sort=\u001b[94mFalse\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            ref = _recompute_safe(g)\u001b[90m\u001b[39;49;00m\n","            \u001b[94mif\u001b[39;49;00m col \u001b[95mnot\u001b[39;49;00m \u001b[95min\u001b[39;49;00m ref.columns:\u001b[90m\u001b[39;49;00m\n","                \u001b[94mcontinue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            a = g[col].to_numpy()\u001b[90m\u001b[39;49;00m\n","            b = ref[col].to_numpy()\u001b[90m\u001b[39;49;00m\n","            \u001b[90m# Allow NaNs at the start; compare where both finite\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            mask = np.isfinite(a) & np.isfinite(b)\u001b[90m\u001b[39;49;00m\n","            \u001b[94mif\u001b[39;49;00m mask.sum() == \u001b[94m0\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                \u001b[94mcontinue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            diff = np.nanmax(np.abs(a[mask] - b[mask]))\u001b[90m\u001b[39;49;00m\n",">           \u001b[94massert\u001b[39;49;00m \u001b[96mfloat\u001b[39;49;00m(diff) <= \u001b[94m1e-6\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcol\u001b[33m}\u001b[39;49;00m\u001b[33m deviates from causal recompute for \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtkr\u001b[33m}\u001b[39;49;00m\u001b[33m: max |Œî|=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdiff\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: ewm_std_20 deviates from causal recompute for AAPL: max |Œî|=0.003007247351255609\u001b[0m\n","\u001b[1m\u001b[31mE           assert 0.003007247351255609 <= 1e-06\u001b[0m\n","\u001b[1m\u001b[31mE            +  where 0.003007247351255609 = float(np.float64(0.003007247351255609))\u001b[0m\n","\n","\u001b[1m\u001b[31mtests/test_leakage_features.py\u001b[0m:65: AssertionError\n","\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/test_leakage_features.py::\u001b[1mtest_label_definition_r1d\u001b[0m - AssertionError: r_1d mismatch for AAPL\n","\u001b[31mFAILED\u001b[0m tests/test_leakage_features.py::\u001b[1mtest_features_match_causal_recompute[exp_mean]\u001b[0m - AssertionError: exp_mean deviates from causal recompute for AAPL: max |Œî|=0...\n","\u001b[31mFAILED\u001b[0m tests/test_leakage_features.py::\u001b[1mtest_features_match_causal_recompute[exp_std]\u001b[0m - AssertionError: exp_std deviates from causal recompute for AAPL: max |Œî|=0....\n","\u001b[31mFAILED\u001b[0m tests/test_leakage_features.py::\u001b[1mtest_features_match_causal_recompute[ewm_mean_20]\u001b[0m - AssertionError: ewm_mean_20 deviates from causal recompute for AAPL: max |Œî...\n","\u001b[31mFAILED\u001b[0m tests/test_leakage_features.py::\u001b[1mtest_features_match_causal_recompute[ewm_std_20]\u001b[0m - AssertionError: ewm_std_20 deviates from causal recompute for AAPL: max |Œî|...\n"]}]},{"cell_type":"markdown","source":["> If a test fails, **fix the pipeline**, don‚Äôt weaken the test.\n","\n","# 4) Add **multi‚Äëstep labels** (e.g., t+5) and tests"],"metadata":{"id":"AoBegdt34wno"}},{"cell_type":"code","source":["# save to scripts/make_multistep_labels.py\n","from __future__ import annotations\n","import pandas as pd, numpy as np\n","from pathlib import Path\n","\n","def make_multistep(in_parquet=\"data/processed/returns.parquet\", horizons=(5,)):\n","    df = pd.read_parquet(in_parquet).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","    for H in horizons:\n","        # r_Hd = sum of next H log returns: shift(-1) ... shift(-H): accumulative log return over H days\n","        s = df.groupby(\"ticker\")[\"log_return\"]\n","        acc = None  # initialize an accumulator\n","        for h in range(1, H+1):\n","            sh = s.shift(-h)\n","            acc = sh if acc is None else (acc + sh)  # accumulative\n","        df[f\"r_{H}d\"] = acc\n","    out = df\n","    Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n","    out.to_parquet(\"data/processed/returns_multistep.parquet\", compression=\"zstd\", index=False)\n","    print(\"Wrote data/processed/returns_multistep.parquet\", out.shape)\n","\n","if __name__ == \"__main__\": #see below for more explanation\n","    make_multistep()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1NrzrM5T41Hc","executionInfo":{"status":"ok","timestamp":1763255401156,"user_tz":360,"elapsed":347,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"98f06117-8f50-402a-bcdd-786068a0e126"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1210775006.py:10: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  s = df.groupby(\"ticker\")[\"log_return\"]\n"]},{"output_type":"stream","name":"stdout","text":["Wrote data/processed/returns_multistep.parquet (4500, 7)\n"]}]},{"cell_type":"markdown","source":["```python\n","if __name__ == \"__main__\":\n","    make_multistep()\n","```\n","\n","\n","##  1. What `__name__` is\n","\n","Every Python file (module) automatically gets a built-in variable called `__name__`.\n","\n","* When you **run a file directly** (like `python my_script.py`),\n","  Python sets `__name__ = \"__main__\"`.\n","* When you **import the file** from another module,\n","  `__name__` is set to that module‚Äôs name (e.g. `\"my_script\"`).\n","\n","So:\n","\n","| How the file is used  | Value of `__name__` |\n","| --------------------- | ------------------- |\n","| `python my_script.py` | `\"__main__\"`        |\n","| `import my_script`    | `\"my_script\"`       |\n","\n","---\n","\n","##  2. The purpose of the `if __name__ == \"__main__\":` block\n","\n","It lets you define code that should **run only when the file is executed directly**,\n","and **not when imported**.\n","\n","That‚Äôs very useful for separating:\n","\n","* **Reusable code** (functions, classes) ‚Äî used by imports.\n","* **Executable script logic** ‚Äî e.g. calling `main()` or `make_multistep()`.\n","\n","---\n","\n","##  3. In the example\n","\n","```python\n","if __name__ == \"__main__\":\n","    make_multistep()\n","```\n","\n","means:\n","\n","> ‚ÄúIf this script is run directly from the command line, call the function `make_multistep()`.‚Äù\n","\n","If the same file is imported as a module inside another script (for example: `from scripts.build_targets import make_multistep`),\n","then this code **will not run automatically**.\n","\n","---\n","\n","###  Typical project pattern\n","\n","```python\n","def make_multistep():\n","    # your logic here (e.g. compute multistep targets)\n","    ...\n","\n","if __name__ == \"__main__\":\n","    make_multistep()\n","```\n","\n","* When you run:\n","\n","  ```bash\n","  python scripts/make_targets.py\n","  ```\n","\n","  ‚Üí the function executes immediately.\n","* When you import the same file in another script or notebook,\n","  you can call `make_multistep()` manually, but the automatic execution is skipped.\n","\n","---\n","\n","##  4. Why it‚Äôs good practice\n","\n","- Prevents unwanted execution when importing code.\n","- Makes the module both *importable* and *runnable*.\n","- Common in data pipelines and CLI tools ‚Äî e.g.:\n","\n","```python\n","def main():\n","    parser = argparse.ArgumentParser()\n","    ...\n","    args = parser.parse_args()\n","    run_pipeline(args)\n","\n","if __name__ == \"__main__\":\n","    main()\n","```\n","\n","---\n","\n","##  Summary\n","\n","| Component                    | Meaning                                                                                      |\n","| ---------------------------- | -------------------------------------------------------------------------------------------- |\n","| `__name__`                   | Automatically set by Python to either `\"__main__\"` (if run) or the module name (if imported) |\n","| `if __name__ == \"__main__\":` | Conditional that runs code only when script executed directly                                |\n","| `make_multistep()`           | Your function ‚Äî called only in standalone execution                                          |\n","| Purpose                      | Separate script behavior from reusable definitions                                           |\n","\n","---\n","\n"],"metadata":{"id":"ViLVa6Nsr27S"}},{"cell_type":"code","source":["!python scripts/make_multistep_labels.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Swd_WTcZZYQF","executionInfo":{"status":"ok","timestamp":1763255402272,"user_tz":360,"elapsed":1114,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"3438aa75-01bf-4bc3-c150-7ee69205c5bf"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/dspt25/STAT4160/scripts/make_multistep_labels.py:10: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  s = df.groupby(\"ticker\")[\"log_return\"]\n","Wrote data/processed/returns_multistep.parquet (4500, 7)\n"]}]},{"cell_type":"markdown","source":["Add a test for label correctness:"],"metadata":{"id":"RoVkAd72qeId"}},{"cell_type":"code","source":["# save to tests/test_labels_multistep.py\n","import pandas as pd, numpy as np\n","\n","def test_r5d_definition():\n","    df = pd.read_parquet(\"data/processed/returns_multistep.parquet\").sort_values([\"ticker\",\"date\"])\n","    if \"r_5d\" not in df.columns:\n","        return\n","    for tkr, g in df.groupby(\"ticker\"):\n","        lr = g[\"log_return\"]\n","        r5 = sum(lr.shift(-h) for h in range(1,6))\n","        diff = (g[\"r_5d\"] - r5).abs().max()\n","        assert float(diff) < 1e-10, f\"r_5d misdefined for {tkr} (max |Œî|={diff})\""],"metadata":{"id":"78aPBYpGqYH4","executionInfo":{"status":"ok","timestamp":1763255402275,"user_tz":360,"elapsed":2,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["!pytest -q tests/test_labels_multistep.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vCnft8HDq0cT","executionInfo":{"status":"ok","timestamp":1763255404807,"user_tz":360,"elapsed":2531,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"787c5bfb-2d69-44a0-929d-a8c3144aa6b5"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[32m.\u001b[0m\u001b[32m                                                                        [100%]\u001b[0m\n"]}]},{"cell_type":"markdown","source":["## Homework (due before Session 18)\n","\n","**Goal:** Document your evaluation protocol and ship a concise ‚Äúleakage & bias‚Äù memo, plus a one‚Äëcommand audit.\n","\n","### Part A ‚Äî Generate a **protocol memo** (`reports/eval_protocol.md`)"],"metadata":{"id":"1Z-foIZcrHwx"}},{"cell_type":"code","source":["# save to scripts/write_eval_protocol.py\n","from __future__ import annotations\n","import pandas as pd, numpy as np\n","from pathlib import Path\n","from datetime import date\n","\n","def make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n","    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n","    i = train_min - 1; out=[]\n","    while True:\n","        if i >= len(u): break\n","        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1\n","        if ve >= len(u): break\n","        out.append((a,b,u[vs],u[ve])); i += step\n","    return out\n","\n","def main():\n","    ret = pd.read_parquet(\"data/processed/returns.parquet\").sort_values([\"ticker\",\"date\"])\n","    # splits = make_rolling_origin_splits(ret[\"date\"]). # this split is empty\n","    splits = make_rolling_origin_splits(ret[\"date\"], train_min=80, val_size=21, step=21, embargo=5)\n","    a,b,c,d = splits[0]\n","    # Universe info\n","    univ_files = sorted(Path(\"data/static\").glob(\"universe_*.csv\")) # see below for more explanantions\n","    univ = univ_files[-1] if univ_files else None # take the last file\n","    univ_count = pd.read_csv(univ).shape[0] if univ else ret[\"ticker\"].nunique()\n","    md = []\n","    md += [\"# Evaluation Protocol (Leakage‚ÄëAware)\", \"\"]\n","    md += [\"**Date:** \" + date.today().isoformat(), \"\"]\n","    md += [\"## Splits\", f\"- Train window (split 1): **{a.date()} ‚Üí {b.date()}**\",\n","           f\"- Embargo: **5** business days\", f\"- Validation window: **{c.date()} ‚Üí {d.date()}**\",\n","           f\"- Step between origins: **63** business days\", \"\"]\n","    md += [\"## Static Universe\", f\"- Universe file: **{univ.name if univ else '(none)'}**\",\n","           f\"- Count: **{univ_count}** tickers\",\n","           \"- Selection rule: tickers with ‚â•252 obs by first train end; fixed for all splits.\", \"\"]\n","    md += [\"## Labels\", \"- `r_1d` = next‚Äëday log return `log_return.shift(-1)` per ticker.\",\n","           \"- `r_5d` (if used) = sum of `log_return.shift(-1..-5)`.\", \"\"]\n","    md += [\"## Leakage Controls\",\n","           \"- Features computed from ‚â§ t only (rolling/ewm/expanding without negative shifts).\",\n","           \"- No forward‚Äëfill across split boundaries; embargo = 5 days.\",\n","           \"- Scalers/normalizers fit on TRAIN only.\",\n","           \"- Tests: `tests/test_leakage_features.py`, `tests/test_labels_multistep.py`.\", \"\"]\n","    md += [\"## Caveats\",\n","           \"- Educational dataset; not investment advice.\",\n","           \"- Survivorship minimized via static universe; still subject to data vendor quirks.\", \"\"]\n","    Path(\"reports\").mkdir(parents=True, exist_ok=True)\n","    Path(\"reports/eval_protocol.md\").write_text(\"\\n\".join(md))\n","    print(\"Wrote reports/eval_protocol.md\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vkqe91WTs2QY","executionInfo":{"status":"ok","timestamp":1763255405086,"user_tz":360,"elapsed":277,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"86b995cb-c83c-43cf-ff10-1f7b368f3f65"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote reports/eval_protocol.md\n"]}]},{"cell_type":"code","source":["[\"a\", \"b\"]+[\"c\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"us6ybPiwwFNw","executionInfo":{"status":"ok","timestamp":1763255405093,"user_tz":360,"elapsed":5,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"f39cd25b-45c6-4aca-b253-baa862c1759f"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['a', 'b', 'c']"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["##  1Ô∏è‚É£ `Path(\"data/static\").glob(\"universe_*.csv\")`\n","\n","* `Path(...)` comes from the **`pathlib`** module ‚Äî a modern replacement for `os.path`.\n","* `.glob(\"universe_*.csv\")` searches that folder for **all files** whose names match the pattern `universe_*.csv`.\n","\n","So if your folder `data/static` contains:\n","\n","```\n","universe_2023.csv\n","universe_2024.csv\n","universe_2025.csv\n","readme.txt\n","```\n","\n","then:\n","\n","```python\n","list(Path(\"data/static\").glob(\"universe_*.csv\"))\n","```\n","\n","returns something like:\n","\n","```python\n","[PosixPath('data/static/universe_2023.csv'),\n"," PosixPath('data/static/universe_2024.csv'),\n"," PosixPath('data/static/universe_2025.csv')]\n","```\n","\n","---\n","\n","##  2Ô∏è‚É£ `sorted(...)`\n","\n","```python\n","univ_files = sorted(Path(\"data/static\").glob(\"universe_*.csv\"))\n","```\n","\n","* Converts the generator returned by `.glob()` into a list and sorts it alphabetically (lexicographically).\n","* Because filenames like `universe_2023.csv`, `universe_2024.csv` sort in chronological order, sorting ensures the latest one appears last ‚Äî assuming your filenames encode time or version numerically or lexicographically.\n","\n","Result:\n","\n","```python\n","univ_files = [\n","    PosixPath('data/static/universe_2023.csv'),\n","    PosixPath('data/static/universe_2024.csv'),\n","    PosixPath('data/static/universe_2025.csv')\n","]\n","```\n","\n","---\n","\n","##  3Ô∏è‚É£ `univ = univ_files[-1] if univ_files else None`\n","\n","This is a **conditional expression** (a one-line `if` statement):\n","\n","* If the list `univ_files` is **non-empty**, take its **last element** (`[-1]`).\n","* Otherwise (if no files matched), set `univ = None`.\n","\n","So effectively:\n","\n","```python\n","if univ_files:\n","    univ = univ_files[-1]\n","else:\n","    univ = None\n","```\n","\n","---\n","\n","##  4Ô∏è‚É£ What `univ` holds\n","\n","* If files are found ‚Üí the latest file (based on sorted order).\n","  e.g. `PosixPath('data/static/universe_2025.csv')`\n","* If none are found ‚Üí `None`.\n","\n","This allows later code to safely check:\n","\n","```python\n","if univ is not None:\n","    df = pd.read_csv(univ)\n","else:\n","    print(\"No universe files found.\")\n","```\n","\n","---\n","\n"],"metadata":{"id":"CW_kTBXnvFN3"}},{"cell_type":"code","source":["!python scripts/write_eval_protocol.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0S-nItk7w0A_","executionInfo":{"status":"ok","timestamp":1763255406105,"user_tz":360,"elapsed":1002,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"c49763f5-97f6-49af-cb03-cbe69e2f86bb"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote reports/eval_protocol.md\n"]}]},{"cell_type":"markdown","source":["### Part B ‚Äî One‚Äëcommand **leakage audit** target\n","\n","Append to your `Makefile`:\n","\n","``` make\n",".PHONY: leakage-audit\n","leakage-audit: ## Run leakage & label tests; write eval protocol\n","\\tpytest -q tests/test_leakage_features.py tests/test_labels_multistep.py\n","\\tpython scripts/write_eval_protocol.py\n","```\n","\n","Then run:\n","\n","``` bash\n","make leakage-audit\n","```"],"metadata":{"id":"dofwr9Bnw8--"}},{"cell_type":"code","source":["# refer to lec14-inclass.ipynb how to add this using Python code\n","\n","from pathlib import Path\n","mk = Path(\"Makefile\")\n","text = mk.read_text() if mk.exists() else \"\"\n","if \"leakage-audit\" not in text:\n","    text += \"\"\"\n","\n",".PHONY: leakage-audit\n","leakage-audit: ## Run leakage & label tests; write eval protocol\n","\\tpytest -q tests/test_leakage_features.py tests/test_labels_multistep.py\n","\\tpython scripts/write_eval_protocol.py\n","\"\"\"\n","mk.write_text(text)\n","print(mk.read_text())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lb2ouVUJxBMk","executionInfo":{"status":"ok","timestamp":1763255406339,"user_tz":360,"elapsed":232,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"2e1754ba-cc5a-4b30-8bd4-6af323fda43c"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["# Makefile ‚Äî unified-stocks\n","SHELL := /bin/bash\n",".SHELLFLAGS := -eu -o pipefail -c\n",".ONESHELL:\n","\n","\n","PY := python\n","QUARTO := quarto\n","\n","START ?= 2020-01-01\n","END   ?= 2025-08-01\n","ROLL  ?= 30\n","\n","DATA_RAW := data/raw/prices.csv\n","FEATS    := data/processed/features.parquet\n","REPORT   := docs/reports/eda.html\n","\n","# Default target\n",".DEFAULT_GOAL := help\n","\n",".PHONY: help all clean clobber qa report backup\n","\n","help: ## Show help for each target\n","\t@awk 'BEGIN {FS = \":.*##\"; printf \"Available targets:\\n\"} /^[a-zA-Z0-9_\\-]+:.*##/ {printf \"  \\033[36m%-18s\\033[0m %s\\n\", $$1, $$2}' $(MAKEFILE_LIST)\n","\n","# all: $(DATA_RAW) $(FEATS) report backup ## Run the full pipeline and back up artifacts\n","all: $(DATA_RAW) $(FEATS) report train backup\n","\n","$(DATA_RAW): scripts/get_prices.py tickers_25.csv\n","\t$(PY) scripts/get_prices.py --tickers tickers_25.csv --start $(START) --end $(END) --out $(DATA_RAW)\n","\n","$(FEATS): scripts/build_features.py $(DATA_RAW) scripts/qa_csv.sh\n","\t# Basic QA first\n","\tscripts/qa_csv.sh $(DATA_RAW)\n","\t$(PY) scripts/build_features.py --input $(DATA_RAW) --out $(FEATS) --roll $(ROLL)\n","\n","# --- add after FEATS definition, near other targets ---\n","\n","TRAIN_METRICS := reports/baseline_metrics.json\n","\n",".PHONY: train\n","train: $(TRAIN_METRICS) ## Train toy baseline and write metrics\n","\n","$(TRAIN_METRICS): scripts/train_baseline.py $(FEATS)\n","\t$(PY) scripts/train_baseline.py --features $(FEATS) --out-metrics $(TRAIN_METRICS)\n","\n","report: $(REPORT) ## Render Quarto EDA to docs1/\n","$(REPORT): reports/eda.qmd _quarto.yml docs1/style.css\n","\t$(QUARTO) render reports/eda.qmd -P symbol:AAPL -P start_date=$(START) -P end_date=$(END) -P rolling=$(ROLL) --output-dir docs1/\n","\t@test -f $(REPORT) || (echo \"Report not generated.\" && exit 1)\n","\n","backup: ## Rsync selected artifacts to backups/<timestamp>/\n","\t./scripts/backup.sh\n","\n","clean: ## Remove intermediate artifacts (safe)\n","\trm -rf data/interim\n","\trm -rf data/processed/*.parquet || true\n","\n","clobber: clean ## Remove generated reports and backups (dangerous)\n","\trm -rf docs/reports || true\n","\trm -rf backups || true\n","\n","DB := data/prices.db\n","\n",".PHONY: db sql-report\n","db: ## Build/refresh SQLite database from CSVs\n","\tpython scripts/build_db.py --db $(DB) --tickers tickers_25.csv --prices data/raw/prices.csv\n","\n","sql-report: db ## Generate a simple SQL-driven CSV summary\n","\t$(PY) - <<-'PY'\n","\timport pandas as pd, sqlite3, os\n","\tcon = sqlite3.connect(\"data/prices.db\")\n","\tdf = pd.read_sql_query(\"\"\"\n","\tSELECT m.sector,\n","\t       COUNT(*) AS n_obs,\n","\t       AVG(ABS(p.log_return)) AS mean_abs_return\n","\tFROM prices p\n","\tJOIN meta m ON p.ticker = m.ticker\n","\tGROUP BY m.sector\n","\tORDER BY n_obs DESC;\n","\t\"\"\", con)\n","\tos.makedirs(\"reports\", exist_ok=True)\n","\tdf.to_csv(\"reports/sql_sector_summary.csv\", index=False)\n","\tprint(df.head())\n","\tcon.close()\n","\tPY\n","\n",".PHONY: prices-parquet returns-parquet\n","prices-parquet:  ## Clean raw prices and save processed Parquet(s)\n","\tpython - <<'PY'\n","\timport pandas as pd, glob, pathlib, numpy as np, re, json\n","\tfrom pathlib import Path\n","\t# (Paste the functions from the lab: standardize_columns, clean_prices, join_meta)\n","\t# Then read raw -> clean -> write parquet as in the lab\n","\tPY\n","\n","returns-parquet: ## Build returns.parquet with r_1d + calendar features\n","\tpython - <<'PY'\n","\timport pandas as pd, numpy as np\n","\tp=\"data/processed/prices.parquet\"; r=pd.read_parquet(p).sort_values([\"ticker\",\"date\"])\n","\tr[\"log_return\"]=r.groupby(\"ticker\")[\"adj_close\"].apply(lambda s: np.log(s/s.shift(1))).reset_index(level=0, drop=True)\n","\tr[\"r_1d\"]=r.groupby(\"ticker\")[\"log_return\"].shift(-1)\n","\tr[\"weekday\"]=r[\"date\"].dt.weekday.astype(\"int8\"); r[\"month\"]=r[\"date\"].dt.month.astype(\"int8\")\n","\tr[[\"date\",\"ticker\",\"log_return\",\"r_1d\",\"weekday\",\"month\"]].to_parquet(\"data/processed/returns.parquet\", compression=\"zstd\", index=False)\n","\tprint(\"Wrote data/processed/returns.parquet\")\n","\tPY\n","\n",".PHONY: health test\n","health: ## Generate health.json and health.md from the current features parquet\n","\tpython scripts/health.py\n","\n","pytest:\n","\tpytest tests/test_logging.py -q\n","test: pytest\n","\n","\n","\n","\n",".PHONY: lint test ci-local\n","lint: ## Run pre-commit hooks on all files\n","\tpre-commit run --all-files\n","\n","test: ## Run fast tests\n","\tpytest -q --maxfail=1\n","\n","ci-local: lint test ## Simulate CI locally\n","\n","\n",".PHONY: baselines\n","baselines: ## Evaluate naive & seasonal-naive baselines across all splits\n","\tpython scripts/baselines_eval.py --seasonality 5\n","\n","\n",".PHONY: leakage-audit\n","leakage-audit: ## Run leakage & label tests; write eval protocol\n","\tpytest -q tests/test_leakage_features.py tests/test_labels_multistep.py\n","\tpython scripts/write_eval_protocol.py\n","\n"]}]},{"cell_type":"code","source":["%%bash\n","make leakage-audit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"pgwcbJsGxB2T","executionInfo":{"status":"error","timestamp":1763255409788,"user_tz":360,"elapsed":3446,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"3ad82b51-4421-49c8-da20-b2674a9150c7"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["pytest -q tests/test_leakage_features.py tests/test_labels_multistep.py\n","python scripts/write_eval_protocol.py\n","F......FFFFs..                                                           [100%]\n","=================================== FAILURES ===================================\n","__________________________ test_label_definition_r1d ___________________________\n","\n","df =            date ticker  log_return  ...   exp_std  adj_close   volume\n","0    2020-01-29   AAPL   -0.018417  ...  0.00847...489  82.670891  1777769\n","3974 2020-09-07    XOM   -0.008826  ...  0.009480  81.944473  1272137\n","\n","[3975 rows x 18 columns]\n","\n","    def test_label_definition_r1d(df):\n","        for tkr, g in df.groupby(\"ticker\"):\n",">           assert g[\"r_1d\"].iloc[:-1].equals(g[\"log_return\"].iloc[1:]), f\"r_1d mismatch for {tkr}\"\n","E           AssertionError: r_1d mismatch for AAPL\n","E           assert False\n","E            +  where False = equals(1     -0.002351\\n2     -0.012675\\n3      0.002713\\n4      0.001568\\n5     -0.001869\\n         ...   \\n154   -0.011083\\n155   -0.012161\\n156    0.013355\\n157   -0.005071\\n158    0.002917\\nName: log_return, Length: 158, dtype: float32)\n","E            +    where equals = 0     -0.002351\\n1     -0.012675\\n2      0.002713\\n3      0.001568\\n4     -0.001869\\n         ...   \\n153   -0.011083\\n154   -0.012161\\n155    0.013355\\n156   -0.005071\\n157    0.002917\\nName: r_1d, Length: 158, dtype: float32.equals\n","\n","tests/test_leakage_features.py:21: AssertionError\n","________________ test_features_match_causal_recompute[exp_mean] ________________\n","\n","df =            date ticker  log_return  ...   exp_std  adj_close   volume\n","0    2020-01-29   AAPL   -0.018417  ...  0.00847...489  82.670891  1777769\n","3974 2020-09-07    XOM   -0.008826  ...  0.009480  81.944473  1272137\n","\n","[3975 rows x 18 columns]\n","col = 'exp_mean'\n","\n","    @pytest.mark.parametrize(\"col\", [\"lag1\",\"lag2\",\"lag3\",\"roll_mean_20\",\"roll_std_20\",\"zscore_20\",\"exp_mean\",\"exp_std\",\"ewm_mean_20\",\"ewm_std_20\",\"rsi_14\"])\n","    def test_features_match_causal_recompute(df, col):\n","        if col not in df.columns:\n","            pytest.skip(f\"{col} not present\")\n","        # Compare per ticker to avoid cross-group alignment issues\n","        for tkr, g in df.groupby(\"ticker\", sort=False):\n","            ref = _recompute_safe(g)\n","            if col not in ref.columns:\n","                continue\n","            a = g[col].to_numpy()\n","            b = ref[col].to_numpy()\n","            # Allow NaNs at the start; compare where both finite\n","            mask = np.isfinite(a) & np.isfinite(b)\n","            if mask.sum() == 0:\n","                continue\n","            diff = np.nanmax(np.abs(a[mask] - b[mask]))\n",">           assert float(diff) <= 1e-6, f\"{col} deviates from causal recompute for {tkr}: max |Œî|={diff}\"\n","E           AssertionError: exp_mean deviates from causal recompute for AAPL: max |Œî|=0.0006860782421426849\n","E           assert 0.0006860782421426849 <= 1e-06\n","E            +  where 0.0006860782421426849 = float(np.float64(0.0006860782421426849))\n","\n","tests/test_leakage_features.py:65: AssertionError\n","________________ test_features_match_causal_recompute[exp_std] _________________\n","\n","df =            date ticker  log_return  ...   exp_std  adj_close   volume\n","0    2020-01-29   AAPL   -0.018417  ...  0.00847...489  82.670891  1777769\n","3974 2020-09-07    XOM   -0.008826  ...  0.009480  81.944473  1272137\n","\n","[3975 rows x 18 columns]\n","col = 'exp_std'\n","\n","    @pytest.mark.parametrize(\"col\", [\"lag1\",\"lag2\",\"lag3\",\"roll_mean_20\",\"roll_std_20\",\"zscore_20\",\"exp_mean\",\"exp_std\",\"ewm_mean_20\",\"ewm_std_20\",\"rsi_14\"])\n","    def test_features_match_causal_recompute(df, col):\n","        if col not in df.columns:\n","            pytest.skip(f\"{col} not present\")\n","        # Compare per ticker to avoid cross-group alignment issues\n","        for tkr, g in df.groupby(\"ticker\", sort=False):\n","            ref = _recompute_safe(g)\n","            if col not in ref.columns:\n","                continue\n","            a = g[col].to_numpy()\n","            b = ref[col].to_numpy()\n","            # Allow NaNs at the start; compare where both finite\n","            mask = np.isfinite(a) & np.isfinite(b)\n","            if mask.sum() == 0:\n","                continue\n","            diff = np.nanmax(np.abs(a[mask] - b[mask]))\n",">           assert float(diff) <= 1e-6, f\"{col} deviates from causal recompute for {tkr}: max |Œî|={diff}\"\n","E           AssertionError: exp_std deviates from causal recompute for AAPL: max |Œî|=0.0007243017298223572\n","E           assert 0.0007243017298223572 <= 1e-06\n","E            +  where 0.0007243017298223572 = float(np.float64(0.0007243017298223572))\n","\n","tests/test_leakage_features.py:65: AssertionError\n","______________ test_features_match_causal_recompute[ewm_mean_20] _______________\n","\n","df =            date ticker  log_return  ...   exp_std  adj_close   volume\n","0    2020-01-29   AAPL   -0.018417  ...  0.00847...489  82.670891  1777769\n","3974 2020-09-07    XOM   -0.008826  ...  0.009480  81.944473  1272137\n","\n","[3975 rows x 18 columns]\n","col = 'ewm_mean_20'\n","\n","    @pytest.mark.parametrize(\"col\", [\"lag1\",\"lag2\",\"lag3\",\"roll_mean_20\",\"roll_std_20\",\"zscore_20\",\"exp_mean\",\"exp_std\",\"ewm_mean_20\",\"ewm_std_20\",\"rsi_14\"])\n","    def test_features_match_causal_recompute(df, col):\n","        if col not in df.columns:\n","            pytest.skip(f\"{col} not present\")\n","        # Compare per ticker to avoid cross-group alignment issues\n","        for tkr, g in df.groupby(\"ticker\", sort=False):\n","            ref = _recompute_safe(g)\n","            if col not in ref.columns:\n","                continue\n","            a = g[col].to_numpy()\n","            b = ref[col].to_numpy()\n","            # Allow NaNs at the start; compare where both finite\n","            mask = np.isfinite(a) & np.isfinite(b)\n","            if mask.sum() == 0:\n","                continue\n","            diff = np.nanmax(np.abs(a[mask] - b[mask]))\n",">           assert float(diff) <= 1e-6, f\"{col} deviates from causal recompute for {tkr}: max |Œî|={diff}\"\n","E           AssertionError: ewm_mean_20 deviates from causal recompute for AAPL: max |Œî|=0.013164876028895378\n","E           assert 0.013164876028895378 <= 1e-06\n","E            +  where 0.013164876028895378 = float(np.float64(0.013164876028895378))\n","\n","tests/test_leakage_features.py:65: AssertionError\n","_______________ test_features_match_causal_recompute[ewm_std_20] _______________\n","\n","df =            date ticker  log_return  ...   exp_std  adj_close   volume\n","0    2020-01-29   AAPL   -0.018417  ...  0.00847...489  82.670891  1777769\n","3974 2020-09-07    XOM   -0.008826  ...  0.009480  81.944473  1272137\n","\n","[3975 rows x 18 columns]\n","col = 'ewm_std_20'\n","\n","    @pytest.mark.parametrize(\"col\", [\"lag1\",\"lag2\",\"lag3\",\"roll_mean_20\",\"roll_std_20\",\"zscore_20\",\"exp_mean\",\"exp_std\",\"ewm_mean_20\",\"ewm_std_20\",\"rsi_14\"])\n","    def test_features_match_causal_recompute(df, col):\n","        if col not in df.columns:\n","            pytest.skip(f\"{col} not present\")\n","        # Compare per ticker to avoid cross-group alignment issues\n","        for tkr, g in df.groupby(\"ticker\", sort=False):\n","            ref = _recompute_safe(g)\n","            if col not in ref.columns:\n","                continue\n","            a = g[col].to_numpy()\n","            b = ref[col].to_numpy()\n","            # Allow NaNs at the start; compare where both finite\n","            mask = np.isfinite(a) & np.isfinite(b)\n","            if mask.sum() == 0:\n","                continue\n","            diff = np.nanmax(np.abs(a[mask] - b[mask]))\n",">           assert float(diff) <= 1e-6, f\"{col} deviates from causal recompute for {tkr}: max |Œî|={diff}\"\n","E           AssertionError: ewm_std_20 deviates from causal recompute for AAPL: max |Œî|=0.003007247351255609\n","E           assert 0.003007247351255609 <= 1e-06\n","E            +  where 0.003007247351255609 = float(np.float64(0.003007247351255609))\n","\n","tests/test_leakage_features.py:65: AssertionError\n","=========================== short test summary info ============================\n","FAILED tests/test_leakage_features.py::test_label_definition_r1d - AssertionE...\n","FAILED tests/test_leakage_features.py::test_features_match_causal_recompute[exp_mean]\n","FAILED tests/test_leakage_features.py::test_features_match_causal_recompute[exp_std]\n","FAILED tests/test_leakage_features.py::test_features_match_causal_recompute[ewm_mean_20]\n","FAILED tests/test_leakage_features.py::test_features_match_causal_recompute[ewm_std_20]\n"]},{"output_type":"stream","name":"stderr","text":["make: *** [Makefile:136: leakage-audit] Error 1\n"]},{"output_type":"error","ename":"CalledProcessError","evalue":"Command 'b'make leakage-audit\\n'' returned non-zero exit status 2.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3859837403.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'make leakage-audit\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'make leakage-audit\\n'' returned non-zero exit status 2."]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"aUycPy1CxAnD"}}]}